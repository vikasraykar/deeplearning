<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Deep Learning</title>
    <link>https://example.org/</link>
    <description>Recent content on Deep Learning</description>
    <generator>Hugo</generator>
    <language>en-us</language>
    <atom:link href="https://example.org/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>Basics</title>
      <link>https://example.org/docs/rl/basics/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://example.org/docs/rl/basics/</guid>
      <description>&lt;h1 id=&#34;reinforcement-learning-basics&#34;&gt;&#xA;  Reinforcement learning basics&#xA;  &lt;a class=&#34;anchor&#34; href=&#34;#reinforcement-learning-basics&#34;&gt;#&lt;/a&gt;&#xA;&lt;/h1&gt;&#xA;&lt;a  href=&#34;https://www.dropbox.com/s/4pdr6y60t0r9mm2/2017_07_29_anthill_deep_reinforcement_learning_tutorial.pdf?dl=0&#34;   target=&#34;_blank&#34; rel=&#34;noopener&#34;  class=&#34;book-btn&#34;&gt;slides&lt;/a&gt;&#xA;&#xA;&lt;blockquote&gt;&#xA;&lt;p&gt;Reinforcement Learning (RL) is a natural computational paradigm for agents learning from interaction to achieve a goal. Deep learning (DL) provides a powerful general-purpose representation learning framework. A combination of these two has recently emerged as a strong contender for artificial general intelligence. This tutorial will provide a gentle exposition of RL concepts and DL based RL with a focus on policy gradients.&lt;/p&gt;&lt;/blockquote&gt;&#xA;&#xA;&lt;link rel=&#34;stylesheet&#34; href=&#34;https://example.org/katex/katex.min.css&#34; /&gt;&#xA;&lt;script defer src=&#34;https://example.org/katex/katex.min.js&#34;&gt;&lt;/script&gt;&#xA;&lt;script defer src=&#34;https://example.org/katex/auto-render.min.js&#34; onload=&#34;renderMathInElement(document.body);&#34;&gt;&lt;/script&gt;&lt;span&gt;&#xA;  \(\)&#xA;&lt;/span&gt;&#xA;&#xA;&lt;h2 id=&#34;the-agent-environment-interaction&#34;&gt;&#xA;  The agent-environment interaction&#xA;  &lt;a class=&#34;anchor&#34; href=&#34;#the-agent-environment-interaction&#34;&gt;#&lt;/a&gt;&#xA;&lt;/h2&gt;&#xA;&lt;p&gt;The reinforcement learning (RL) framework is an abstraction of the problem of &lt;strong&gt;goal directed learning from interaction&lt;/strong&gt;.&lt;/p&gt;</description>
    </item>
    <item>
      <title>Linear Regression</title>
      <link>https://example.org/docs/neural_networks/linear_regression/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://example.org/docs/neural_networks/linear_regression/</guid>
      <description>&lt;h2 id=&#34;linear-regression&#34;&gt;&#xA;  Linear Regression&#xA;  &lt;a class=&#34;anchor&#34; href=&#34;#linear-regression&#34;&gt;#&lt;/a&gt;&#xA;&lt;/h2&gt;&#xA;&lt;p&gt;Linear Regression is a single layer neural network for regression.&lt;/p&gt;&#xA;&#xA;&#xA;&lt;script src=&#34;https://example.org/mermaid.min.js&#34;&gt;&lt;/script&gt;&#xA;&#xA;  &lt;script&gt;mermaid.initialize({&#xA;  &#34;flowchart&#34;: {&#xA;    &#34;useMaxWidth&#34;:true&#xA;  },&#xA;  &#34;theme&#34;: &#34;default&#34;&#xA;}&#xA;)&lt;/script&gt;&#xA;&#xA;&#xA;&#xA;&#xA;&lt;pre class=&#34;mermaid&#34;&gt;&#xA;stateDiagram-v2&#xA;    direction LR&#xA;    z1: $$x_1$$&#xA;    z2: $$x_2$$&#xA;    zi: $$x_i$$&#xA;    zM: $$x_d$$&#xA;    aj: $$a=\sum_i w_{i} x_i$$&#xA;    zj: $$z=a$$&#xA;    z1 --&gt; aj:$$w_{1}$$&#xA;    z2 --&gt; aj:$$w_{2}$$&#xA;    zi --&gt; aj:$$w_{i}$$&#xA;    zM --&gt; aj:$$w_{d}$$&#xA;    aj --&gt; zj&#xA;    zj --&gt; END:::hidden&#xA;    note left of zM : Inputs&#xA;    note left of aj : Pre-activation&#xA;    note left of zj : Activation&#xA;    note left of END : Output&#xA;    classDef hidden display: none;&#xA;&lt;/pre&gt;&#xA;&#xA;&lt;h3 id=&#34;model&#34;&gt;&#xA;  Model&#xA;  &lt;a class=&#34;anchor&#34; href=&#34;#model&#34;&gt;#&lt;/a&gt;&#xA;&lt;/h3&gt;&#xA;&lt;p&gt;Linear Regression assumes a &lt;strong&gt;linear relationship&lt;/strong&gt; between the target &#xA;&lt;link rel=&#34;stylesheet&#34; href=&#34;https://example.org/katex/katex.min.css&#34; /&gt;&#xA;&lt;script defer src=&#34;https://example.org/katex/katex.min.js&#34;&gt;&lt;/script&gt;&#xA;&lt;script defer src=&#34;https://example.org/katex/auto-render.min.js&#34; onload=&#34;renderMathInElement(document.body);&#34;&gt;&lt;/script&gt;&lt;span&gt;&#xA;  \(y \in \mathbb{R}\)&#xA;&lt;/span&gt;&#xA; and the features &lt;span&gt;&#xA;  \(\mathbf{x}\in \mathbb{R}^d\)&#xA;&lt;/span&gt;&#xA;.&#xA;&lt;span&gt;&#xA;  \[&#xA;y = f(\mathbf{x}) = w_1 x_1 + w_2 x_2 + ... + w_d x_d + b = \mathbf{w}^T\mathbf{x} + b,&#xA;\]&#xA;&lt;/span&gt;&#xA;&#xA;where &lt;span&gt;&#xA;  \(\mathbf{w}\in \mathbb{R}^d\)&#xA;&lt;/span&gt;&#xA; is the &lt;span&gt;&#xA;  \(d\)&#xA;&lt;/span&gt;&#xA;-dimensional &lt;em&gt;weight vector&lt;/em&gt; and &lt;span&gt;&#xA;  \(b \in \mathbb{R}\)&#xA;&lt;/span&gt;&#xA; is the &lt;em&gt;bias term&lt;/em&gt;.&lt;/p&gt;</description>
    </item>
    <item>
      <title>Transformers</title>
      <link>https://example.org/docs/transformers/transformers101/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://example.org/docs/transformers/transformers101/</guid>
      <description>&lt;h1 id=&#34;alignment&#34;&gt;&#xA;  Alignment&#xA;  &lt;a class=&#34;anchor&#34; href=&#34;#alignment&#34;&gt;#&lt;/a&gt;&#xA;&lt;/h1&gt;&#xA;&lt;p&gt;&lt;a href=&#34;https://github.com/vikasraykar/wiki/wiki/Transformers-101&#34;&gt;https://github.com/vikasraykar/wiki/wiki/Transformers-101&lt;/a&gt;&lt;/p&gt;</description>
    </item>
    <item>
      <title>Alignment</title>
      <link>https://example.org/docs/transformers/alignment/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://example.org/docs/transformers/alignment/</guid>
      <description>&lt;h1 id=&#34;alignment&#34;&gt;&#xA;  Alignment&#xA;  &lt;a class=&#34;anchor&#34; href=&#34;#alignment&#34;&gt;#&lt;/a&gt;&#xA;&lt;/h1&gt;&#xA;&lt;p&gt;LLMs are typically trained for next-token prediction.&lt;/p&gt;&#xA;&lt;p&gt;Pre-trained LLMs may not be able to follow user instructions because they were not trained to do so.&lt;/p&gt;&#xA;&lt;p&gt;Pre-trained LLMs may generate harmful content or perpetuate  biases inherent in their training data.&lt;/p&gt;&#xA;&#xA;&#xA;&lt;script src=&#34;https://example.org/mermaid.min.js&#34;&gt;&lt;/script&gt;&#xA;&#xA;  &lt;script&gt;mermaid.initialize({&#xA;  &#34;flowchart&#34;: {&#xA;    &#34;useMaxWidth&#34;:true&#xA;  },&#xA;  &#34;theme&#34;: &#34;default&#34;&#xA;}&#xA;)&lt;/script&gt;&#xA;&#xA;&#xA;&#xA;&#xA;&lt;pre class=&#34;mermaid&#34;&gt;&#xA;---&#xA;title: LLM training stages&#xA;---&#xA;flowchart LR&#xA;    subgraph Pre-training&#xA;    A[Pre-training]&#xA;    end&#xA;    subgraph Post-training&#xA;    B[&#34;Instruction Alignment (SFT)&#34;]&#xA;    C[&#34;Preference Alignment (RLHF)&#34;]&#xA;    end&#xA;    subgraph Inference&#xA;    D[Prompt engineering]&#xA;    end&#xA;    A--&gt;B&#xA;    B--&gt;C&#xA;    C--&gt;D&#xA;&lt;/pre&gt;&#xA;&#xA;&lt;h2 id=&#34;fine-tune-llms-with-labelled-data&#34;&gt;&#xA;  Fine tune LLMs with labelled data&#xA;  &lt;a class=&#34;anchor&#34; href=&#34;#fine-tune-llms-with-labelled-data&#34;&gt;#&lt;/a&gt;&#xA;&lt;/h2&gt;&#xA;&lt;h3 id=&#34;supervised-fine-tuning-sft&#34;&gt;&#xA;  Supervised Fine Tuning (SFT)&#xA;  &lt;a class=&#34;anchor&#34; href=&#34;#supervised-fine-tuning-sft&#34;&gt;#&lt;/a&gt;&#xA;&lt;/h3&gt;&#xA;&lt;p&gt;Training data is task specific instructions paired with their expected outputs.&lt;/p&gt;</description>
    </item>
    <item>
      <title>Logistic Regression</title>
      <link>https://example.org/docs/neural_networks/logistic_regression/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://example.org/docs/neural_networks/logistic_regression/</guid>
      <description>&lt;h2 id=&#34;logistic-regression&#34;&gt;&#xA;  Logistic Regression&#xA;  &lt;a class=&#34;anchor&#34; href=&#34;#logistic-regression&#34;&gt;#&lt;/a&gt;&#xA;&lt;/h2&gt;&#xA;&lt;p&gt;Logistic Regression is a single layer neural network for binary classification.&lt;/p&gt;&#xA;&#xA;&#xA;&lt;script src=&#34;https://example.org/mermaid.min.js&#34;&gt;&lt;/script&gt;&#xA;&#xA;  &lt;script&gt;mermaid.initialize({&#xA;  &#34;flowchart&#34;: {&#xA;    &#34;useMaxWidth&#34;:true&#xA;  },&#xA;  &#34;theme&#34;: &#34;default&#34;&#xA;}&#xA;)&lt;/script&gt;&#xA;&#xA;&#xA;&#xA;&#xA;&lt;pre class=&#34;mermaid&#34;&gt;&#xA;stateDiagram-v2&#xA;    direction LR&#xA;    z1: $$x_1$$&#xA;    z2: $$x_2$$&#xA;    zi: $$x_i$$&#xA;    zM: $$x_d$$&#xA;    aj: $$a=\sum_i w_{i} x_i$$&#xA;    zj: $$z=\sigma(a)$$&#xA;    z1 --&gt; aj:$$w_{1}$$&#xA;    z2 --&gt; aj:$$w_{2}$$&#xA;    zi --&gt; aj:$$w_{i}$$&#xA;    zM --&gt; aj:$$w_{d}$$&#xA;    aj --&gt; zj&#xA;    zj --&gt; END:::hidden&#xA;    note left of zM : Inputs&#xA;    note left of aj : Pre-activation&#xA;    note left of zj : Activation&#xA;    note left of END : Output&#xA;    classDef hidden display: none;&#xA;&lt;/pre&gt;&#xA;&#xA;&lt;h2 id=&#34;statistical-model&#34;&gt;&#xA;  Statistical model&#xA;  &lt;a class=&#34;anchor&#34; href=&#34;#statistical-model&#34;&gt;#&lt;/a&gt;&#xA;&lt;/h2&gt;&#xA;&lt;p&gt;The probability of the positive class (&#xA;&lt;link rel=&#34;stylesheet&#34; href=&#34;https://example.org/katex/katex.min.css&#34; /&gt;&#xA;&lt;script defer src=&#34;https://example.org/katex/katex.min.js&#34;&gt;&lt;/script&gt;&#xA;&lt;script defer src=&#34;https://example.org/katex/auto-render.min.js&#34; onload=&#34;renderMathInElement(document.body);&#34;&gt;&lt;/script&gt;&lt;span&gt;&#xA;  \(y=1\)&#xA;&lt;/span&gt;&#xA;) for a given feature vector (&lt;span&gt;&#xA;  \(\mathbf{x}\in \mathbb{R}^d\)&#xA;&lt;/span&gt;&#xA;) is given by&#xA;&lt;span&gt;&#xA;  \[&#xA;\text{Pr}[y=1|\mathbf{x},\mathbf{w}] = \sigma(\mathbf{w}^T\mathbf{x})&#xA;\]&#xA;&lt;/span&gt;&#xA;&#xA;where &lt;span&gt;&#xA;  \(\mathbf{w}\in \mathbb{R}^d\)&#xA;&lt;/span&gt;&#xA; are the weights/&lt;strong&gt;parameters&lt;/strong&gt; of the model and &lt;span&gt;&#xA;  \(\sigma\)&#xA;&lt;/span&gt;&#xA; is the &lt;strong&gt;sigmoid&lt;/strong&gt; activation function defined as&#xA;&lt;span&gt;&#xA;  \[&#xA;\sigma(x) = \frac{1}{1-e^{-z}}&#xA;\]&#xA;&lt;/span&gt;&#xA;&lt;/p&gt;</description>
    </item>
    <item>
      <title>Entropy</title>
      <link>https://example.org/docs/neural_networks/entropy/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://example.org/docs/neural_networks/entropy/</guid>
      <description>&lt;h2 id=&#34;entropy&#34;&gt;&#xA;  Entropy&#xA;  &lt;a class=&#34;anchor&#34; href=&#34;#entropy&#34;&gt;#&lt;/a&gt;&#xA;&lt;/h2&gt;&#xA;&lt;p&gt;&#xA;&lt;link rel=&#34;stylesheet&#34; href=&#34;https://example.org/katex/katex.min.css&#34; /&gt;&#xA;&lt;script defer src=&#34;https://example.org/katex/katex.min.js&#34;&gt;&lt;/script&gt;&#xA;&lt;script defer src=&#34;https://example.org/katex/auto-render.min.js&#34; onload=&#34;renderMathInElement(document.body);&#34;&gt;&lt;/script&gt;&lt;span&gt;&#xA;  \(\)&#xA;&lt;/span&gt;&#xA;The entropy of a discrete random variable &lt;span&gt;&#xA;  \(X\)&#xA;&lt;/span&gt;&#xA; with &lt;span&gt;&#xA;  \(K\)&#xA;&lt;/span&gt;&#xA; states/categories with distribution &lt;span&gt;&#xA;  \(p_k = \text{Pr}(X=k)\)&#xA;&lt;/span&gt;&#xA; for &lt;span&gt;&#xA;  \(k=1,...,K\)&#xA;&lt;/span&gt;&#xA;  is a measure of uncertainty and is defined as follows.&#xA;&lt;span&gt;&#xA;  \[H(X) = \sum_{k=1}^{K} p_k \log_2 \frac{1}{p_k} = - \sum_{k=1}^{K} p_k \log_2 p_k \]&#xA;&lt;/span&gt;&#xA;&#xA;&lt;span&gt;&#xA;  \(\)&#xA;&lt;/span&gt;&#xA;The term &lt;span&gt;&#xA;  \(\log_2\frac{1}{p}\)&#xA;&lt;/span&gt;&#xA; quantifies the notion or surprise or uncertainty and entropy is the average uncertainty. The unit is bits (&lt;span&gt;&#xA;  \(\in [0,\log_2 K]\)&#xA;&lt;/span&gt;&#xA;) (or nats incase of natural log). The discrete distribution with maximum entropy (&lt;span&gt;&#xA;  \(\log_2 K\)&#xA;&lt;/span&gt;&#xA;) is uniform. The discrete distribution with minimum entropy (&lt;span&gt;&#xA;  \(0\)&#xA;&lt;/span&gt;&#xA;) is any delta function which puts all mass on one state/category.&lt;/p&gt;</description>
    </item>
    <item>
      <title>Gradient Descent</title>
      <link>https://example.org/docs/training/gradient_descent/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://example.org/docs/training/gradient_descent/</guid>
      <description>&lt;h2 id=&#34;gradient-descent&#34;&gt;&#xA;  Gradient Descent&#xA;  &lt;a class=&#34;anchor&#34; href=&#34;#gradient-descent&#34;&gt;#&lt;/a&gt;&#xA;&lt;/h2&gt;&#xA;&lt;blockquote&gt;&#xA;&lt;p&gt;Steepest descent.&lt;/p&gt;&lt;/blockquote&gt;&#xA;&lt;p&gt;Let &lt;span&gt;&#xA;  \(\mathbf{w}\)&#xA;&lt;/span&gt;&#xA; be a vector of all the parameters for a model.&lt;/p&gt;&#xA;&lt;p&gt;Let &lt;span&gt;&#xA;  \(L(\mathbf{w})\)&#xA;&lt;/span&gt;&#xA; be the loss function (or error function).&lt;/p&gt;&#xA;&lt;p&gt;We need to choose the model parameters that optimizes (minimizes) the loss function.&lt;/p&gt;&#xA;&lt;span&gt;&#xA;  \[&#xA;\hat{\mathbf{w}} = \argmin_{\mathbf{w}} L(\mathbf{w})&#xA;\]&#xA;&lt;/span&gt;&#xA;&#xA;&lt;p&gt;Let &lt;span&gt;&#xA;  \(\nabla L(\mathbf{w})\)&#xA;&lt;/span&gt;&#xA; be the &lt;strong&gt;gradient vector&lt;/strong&gt;, where each element is the partial derivative of the loss fucntion wrt each parameter.&lt;/p&gt;&#xA;&lt;p&gt;The gradient vector points in the direction of the greatest rate of increase of the loss function.&lt;/p&gt;</description>
    </item>
    <item>
      <title>Backpropagation</title>
      <link>https://example.org/docs/training/backpropagation/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://example.org/docs/training/backpropagation/</guid>
      <description>&lt;h2 id=&#34;backpropagation&#34;&gt;&#xA;  Backpropagation&#xA;  &lt;a class=&#34;anchor&#34; href=&#34;#backpropagation&#34;&gt;#&lt;/a&gt;&#xA;&lt;/h2&gt;&#xA;&lt;blockquote&gt;&#xA;&lt;p&gt;Backprop, Error Backpropagation.&lt;/p&gt;&lt;/blockquote&gt;&#xA;&lt;p&gt;Backpropagation (or backprop) is an efficient technique to compute the gradient of the loss function.&lt;/p&gt;&#xA;&lt;p&gt;It boils down to a local message passing scheme in which information is sent backwards through the network.&lt;/p&gt;&#xA;&lt;h3 id=&#34;forward-propagation&#34;&gt;&#xA;  Forward propagation&#xA;  &lt;a class=&#34;anchor&#34; href=&#34;#forward-propagation&#34;&gt;#&lt;/a&gt;&#xA;&lt;/h3&gt;&#xA;&#xA;&#xA;&lt;script src=&#34;https://example.org/mermaid.min.js&#34;&gt;&lt;/script&gt;&#xA;&#xA;  &lt;script&gt;mermaid.initialize({&#xA;  &#34;flowchart&#34;: {&#xA;    &#34;useMaxWidth&#34;:true&#xA;  },&#xA;  &#34;theme&#34;: &#34;default&#34;&#xA;}&#xA;)&lt;/script&gt;&#xA;&#xA;&#xA;&#xA;&#xA;&lt;pre class=&#34;mermaid&#34;&gt;&#xA;stateDiagram-v2&#xA;    direction LR&#xA;    z1: $$z_1$$&#xA;    z2: $$z_2$$&#xA;    zi: $$z_i$$&#xA;    zM: $$...$$&#xA;    aj: $$a_j=\sum_i w_{ji} z_i$$&#xA;    zj: $$z_j=h(a_j)$$&#xA;    START1:::hidden --&gt; z1&#xA;    START2:::hidden --&gt; z2&#xA;    STARTi:::hidden --&gt; zi&#xA;    STARTM:::hidden --&gt; zM&#xA;    z1 --&gt; aj&#xA;    z2 --&gt; aj&#xA;    zi --&gt; aj:$$w_{ji}$$&#xA;    zM --&gt; aj&#xA;    aj --&gt; zj&#xA;    zj --&gt; END:::hidden&#xA;    note left of aj : Pre-activation&#xA;    note left of zj : Activation&#xA;    classDef hidden display: none;&#xA;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;Let&amp;rsquo;s consider a hidden unit in a general feed forward neural nework.&#xA;&lt;span&gt;&#xA;  \[&#xA;a_j=\sum_i w_{ji} z_i&#xA;\]&#xA;&lt;/span&gt;&#xA;&#xA;where &lt;span&gt;&#xA;  \(z_i\)&#xA;&lt;/span&gt;&#xA; is the activation of anoter unit or an input that sends an connection of unit &lt;span&gt;&#xA;  \(j\)&#xA;&lt;/span&gt;&#xA; and &lt;span&gt;&#xA;  \(w_{ji}\)&#xA;&lt;/span&gt;&#xA; is the weight associated with that connection. &lt;span&gt;&#xA;  \(a_j\)&#xA;&lt;/span&gt;&#xA; is known as &lt;strong&gt;pre-activation&lt;/strong&gt; and is transformed by a non-linear activation fucntion to give the &lt;strong&gt;activation&lt;/strong&gt; &lt;span&gt;&#xA;  \(z_j\)&#xA;&lt;/span&gt;&#xA; of unit &lt;span&gt;&#xA;  \(j\)&#xA;&lt;/span&gt;&#xA;.&#xA;&lt;span&gt;&#xA;  \[&#xA;z_j=h(a_j)&#xA;\]&#xA;&lt;/span&gt;&#xA;&#xA;For any given data point in the traning set, we can pass the input and compute the activations of all the hidden and output units. This process is called &lt;strong&gt;forward propagation&lt;/strong&gt; since it is the forward flow of information through the network.&lt;/p&gt;</description>
    </item>
    <item>
      <title>Multi-layer perceptrons</title>
      <link>https://example.org/docs/neural_networks/mlp/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://example.org/docs/neural_networks/mlp/</guid>
      <description>&lt;h2 id=&#34;multi-layer-perceptrons&#34;&gt;&#xA;  Multi-layer perceptrons&#xA;  &lt;a class=&#34;anchor&#34; href=&#34;#multi-layer-perceptrons&#34;&gt;#&lt;/a&gt;&#xA;&lt;/h2&gt;</description>
    </item>
    <item>
      <title>Activation functions</title>
      <link>https://example.org/docs/neural_networks/activations/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://example.org/docs/neural_networks/activations/</guid>
      <description>&lt;h2 id=&#34;sigmoid&#34;&gt;&#xA;  Sigmoid&#xA;  &lt;a class=&#34;anchor&#34; href=&#34;#sigmoid&#34;&gt;#&lt;/a&gt;&#xA;&lt;/h2&gt;&#xA;&lt;h2 id=&#34;relu&#34;&gt;&#xA;  ReLU&#xA;  &lt;a class=&#34;anchor&#34; href=&#34;#relu&#34;&gt;#&lt;/a&gt;&#xA;&lt;/h2&gt;&#xA;&lt;h2 id=&#34;gelu&#34;&gt;&#xA;  GeLU&#xA;  &lt;a class=&#34;anchor&#34; href=&#34;#gelu&#34;&gt;#&lt;/a&gt;&#xA;&lt;/h2&gt;&#xA;&lt;h2 id=&#34;glu&#34;&gt;&#xA;  GLU&#xA;  &lt;a class=&#34;anchor&#34; href=&#34;#glu&#34;&gt;#&lt;/a&gt;&#xA;&lt;/h2&gt;&#xA;&lt;h2 id=&#34;swish&#34;&gt;&#xA;  Swish&#xA;  &lt;a class=&#34;anchor&#34; href=&#34;#swish&#34;&gt;#&lt;/a&gt;&#xA;&lt;/h2&gt;&#xA;&lt;h2 id=&#34;swiglu&#34;&gt;&#xA;  SwiGLU&#xA;  &lt;a class=&#34;anchor&#34; href=&#34;#swiglu&#34;&gt;#&lt;/a&gt;&#xA;&lt;/h2&gt;&#xA;&lt;p&gt;&lt;a href=&#34;https://arxiv.org/pdf/2002.05202&#34;&gt;GLU Variants Improve Transformer&lt;/a&gt;&lt;/p&gt;&#xA;&lt;blockquote&gt;&#xA;&lt;p&gt;We offer no explanation as to why these architectures seem to work; we attribute their success, as all else, to divine benevolence&lt;/p&gt;&lt;/blockquote&gt;</description>
    </item>
    <item>
      <title>Normalization</title>
      <link>https://example.org/docs/training/normalization/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://example.org/docs/training/normalization/</guid>
      <description>&lt;h2 id=&#34;batch-normalization&#34;&gt;&#xA;  Batch normalization&#xA;  &lt;a class=&#34;anchor&#34; href=&#34;#batch-normalization&#34;&gt;#&lt;/a&gt;&#xA;&lt;/h2&gt;&#xA;&lt;img src=&#34;https://vikasraykar.github.io/deeplearning/batch.jpeg&#34; alt=&#34;Batch normalization&#34; width=&#34;400&#34;/&gt;&#xA;&lt;p&gt;In batch normalization the mean and variance are computed across the mini-batch separately for each feature/hidden unit. For a mini-batch of size B&#xA;&#xA;&lt;link rel=&#34;stylesheet&#34; href=&#34;https://example.org/katex/katex.min.css&#34; /&gt;&#xA;&lt;script defer src=&#34;https://example.org/katex/katex.min.js&#34;&gt;&lt;/script&gt;&#xA;&lt;script defer src=&#34;https://example.org/katex/auto-render.min.js&#34; onload=&#34;renderMathInElement(document.body);&#34;&gt;&lt;/script&gt;&lt;span&gt;&#xA;  \[&#xA;\mu_i = \frac{1}{B} \sum_{n=1}^{B} a_{ni}&#xA;\]&#xA;&lt;/span&gt;&#xA;&#xA;&lt;span&gt;&#xA;  \[&#xA;\sigma_i^2 = \frac{1}{B} \sum_{n=1}^{B} (a_{ni}-\mu_i)^2&#xA;\]&#xA;&lt;/span&gt;&#xA;&#xA;We normalize the pre-activations as follows.&#xA;&lt;span&gt;&#xA;  \[&#xA;\hat{a}_{ni} = \frac{a_{ni}-\mu_i}{\sqrt{\sigma_i^2+\delta}}&#xA;\]&#xA;&lt;/span&gt;&#xA;&#xA;&lt;span&gt;&#xA;  \[&#xA;\tilde{a}_{ni} = \gamma_i \hat{a}_{ni} + \beta_i&#xA;\]&#xA;&lt;/span&gt;&#xA;&lt;/p&gt;&#xA;&lt;a  href=&#34;https://pytorch.org/docs/stable/generated/torch.nn.BatchNorm1d.html#torch.nn.BatchNorm1d&#34;   target=&#34;_blank&#34; rel=&#34;noopener&#34;  class=&#34;book-btn&#34;&gt;PyTorch&lt;/a&gt;&#xA;&#xA;&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#272822;background-color:#fafafa;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#111&#34;&gt;m&lt;/span&gt; &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; &lt;span style=&#34;color:#111&#34;&gt;nn&lt;/span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;&lt;span style=&#34;color:#111&#34;&gt;BatchNorm1d&lt;/span&gt;&lt;span style=&#34;color:#111&#34;&gt;(&lt;/span&gt;&lt;span style=&#34;color:#111&#34;&gt;num_features&lt;/span&gt;&lt;span style=&#34;color:#111&#34;&gt;)&lt;/span&gt;&#xA;&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;h2 id=&#34;layer-normalization&#34;&gt;&#xA;  Layer normalization&#xA;  &lt;a class=&#34;anchor&#34; href=&#34;#layer-normalization&#34;&gt;#&lt;/a&gt;&#xA;&lt;/h2&gt;&#xA;&lt;img src=&#34;https://vikasraykar.github.io/deeplearning/layer.jpeg&#34; alt=&#34;Layer normalization&#34; width=&#34;300&#34;/&gt;&#xA;&lt;p&gt;In layer normalization the mean and variance are computed across the feature/hidden unit for each example seprately.&#xA;&lt;span&gt;&#xA;  \[&#xA;\mu_n = \frac{1}{M} \sum_{i=1}^{M} a_{ni}&#xA;\]&#xA;&lt;/span&gt;&#xA;&#xA;&lt;span&gt;&#xA;  \[&#xA;\sigma_n^2 = \frac{1}{M} \sum_{i=1}^{M} (a_{ni}-\mu_i)^2&#xA;\]&#xA;&lt;/span&gt;&#xA;&#xA;We normalize the pre-activations as follows.&#xA;&lt;span&gt;&#xA;  \[&#xA;\hat{a}_{ni} = \frac{a_{ni}-\mu_n}{\sqrt{\sigma_n^2+\delta}}&#xA;\]&#xA;&lt;/span&gt;&#xA;&#xA;&lt;span&gt;&#xA;  \[&#xA;\tilde{a}_{ni} = \gamma_n \hat{a}_{ni} + \beta_n&#xA;\]&#xA;&lt;/span&gt;&#xA;&lt;/p&gt;</description>
    </item>
    <item>
      <title>Regularization</title>
      <link>https://example.org/docs/training/regularization/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://example.org/docs/training/regularization/</guid>
      <description>&lt;h2 id=&#34;dropout&#34;&gt;&#xA;  Dropout&#xA;  &lt;a class=&#34;anchor&#34; href=&#34;#dropout&#34;&gt;#&lt;/a&gt;&#xA;&lt;/h2&gt;&#xA;&lt;h2 id=&#34;early-stopping&#34;&gt;&#xA;  Early stopping&#xA;  &lt;a class=&#34;anchor&#34; href=&#34;#early-stopping&#34;&gt;#&lt;/a&gt;&#xA;&lt;/h2&gt;</description>
    </item>
    <item>
      <title>Training loop</title>
      <link>https://example.org/docs/training/training_loop/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://example.org/docs/training/training_loop/</guid>
      <description>&lt;h2 id=&#34;training-loop&#34;&gt;&#xA;  Training loop&#xA;  &lt;a class=&#34;anchor&#34; href=&#34;#training-loop&#34;&gt;#&lt;/a&gt;&#xA;&lt;/h2&gt;&#xA;&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#272822;background-color:#fafafa;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#75715e&#34;&gt;# Load the dataset.&lt;/span&gt;&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#111&#34;&gt;train_dataset&lt;/span&gt; &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; &lt;span style=&#34;color:#111&#34;&gt;SampleDataset&lt;/span&gt;&lt;span style=&#34;color:#111&#34;&gt;(&lt;/span&gt;&lt;span style=&#34;color:#111&#34;&gt;X_train&lt;/span&gt;&lt;span style=&#34;color:#111&#34;&gt;,&lt;/span&gt; &lt;span style=&#34;color:#111&#34;&gt;y_train&lt;/span&gt;&lt;span style=&#34;color:#111&#34;&gt;)&lt;/span&gt;&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#111&#34;&gt;test_dataset&lt;/span&gt; &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; &lt;span style=&#34;color:#111&#34;&gt;SampleDataset&lt;/span&gt;&lt;span style=&#34;color:#111&#34;&gt;(&lt;/span&gt;&lt;span style=&#34;color:#111&#34;&gt;X_test&lt;/span&gt;&lt;span style=&#34;color:#111&#34;&gt;,&lt;/span&gt; &lt;span style=&#34;color:#111&#34;&gt;y_test&lt;/span&gt;&lt;span style=&#34;color:#111&#34;&gt;)&lt;/span&gt;&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#75715e&#34;&gt;# Preparing your data for training with DataLoaders.&lt;/span&gt;&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#111&#34;&gt;batch_size&lt;/span&gt; &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; &lt;span style=&#34;color:#ae81ff&#34;&gt;64&lt;/span&gt;&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#111&#34;&gt;train_dataloader&lt;/span&gt; &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; &lt;span style=&#34;color:#111&#34;&gt;DataLoader&lt;/span&gt;&lt;span style=&#34;color:#111&#34;&gt;(&lt;/span&gt;&lt;span style=&#34;color:#111&#34;&gt;train_dataset&lt;/span&gt;&lt;span style=&#34;color:#111&#34;&gt;,&lt;/span&gt; &lt;span style=&#34;color:#111&#34;&gt;batch_size&lt;/span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#111&#34;&gt;batch_size&lt;/span&gt;&lt;span style=&#34;color:#111&#34;&gt;,&lt;/span&gt; &lt;span style=&#34;color:#111&#34;&gt;shuffle&lt;/span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#00a8c8&#34;&gt;True&lt;/span&gt;&lt;span style=&#34;color:#111&#34;&gt;)&lt;/span&gt;&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#111&#34;&gt;test_dataloader&lt;/span&gt; &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; &lt;span style=&#34;color:#111&#34;&gt;DataLoader&lt;/span&gt;&lt;span style=&#34;color:#111&#34;&gt;(&lt;/span&gt;&lt;span style=&#34;color:#111&#34;&gt;test_dataset&lt;/span&gt;&lt;span style=&#34;color:#111&#34;&gt;,&lt;/span&gt; &lt;span style=&#34;color:#111&#34;&gt;batch_size&lt;/span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#111&#34;&gt;batch_size&lt;/span&gt;&lt;span style=&#34;color:#111&#34;&gt;,&lt;/span&gt; &lt;span style=&#34;color:#111&#34;&gt;shuffle&lt;/span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#00a8c8&#34;&gt;True&lt;/span&gt;&lt;span style=&#34;color:#111&#34;&gt;)&lt;/span&gt;&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#75715e&#34;&gt;# Define the model class.&lt;/span&gt;&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#111&#34;&gt;model&lt;/span&gt; &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; &lt;span style=&#34;color:#111&#34;&gt;LogisticRegression&lt;/span&gt;&lt;span style=&#34;color:#111&#34;&gt;(&lt;/span&gt;&lt;span style=&#34;color:#111&#34;&gt;num_features&lt;/span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#111&#34;&gt;d&lt;/span&gt;&lt;span style=&#34;color:#111&#34;&gt;)&lt;/span&gt;&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#75715e&#34;&gt;# Loss fucntion.&lt;/span&gt;&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#111&#34;&gt;loss_fn&lt;/span&gt; &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; &lt;span style=&#34;color:#111&#34;&gt;nn&lt;/span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;&lt;span style=&#34;color:#111&#34;&gt;BCELoss&lt;/span&gt;&lt;span style=&#34;color:#111&#34;&gt;()&lt;/span&gt;&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#75715e&#34;&gt;# Optimizer.&lt;/span&gt;&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#111&#34;&gt;optimizer&lt;/span&gt; &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; &lt;span style=&#34;color:#111&#34;&gt;SGD&lt;/span&gt;&lt;span style=&#34;color:#111&#34;&gt;(&lt;/span&gt;&lt;span style=&#34;color:#111&#34;&gt;model&lt;/span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;&lt;span style=&#34;color:#111&#34;&gt;parameters&lt;/span&gt;&lt;span style=&#34;color:#111&#34;&gt;(),&lt;/span&gt; &lt;span style=&#34;color:#111&#34;&gt;lr&lt;/span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;0.01&lt;/span&gt;&lt;span style=&#34;color:#111&#34;&gt;,&lt;/span&gt; &lt;span style=&#34;color:#111&#34;&gt;momentum&lt;/span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;0.9&lt;/span&gt;&lt;span style=&#34;color:#111&#34;&gt;)&lt;/span&gt;&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#75715e&#34;&gt;# Learning rate scheduler.&lt;/span&gt;&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#111&#34;&gt;scheduler&lt;/span&gt; &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; &lt;span style=&#34;color:#111&#34;&gt;ExponentialLR&lt;/span&gt;&lt;span style=&#34;color:#111&#34;&gt;(&lt;/span&gt;&lt;span style=&#34;color:#111&#34;&gt;optimizer&lt;/span&gt;&lt;span style=&#34;color:#111&#34;&gt;,&lt;/span&gt; &lt;span style=&#34;color:#111&#34;&gt;gamma&lt;/span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;0.9&lt;/span&gt;&lt;span style=&#34;color:#111&#34;&gt;)&lt;/span&gt;&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#75715e&#34;&gt;# Run for a few epochs.&lt;/span&gt;&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#00a8c8&#34;&gt;for&lt;/span&gt; &lt;span style=&#34;color:#111&#34;&gt;epoch&lt;/span&gt; &lt;span style=&#34;color:#f92672&#34;&gt;in&lt;/span&gt; &lt;span style=&#34;color:#111&#34;&gt;range&lt;/span&gt;&lt;span style=&#34;color:#111&#34;&gt;(&lt;/span&gt;&lt;span style=&#34;color:#111&#34;&gt;n_epochs&lt;/span&gt;&lt;span style=&#34;color:#111&#34;&gt;):&lt;/span&gt;&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#75715e&#34;&gt;# Iterate through the DataLoader to access mini-batches.&lt;/span&gt;&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#00a8c8&#34;&gt;for&lt;/span&gt; &lt;span style=&#34;color:#111&#34;&gt;batch&lt;/span&gt;&lt;span style=&#34;color:#111&#34;&gt;,&lt;/span&gt; &lt;span style=&#34;color:#111&#34;&gt;(&lt;/span&gt;&lt;span style=&#34;color:#111&#34;&gt;input&lt;/span&gt;&lt;span style=&#34;color:#111&#34;&gt;,&lt;/span&gt; &lt;span style=&#34;color:#111&#34;&gt;target&lt;/span&gt;&lt;span style=&#34;color:#111&#34;&gt;)&lt;/span&gt; &lt;span style=&#34;color:#f92672&#34;&gt;in&lt;/span&gt; &lt;span style=&#34;color:#111&#34;&gt;enumerate&lt;/span&gt;&lt;span style=&#34;color:#111&#34;&gt;(&lt;/span&gt;&lt;span style=&#34;color:#111&#34;&gt;train_dataloader&lt;/span&gt;&lt;span style=&#34;color:#111&#34;&gt;):&lt;/span&gt;&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        &lt;span style=&#34;color:#75715e&#34;&gt;# Prediction.&lt;/span&gt;&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        &lt;span style=&#34;color:#111&#34;&gt;output&lt;/span&gt; &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; &lt;span style=&#34;color:#111&#34;&gt;model&lt;/span&gt;&lt;span style=&#34;color:#111&#34;&gt;(&lt;/span&gt;&lt;span style=&#34;color:#111&#34;&gt;input&lt;/span&gt;&lt;span style=&#34;color:#111&#34;&gt;)&lt;/span&gt;&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        &lt;span style=&#34;color:#75715e&#34;&gt;# Compute loss.&lt;/span&gt;&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        &lt;span style=&#34;color:#111&#34;&gt;loss&lt;/span&gt; &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; &lt;span style=&#34;color:#111&#34;&gt;loss_fn&lt;/span&gt;&lt;span style=&#34;color:#111&#34;&gt;(&lt;/span&gt;&lt;span style=&#34;color:#111&#34;&gt;output&lt;/span&gt;&lt;span style=&#34;color:#111&#34;&gt;,&lt;/span&gt; &lt;span style=&#34;color:#111&#34;&gt;target&lt;/span&gt;&lt;span style=&#34;color:#111&#34;&gt;)&lt;/span&gt;&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        &lt;span style=&#34;color:#75715e&#34;&gt;# Compute gradient.&lt;/span&gt;&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        &lt;span style=&#34;color:#111&#34;&gt;loss&lt;/span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;&lt;span style=&#34;color:#111&#34;&gt;backward&lt;/span&gt;&lt;span style=&#34;color:#111&#34;&gt;()&lt;/span&gt;&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        &lt;span style=&#34;color:#75715e&#34;&gt;# Gradient descent.&lt;/span&gt;&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        &lt;span style=&#34;color:#111&#34;&gt;optimizer&lt;/span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;&lt;span style=&#34;color:#111&#34;&gt;step&lt;/span&gt;&lt;span style=&#34;color:#111&#34;&gt;()&lt;/span&gt;&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        &lt;span style=&#34;color:#75715e&#34;&gt;# Prevent gradient accumulation&lt;/span&gt;&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        &lt;span style=&#34;color:#111&#34;&gt;optimizer&lt;/span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;&lt;span style=&#34;color:#111&#34;&gt;zero_grad&lt;/span&gt;&lt;span style=&#34;color:#111&#34;&gt;()&lt;/span&gt;&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#75715e&#34;&gt;# Adjust learning rate&lt;/span&gt;&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#111&#34;&gt;scheduler&lt;/span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;&lt;span style=&#34;color:#111&#34;&gt;step&lt;/span&gt;&lt;span style=&#34;color:#111&#34;&gt;()&lt;/span&gt;&#xA;&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;</description>
    </item>
    <item>
      <title>Quiz</title>
      <link>https://example.org/docs/training/quiz/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://example.org/docs/training/quiz/</guid>
      <description>&lt;h2 id=&#34;quiz&#34;&gt;&#xA;  Quiz&#xA;  &lt;a class=&#34;anchor&#34; href=&#34;#quiz&#34;&gt;#&lt;/a&gt;&#xA;&lt;/h2&gt;&#xA;&#xA;&lt;link rel=&#34;stylesheet&#34; href=&#34;https://example.org/katex/katex.min.css&#34; /&gt;&#xA;&lt;script defer src=&#34;https://example.org/katex/katex.min.js&#34;&gt;&lt;/script&gt;&#xA;&lt;script defer src=&#34;https://example.org/katex/auto-render.min.js&#34; onload=&#34;renderMathInElement(document.body);&#34;&gt;&lt;/script&gt;&lt;span&gt;&#xA;  \(\)&#xA;&lt;/span&gt;&#xA;&#xA;&lt;blockquote&gt;&#xA;&lt;p&gt;Derive the gradient of the loss function for linear regression and logistic regression.&lt;/p&gt;&lt;/blockquote&gt;&#xA;&lt;blockquote&gt;&#xA;&lt;p&gt;What is the most widely used optimizer ? What are the typically used parameters of the optimizer ?&lt;/p&gt;&lt;/blockquote&gt;&#xA;&lt;blockquote&gt;&#xA;&lt;p&gt;For SGD with momemtum show that it increases the effective learning rate from &lt;span&gt;&#xA;  \(\eta\)&#xA;&lt;/span&gt;&#xA; to &lt;span&gt;&#xA;  \(\frac{\eta}{(1-\mu)}\)&#xA;&lt;/span&gt;&#xA;.&lt;/p&gt;&lt;/blockquote&gt;&#xA;&lt;blockquote&gt;&#xA;&lt;p&gt;In &lt;a href=&#34;https://arxiv.org/abs/1706.03762&#34;&gt;Attention Is All You Need&lt;/a&gt; paper what is the optimizer and the learning rate scheduler used ?&lt;/p&gt;</description>
    </item>
    <item>
      <title>Coding</title>
      <link>https://example.org/docs/training/coding/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://example.org/docs/training/coding/</guid>
      <description>&lt;h2 id=&#34;coding-assignment&#34;&gt;&#xA;  Coding assignment&#xA;  &lt;a class=&#34;anchor&#34; href=&#34;#coding-assignment&#34;&gt;#&lt;/a&gt;&#xA;&lt;/h2&gt;&#xA;&#xA;&lt;link rel=&#34;stylesheet&#34; href=&#34;https://example.org/katex/katex.min.css&#34; /&gt;&#xA;&lt;script defer src=&#34;https://example.org/katex/katex.min.js&#34;&gt;&lt;/script&gt;&#xA;&lt;script defer src=&#34;https://example.org/katex/auto-render.min.js&#34; onload=&#34;renderMathInElement(document.body);&#34;&gt;&lt;/script&gt;&lt;span&gt;&#xA;  \(\)&#xA;&lt;/span&gt;&#xA;&#xA;&lt;h3 id=&#34;setup&#34;&gt;&#xA;  Setup&#xA;  &lt;a class=&#34;anchor&#34; href=&#34;#setup&#34;&gt;#&lt;/a&gt;&#xA;&lt;/h3&gt;&#xA;&lt;p&gt;&lt;a href=&#34;https://github.com/vikasraykar/deeplearning-dojo/&#34;&gt;https://github.com/vikasraykar/deeplearning-dojo/&lt;/a&gt;&lt;/p&gt;&#xA;&lt;pre tabindex=&#34;0&#34;&gt;&lt;code&gt;git clone https://github.com/vikasraykar/deeplearning-dojo.git&#xA;cd deeplearning-dojo&#xA;&#xA;python -m venv .env&#xA;source .env/bin/activate&#xA;pip install -r requirements.txt&#xA;&lt;/code&gt;&lt;/pre&gt;&lt;h3 id=&#34;problem-1&#34;&gt;&#xA;  Problem 1&#xA;  &lt;a class=&#34;anchor&#34; href=&#34;#problem-1&#34;&gt;#&lt;/a&gt;&#xA;&lt;/h3&gt;&#xA;&lt;blockquote&gt;&#xA;&lt;p&gt;Linear Regression with numpy and batch gradient descent.&lt;/p&gt;&lt;/blockquote&gt;&#xA;&lt;p&gt;In the first coding assigment you will be implementing a basic &lt;strong&gt;Linear Regression&lt;/strong&gt; model from scratch using &lt;strong&gt;only &lt;code&gt;numpy&lt;/code&gt;&lt;/strong&gt;. You will be implementing a basic batch gradient descent optimizer.&lt;/p&gt;&#xA;&lt;blockquote class=&#34;book-hint danger&#34;&gt;&#xA;&lt;p&gt;You can use only &lt;code&gt;numpy&lt;/code&gt; and are not allowed to to use &lt;code&gt;torch&lt;/code&gt; or any other python libraries.&lt;/p&gt;</description>
    </item>
  </channel>
</rss>
