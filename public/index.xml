<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Deep Learning</title>
    <link>http://localhost:1313/</link>
    <description>Recent content on Deep Learning</description>
    <generator>Hugo</generator>
    <language>en-us</language>
    <atom:link href="http://localhost:1313/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>Basics</title>
      <link>http://localhost:1313/docs/rl/basics/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>http://localhost:1313/docs/rl/basics/</guid>
      <description>&lt;h1 id=&#34;reinforcement-learning-basics&#34;&gt;&#xA;  Reinforcement learning basics&#xA;  &lt;a class=&#34;anchor&#34; href=&#34;#reinforcement-learning-basics&#34;&gt;#&lt;/a&gt;&#xA;&lt;/h1&gt;&#xA;&lt;a  href=&#34;https://www.dropbox.com/s/4pdr6y60t0r9mm2/2017_07_29_anthill_deep_reinforcement_learning_tutorial.pdf?dl=0&#34;   target=&#34;_blank&#34; rel=&#34;noopener&#34;  class=&#34;book-btn&#34;&gt;slides&lt;/a&gt;&#xA;&#xA;&lt;blockquote&gt;&#xA;&lt;p&gt;Reinforcement Learning (RL) is a natural computational paradigm for agents learning from interaction to achieve a goal. Deep learning (DL) provides a powerful general-purpose representation learning framework. A combination of these two has recently emerged as a strong contender for artificial general intelligence. This tutorial will provide a gentle exposition of RL concepts and DL based RL with a focus on policy gradients.&lt;/p&gt;&lt;/blockquote&gt;&#xA;&lt;span&gt;&#xA;  \(\)&#xA;&lt;/span&gt;&#xA;&#xA;&lt;h2 id=&#34;the-agent-environment-interaction&#34;&gt;&#xA;  The agent-environment interaction&#xA;  &lt;a class=&#34;anchor&#34; href=&#34;#the-agent-environment-interaction&#34;&gt;#&lt;/a&gt;&#xA;&lt;/h2&gt;&#xA;&lt;p&gt;The reinforcement learning (RL) framework is an abstraction of the problem of &lt;strong&gt;goal directed learning from interaction&lt;/strong&gt;.&lt;/p&gt;</description>
    </item>
    <item>
      <title>Gradient Descent</title>
      <link>http://localhost:1313/docs/training/gradient_descent/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>http://localhost:1313/docs/training/gradient_descent/</guid>
      <description>&lt;h2 id=&#34;gradient-descent&#34;&gt;&#xA;  Gradient Descent&#xA;  &lt;a class=&#34;anchor&#34; href=&#34;#gradient-descent&#34;&gt;#&lt;/a&gt;&#xA;&lt;/h2&gt;&#xA;&lt;blockquote&gt;&#xA;&lt;p&gt;Steepest descent.&lt;/p&gt;&lt;/blockquote&gt;&#xA;&lt;p&gt;Let &lt;span&gt;&#xA;  \(\mathbf{w}\)&#xA;&lt;/span&gt;&#xA; be a vector of all the parameters for a model.&lt;/p&gt;&#xA;&lt;p&gt;Let &lt;span&gt;&#xA;  \(L(\mathbf{w})\)&#xA;&lt;/span&gt;&#xA; be the loss function (or error function).&lt;/p&gt;&#xA;&lt;p&gt;We need to choose the model parameters that optimizes (minimizes) the loss function.&lt;/p&gt;&#xA;&lt;span&gt;&#xA;  \[&#xA;\hat{\mathbf{w}} = \argmin_{\mathbf{w}} L(\mathbf{w})&#xA;\]&#xA;&lt;/span&gt;&#xA;&#xA;&lt;p&gt;Let &lt;span&gt;&#xA;  \(\nabla L(\mathbf{w})\)&#xA;&lt;/span&gt;&#xA; be the &lt;strong&gt;gradient vector&lt;/strong&gt;, where each element is the partial derivative of the loss fucntion wrt each parameter.&lt;/p&gt;&#xA;&lt;p&gt;The gradient vector points in the direction of the greatest rate of increase of the loss function.&lt;/p&gt;</description>
    </item>
    <item>
      <title>Linear Regression</title>
      <link>http://localhost:1313/docs/neural_networks/linear_regression/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>http://localhost:1313/docs/neural_networks/linear_regression/</guid>
      <description>&lt;h2 id=&#34;linear-regression&#34;&gt;&#xA;  Linear Regression&#xA;  &lt;a class=&#34;anchor&#34; href=&#34;#linear-regression&#34;&gt;#&lt;/a&gt;&#xA;&lt;/h2&gt;&#xA;&lt;p&gt;Linear Regression is a single layer neural network for regression.&lt;/p&gt;&#xA;&#xA;&#xA;&lt;pre class=&#34;mermaid&#34;&gt;&#xA;stateDiagram-v2&#xA;    direction LR&#xA;    z1: $$x_1$$&#xA;    z2: $$x_2$$&#xA;    zi: $$x_i$$&#xA;    zM: $$x_d$$&#xA;    aj: $$a=\sum_i w_{i} x_i$$&#xA;    zj: $$z=h(a)=a$$&#xA;    z1 --&gt; aj:$$w_{1}$$&#xA;    z2 --&gt; aj:$$w_{2}$$&#xA;    zi --&gt; aj:$$w_{i}$$&#xA;    zM --&gt; aj:$$w_{d}$$&#xA;    aj --&gt; zj&#xA;    zj --&gt; END:::hidden&#xA;    note left of zM : Inputs&#xA;    note left of aj : Pre-activation&#xA;    note left of zj : Activation&#xA;    note left of END : Output&#xA;    classDef hidden display: none;&#xA;&lt;/pre&gt;&#xA;&#xA;&lt;h2 id=&#34;model&#34;&gt;&#xA;  Model&#xA;  &lt;a class=&#34;anchor&#34; href=&#34;#model&#34;&gt;#&lt;/a&gt;&#xA;&lt;/h2&gt;&#xA;&lt;p&gt;Linear Regression assumes a &lt;strong&gt;linear relationship&lt;/strong&gt; between the target &lt;span&gt;&#xA;  \(y \in \mathbb{R}\)&#xA;&lt;/span&gt;&#xA; and the features &lt;span&gt;&#xA;  \(\mathbf{x}\in \mathbb{R}^d\)&#xA;&lt;/span&gt;&#xA;.&#xA;&lt;span&gt;&#xA;  \[&#xA;y = f(\mathbf{x}) = w_1 x_1 + w_2 x_2 + ... + w_d x_d + b = \mathbf{w}^T\mathbf{x} + b,&#xA;\]&#xA;&lt;/span&gt;&#xA;&#xA;where &lt;span&gt;&#xA;  \(\mathbf{w}\in \mathbb{R}^d\)&#xA;&lt;/span&gt;&#xA; is the &lt;span&gt;&#xA;  \(d\)&#xA;&lt;/span&gt;&#xA;-dimensional &lt;em&gt;weight vector&lt;/em&gt; and &lt;span&gt;&#xA;  \(b \in \mathbb{R}\)&#xA;&lt;/span&gt;&#xA; is the &lt;em&gt;bias term&lt;/em&gt;.&lt;/p&gt;</description>
    </item>
    <item>
      <title>Transformers</title>
      <link>http://localhost:1313/docs/transformers/transformers101/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>http://localhost:1313/docs/transformers/transformers101/</guid>
      <description>&lt;p&gt;&lt;a href=&#34;https://github.com/vikasraykar/wiki/wiki/Transformers-101&#34;&gt;https://github.com/vikasraykar/wiki/wiki/Transformers-101&lt;/a&gt;&lt;/p&gt;</description>
    </item>
    <item>
      <title>Alignment</title>
      <link>http://localhost:1313/docs/transformers/alignment/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>http://localhost:1313/docs/transformers/alignment/</guid>
      <description>&lt;h1 id=&#34;alignment&#34;&gt;&#xA;  Alignment&#xA;  &lt;a class=&#34;anchor&#34; href=&#34;#alignment&#34;&gt;#&lt;/a&gt;&#xA;&lt;/h1&gt;&#xA;&lt;p&gt;LLMs are typically trained for next-token prediction.&lt;/p&gt;&#xA;&lt;p&gt;Pre-trained LLMs may not be able to follow user instructions because they were not trained to do so.&lt;/p&gt;&#xA;&lt;p&gt;Pre-trained LLMs may generate harmful content or perpetuate  biases inherent in their training data.&lt;/p&gt;&#xA;&#xA;&#xA;&lt;pre class=&#34;mermaid&#34;&gt;&#xA;---&#xA;title: LLM training stages&#xA;---&#xA;flowchart LR&#xA;    subgraph Pre-training&#xA;    A[Pre-training]&#xA;    end&#xA;    subgraph Post-training&#xA;    B[&#34;Instruction Alignment (SFT)&#34;]&#xA;    C[&#34;Preference Alignment (RLHF)&#34;]&#xA;    end&#xA;    subgraph Inference&#xA;    D[Prompt engineering]&#xA;    end&#xA;    A--&gt;B&#xA;    B--&gt;C&#xA;    C--&gt;D&#xA;&lt;/pre&gt;&#xA;&#xA;&lt;h2 id=&#34;fine-tune-llms-with-labelled-data&#34;&gt;&#xA;  Fine tune LLMs with labelled data&#xA;  &lt;a class=&#34;anchor&#34; href=&#34;#fine-tune-llms-with-labelled-data&#34;&gt;#&lt;/a&gt;&#xA;&lt;/h2&gt;&#xA;&lt;h3 id=&#34;supervised-fine-tuning-sft&#34;&gt;&#xA;  Supervised Fine Tuning (SFT)&#xA;  &lt;a class=&#34;anchor&#34; href=&#34;#supervised-fine-tuning-sft&#34;&gt;#&lt;/a&gt;&#xA;&lt;/h3&gt;&#xA;&lt;p&gt;Training data is task specific instructions paired with their expected outputs.&lt;/p&gt;</description>
    </item>
    <item>
      <title>Backpropagation</title>
      <link>http://localhost:1313/docs/training/backpropagation/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>http://localhost:1313/docs/training/backpropagation/</guid>
      <description>&lt;h1 id=&#34;backpropagation&#34;&gt;&#xA;  Backpropagation&#xA;  &lt;a class=&#34;anchor&#34; href=&#34;#backpropagation&#34;&gt;#&lt;/a&gt;&#xA;&lt;/h1&gt;&#xA;&lt;blockquote&gt;&#xA;&lt;p&gt;Backpropagation, Error Backpropagation, Backprop.&lt;/p&gt;&lt;/blockquote&gt;&#xA;&lt;p&gt;Backpropagation (or backprop) is an efficient technique to compute the gradient of the loss function.&lt;/p&gt;&#xA;&lt;p&gt;It boils down to a local message passing scheme in which information is sent backwards through the network.&lt;/p&gt;&#xA;&lt;h2 id=&#34;forward-propagation&#34;&gt;&#xA;  Forward propagation&#xA;  &lt;a class=&#34;anchor&#34; href=&#34;#forward-propagation&#34;&gt;#&lt;/a&gt;&#xA;&lt;/h2&gt;&#xA;&#xA;&#xA;&lt;pre class=&#34;mermaid&#34;&gt;&#xA;stateDiagram-v2&#xA;    direction LR&#xA;    z1: $$z_1$$&#xA;    z2: $$z_2$$&#xA;    zi: $$z_i$$&#xA;    zM: $$...$$&#xA;    aj: $$a_j=\sum_i w_{ji} z_i$$&#xA;    zj: $$z_j=h(a_j)$$&#xA;    START1:::hidden --&gt; z1&#xA;    START2:::hidden --&gt; z2&#xA;    STARTi:::hidden --&gt; zi&#xA;    STARTM:::hidden --&gt; zM&#xA;    z1 --&gt; aj&#xA;    z2 --&gt; aj&#xA;    zi --&gt; aj:$$w_{ji}$$&#xA;    zM --&gt; aj&#xA;    aj --&gt; zj&#xA;    zj --&gt; END:::hidden&#xA;    note left of aj : Pre-activation&#xA;    note left of zj : Activation&#xA;    classDef hidden display: none;&#xA;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;Let&amp;rsquo;s consider a hidden unit in a general feed forward neural network.&#xA;&lt;span&gt;&#xA;  \[&#xA;a_j=\sum_i w_{ji} z_i&#xA;\]&#xA;&lt;/span&gt;&#xA;&#xA;where &lt;span&gt;&#xA;  \(z_i\)&#xA;&lt;/span&gt;&#xA; is the activation of another unit or an input that sends an connection of unit &lt;span&gt;&#xA;  \(j\)&#xA;&lt;/span&gt;&#xA; and &lt;span&gt;&#xA;  \(w_{ji}\)&#xA;&lt;/span&gt;&#xA; is the weight associated with that connection. The sum &lt;span&gt;&#xA;  \(a_j\)&#xA;&lt;/span&gt;&#xA; is known as &lt;strong&gt;pre-activation&lt;/strong&gt; and is transformed by a non-linear activation function &lt;span&gt;&#xA;  \(h()\)&#xA;&lt;/span&gt;&#xA; to give the &lt;strong&gt;activation&lt;/strong&gt; &lt;span&gt;&#xA;  \(z_j\)&#xA;&lt;/span&gt;&#xA; of unit &lt;span&gt;&#xA;  \(j\)&#xA;&lt;/span&gt;&#xA;.&#xA;&lt;span&gt;&#xA;  \[&#xA;z_j=h(a_j)&#xA;\]&#xA;&lt;/span&gt;&#xA;&#xA;For any given data point in the training set, we can pass the input and compute the activations of all the hidden and output units. This process is called &lt;strong&gt;forward propagation&lt;/strong&gt; since it is the forward flow of information through the network.&lt;/p&gt;</description>
    </item>
    <item>
      <title>Logistic Regression</title>
      <link>http://localhost:1313/docs/neural_networks/logistic_regression/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>http://localhost:1313/docs/neural_networks/logistic_regression/</guid>
      <description>&lt;h2 id=&#34;logistic-regression&#34;&gt;&#xA;  Logistic Regression&#xA;  &lt;a class=&#34;anchor&#34; href=&#34;#logistic-regression&#34;&gt;#&lt;/a&gt;&#xA;&lt;/h2&gt;&#xA;&lt;p&gt;Logistic Regression is a single layer neural network for binary classification.&lt;/p&gt;&#xA;&#xA;&#xA;&lt;pre class=&#34;mermaid&#34;&gt;&#xA;stateDiagram-v2&#xA;    direction LR&#xA;    z1: $$x_1$$&#xA;    z2: $$x_2$$&#xA;    zi: $$x_i$$&#xA;    zM: $$x_d$$&#xA;    aj: $$a=\sum_i w_{i} x_i$$&#xA;    zj: $$z=\sigma(a)$$&#xA;    z1 --&gt; aj:$$w_{1}$$&#xA;    z2 --&gt; aj:$$w_{2}$$&#xA;    zi --&gt; aj:$$w_{i}$$&#xA;    zM --&gt; aj:$$w_{d}$$&#xA;    aj --&gt; zj&#xA;    zj --&gt; END:::hidden&#xA;    note left of zM : Inputs&#xA;    note left of aj : Pre-activation&#xA;    note left of zj : Activation&#xA;    note left of END : Output&#xA;    classDef hidden display: none;&#xA;&lt;/pre&gt;&#xA;&#xA;&lt;h2 id=&#34;statistical-model&#34;&gt;&#xA;  Statistical model&#xA;  &lt;a class=&#34;anchor&#34; href=&#34;#statistical-model&#34;&gt;#&lt;/a&gt;&#xA;&lt;/h2&gt;&#xA;&lt;p&gt;The probability of the positive class (&lt;span&gt;&#xA;  \(y=1\)&#xA;&lt;/span&gt;&#xA;) for a given feature vector (&lt;span&gt;&#xA;  \(\mathbf{x}\in \mathbb{R}^d\)&#xA;&lt;/span&gt;&#xA;) is given by&#xA;&lt;span&gt;&#xA;  \[&#xA;\text{Pr}[y=1|\mathbf{x},\mathbf{w}] = \sigma(\mathbf{w}^T\mathbf{x})&#xA;\]&#xA;&lt;/span&gt;&#xA;&#xA;where &lt;span&gt;&#xA;  \(\mathbf{w}\in \mathbb{R}^d\)&#xA;&lt;/span&gt;&#xA; are the weights/&lt;strong&gt;parameters&lt;/strong&gt; of the model and &lt;span&gt;&#xA;  \(\sigma\)&#xA;&lt;/span&gt;&#xA; is the &lt;strong&gt;sigmoid&lt;/strong&gt; activation function defined as&#xA;&lt;span&gt;&#xA;  \[&#xA;\sigma(x) = \frac{1}{1-e^{-z}}&#xA;\]&#xA;&lt;/span&gt;&#xA;&lt;/p&gt;</description>
    </item>
    <item>
      <title>Automatic Differentiation</title>
      <link>http://localhost:1313/docs/training/autograd/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>http://localhost:1313/docs/training/autograd/</guid>
      <description>&lt;h1 id=&#34;automatic-differentiation&#34;&gt;&#xA;  Automatic differentiation&#xA;  &lt;a class=&#34;anchor&#34; href=&#34;#automatic-differentiation&#34;&gt;#&lt;/a&gt;&#xA;&lt;/h1&gt;&#xA;&lt;blockquote&gt;&#xA;&lt;p&gt;Algorithmic differentiation, autodiff, autograd&lt;/p&gt;&lt;/blockquote&gt;&#xA;&lt;p&gt;There are broadly 4 appoaches to compute derivatives.&lt;/p&gt;&#xA;&lt;table&gt;&#xA;  &lt;thead&gt;&#xA;      &lt;tr&gt;&#xA;          &lt;th&gt;Approach&lt;/th&gt;&#xA;          &lt;th&gt;Pros&lt;/th&gt;&#xA;          &lt;th&gt;Cons&lt;/th&gt;&#xA;      &lt;/tr&gt;&#xA;  &lt;/thead&gt;&#xA;  &lt;tbody&gt;&#xA;      &lt;tr&gt;&#xA;          &lt;td&gt;&lt;strong&gt;Manual&lt;/strong&gt; derivation of backprop equations.&lt;/td&gt;&#xA;          &lt;td&gt;If done carefully can result in efficient code.&lt;/td&gt;&#xA;          &lt;td&gt;Manual process, prone to errors and not easy to iterate on models&lt;/td&gt;&#xA;      &lt;/tr&gt;&#xA;      &lt;tr&gt;&#xA;          &lt;td&gt;&lt;strong&gt;Numerical&lt;/strong&gt; evaluation of gradients via finite differences.&lt;/td&gt;&#xA;          &lt;td&gt;Sometimes used to check for correctness of other methods.&lt;/td&gt;&#xA;          &lt;td&gt;Limited by computational accuracy. Scales poorly with the size of the network.&lt;/td&gt;&#xA;      &lt;/tr&gt;&#xA;      &lt;tr&gt;&#xA;          &lt;td&gt;&lt;strong&gt;Symbolic&lt;/strong&gt; differentiation using packages like &lt;code&gt;sympy&lt;/code&gt;&lt;/td&gt;&#xA;          &lt;td&gt;&lt;/td&gt;&#xA;          &lt;td&gt;Closed form needed. Resulting expression can be very long (&lt;em&gt;expression swell&lt;/em&gt;).&lt;/td&gt;&#xA;      &lt;/tr&gt;&#xA;      &lt;tr&gt;&#xA;          &lt;td&gt;&lt;strong&gt;Automatic differentiation&lt;/strong&gt;&lt;/td&gt;&#xA;          &lt;td&gt;Most preferred.&lt;/td&gt;&#xA;          &lt;td&gt;&lt;/td&gt;&#xA;      &lt;/tr&gt;&#xA;  &lt;/tbody&gt;&#xA;&lt;/table&gt;&#xA;&lt;blockquote class=&#34;book-hint warning&#34;&gt;&#xA;&lt;p&gt;Atılım Günes Baydin, Barak A. Pearlmutter, Alexey Andreyevich Radul, and Jeffrey Mark Siskind. 2017. &lt;a href=&#34;https://dl.acm.org/doi/pdf/10.5555/3122009.3242010&#34;&gt;Automatic differentiation in machine learning: a survey.&lt;/a&gt; J. Mach. Learn. Res. 18, 1 (January 2017), 5595–5637.&lt;/p&gt;</description>
    </item>
    <item>
      <title>Initialization</title>
      <link>http://localhost:1313/docs/training/initialization/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>http://localhost:1313/docs/training/initialization/</guid>
      <description>&lt;h2 id=&#34;parameter-initialization&#34;&gt;&#xA;  Parameter initialization&#xA;  &lt;a class=&#34;anchor&#34; href=&#34;#parameter-initialization&#34;&gt;#&lt;/a&gt;&#xA;&lt;/h2&gt;&#xA;&lt;p&gt;Initialization before starting the gradient descent.&lt;/p&gt;&#xA;&lt;p&gt;Avoid all parameters set to same value. (&lt;strong&gt;symmetry breaking&lt;/strong&gt;)&lt;/p&gt;&#xA;&lt;p&gt;Uniform distribution in the range &lt;span&gt;&#xA;  \([-\epsilon,\epsilon]\)&#xA;&lt;/span&gt;&#xA;&lt;/p&gt;&#xA;&lt;p&gt;Zero-mean Gaussian &lt;span&gt;&#xA;  \(\mathcal{N}(0,\epsilon^2)\)&#xA;&lt;/span&gt;&#xA;&lt;/p&gt;&#xA;&lt;a  href=&#34;https://pytorch.org/docs/stable/nn.init.html&#34;   target=&#34;_blank&#34; rel=&#34;noopener&#34;  class=&#34;book-btn&#34;&gt;nn.init&lt;/a&gt;</description>
    </item>
    <item>
      <title>Softmax Regression</title>
      <link>http://localhost:1313/docs/neural_networks/softmax_regression/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>http://localhost:1313/docs/neural_networks/softmax_regression/</guid>
      <description>&lt;h2 id=&#34;softmax-regression&#34;&gt;&#xA;  Softmax Regression&#xA;  &lt;a class=&#34;anchor&#34; href=&#34;#softmax-regression&#34;&gt;#&lt;/a&gt;&#xA;&lt;/h2&gt;&#xA;&lt;p&gt;Softmax Regression is a single layer neural network for multi-class classification.&lt;/p&gt;&#xA;&#xA;&#xA;&lt;pre class=&#34;mermaid&#34;&gt;&#xA;stateDiagram-v2&#xA;    direction LR&#xA;    x1: $$x_1$$&#xA;    x2: $$x_2$$&#xA;    x3: $$x_3$$&#xA;    xd: $$x_d$$&#xA;    a1: $$a_1=\sum_i w_{1i} x_i$$&#xA;    a2: $$a_2=\sum_i w_{2i} x_i$$&#xA;    ak: $$a_k=\sum_i w_{ki} x_i$$&#xA;    z1: $$z_1=\text{softmax}(\mathbf{a})_1$$&#xA;    z2: $$z_2=\text{softmax}(\mathbf{a})_2$$&#xA;    zk: $$z_k=\text{softmax}(\mathbf{a})_k$$&#xA;    x1 --&gt; a1:$$w_{11}$$&#xA;    x2 --&gt; a1:$$w_{12}$$&#xA;    x3 --&gt; a1:$$w_{13}$$&#xA;    xd --&gt; a1:$$w_{1d}$$&#xA;    x1 --&gt; a2:$$w_{21}$$&#xA;    x2 --&gt; a2:$$w_{22}$$&#xA;    x3 --&gt; a2:$$w_{23}$$&#xA;    xd --&gt; a2:$$w_{2d}$$&#xA;    x1 --&gt; ak:$$w_{k1}$$&#xA;    x2 --&gt; ak:$$w_{k2}$$&#xA;    x3 --&gt; ak:$$w_{k3}$$&#xA;    xd --&gt; ak:$$w_{kd}$$&#xA;    a1 --&gt; z1&#xA;    a2 --&gt; z2&#xA;    ak --&gt; zk&#xA;    z1 --&gt; END1:::hidden&#xA;    z2 --&gt; END2:::hidden&#xA;    zk --&gt; END:::hidden&#xA;    note left of xd : Inputs&#xA;    note right of a1 : Pre-activations&#xA;    note left of zk : Activations&#xA;    note left of END : Outputs&#xA;    classDef hidden display: none;&#xA;&lt;/pre&gt;&#xA;&#xA;&lt;h2 id=&#34;statistical-model&#34;&gt;&#xA;  Statistical model&#xA;  &lt;a class=&#34;anchor&#34; href=&#34;#statistical-model&#34;&gt;#&lt;/a&gt;&#xA;&lt;/h2&gt;&#xA;&lt;p&gt;Given &lt;span&gt;&#xA;  \(k\)&#xA;&lt;/span&gt;&#xA; classes the probability of class &lt;span&gt;&#xA;  \(i\)&#xA;&lt;/span&gt;&#xA; for a given feature vector (&lt;span&gt;&#xA;  \(\mathbf{x}\in \mathbb{R}^d\)&#xA;&lt;/span&gt;&#xA;) is given by&#xA;&lt;span&gt;&#xA;  \[&#xA;\text{Pr}[y=i|\mathbf{x},(\mathbf{w}_1,\mathbf{w}_2,...,\mathbf{w}_k)] = \frac{\exp(\mathbf{w}_i^T\mathbf{x})}{\sum_{j=1}^{k} \exp(\mathbf{w}_j^T\mathbf{x})}&#xA;\]&#xA;&lt;/span&gt;&#xA;&#xA;where &lt;span&gt;&#xA;  \(\mathbf{w}_1,\mathbf{w}_2,...,\mathbf{w}_k\in \mathbb{R}^d\)&#xA;&lt;/span&gt;&#xA; are the weight vector or &lt;strong&gt;parameters&lt;/strong&gt; of the model for each class.&lt;/p&gt;</description>
    </item>
    <item>
      <title>Multilayer perceptron</title>
      <link>http://localhost:1313/docs/neural_networks/mlp/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>http://localhost:1313/docs/neural_networks/mlp/</guid>
      <description>&lt;h2 id=&#34;multilayer-perceptron&#34;&gt;&#xA;  Multilayer perceptron&#xA;  &lt;a class=&#34;anchor&#34; href=&#34;#multilayer-perceptron&#34;&gt;#&lt;/a&gt;&#xA;&lt;/h2&gt;&#xA;&lt;p&gt;A 3-layer multilayer perceptron.&#xA;&lt;span&gt;&#xA;  \[&#xA;\begin{align}&#xA;\mathbf{X}  &amp;= \mathbf{X}  \nonumber \\&#xA;\mathbf{H}^{(1)} &amp;= g_1\left(\mathbf{X}\mathbf{W}^{(1)}+\mathbf{b}^{(1)}\right) \nonumber \\&#xA;\mathbf{H}^{(2)} &amp;= g_2\left(\mathbf{H}^{(1)}\mathbf{W}^{(2)}+\mathbf{b}^{(2)}\right) \nonumber \\&#xA;\mathbf{O} &amp;= \mathbf{H}^{(2)}\mathbf{W}^{(3)}+\mathbf{b}^{(3)} \nonumber \\&#xA;\end{align}&#xA;\]&#xA;&lt;/span&gt;&#xA;&lt;/p&gt;&#xA;&lt;p&gt;&lt;span&gt;&#xA;  \(g\)&#xA;&lt;/span&gt;&#xA; is a nonlinear &lt;strong&gt;activation function&lt;/strong&gt;&lt;/p&gt;&#xA;&#xA;&#xA;&lt;pre class=&#34;mermaid&#34;&gt;&#xA;stateDiagram-v2&#xA;    direction LR&#xA;    x1: $$x_1$$&#xA;    x2: $$x_2$$&#xA;    x3: $$x_3$$&#xA;    h11: $$h^1_1$$&#xA;    h12: $$h^1_2$$&#xA;    h13: $$h^1_3$$&#xA;    h14: $$h^1_4$$&#xA;    h21: $$h^2_1$$&#xA;    h22: $$h^2_2$$&#xA;    h23: $$h^2_3$$&#xA;    h24: $$h^2_4$$&#xA;    h25: $$h^2_5$$&#xA;    o1: $$o_1$$&#xA;    o2: $$o_2$$&#xA;    o3: $$o_3$$&#xA;    x1 --&gt; h11&#xA;    x1 --&gt; h12&#xA;    x1 --&gt; h13&#xA;    x1 --&gt; h14&#xA;    x2 --&gt; h11&#xA;    x2 --&gt; h12&#xA;    x2 --&gt; h13&#xA;    x2 --&gt; h14&#xA;    x3 --&gt; h11&#xA;    x3 --&gt; h12&#xA;    x3 --&gt; h13&#xA;    x3 --&gt; h14&#xA;    h11 --&gt; h21&#xA;    h11 --&gt; h22&#xA;    h11 --&gt; h23&#xA;    h11 --&gt; h24&#xA;    h11 --&gt; h25&#xA;    h12 --&gt; h21&#xA;    h12 --&gt; h22&#xA;    h12 --&gt; h23&#xA;    h12 --&gt; h24&#xA;    h12 --&gt; h25&#xA;    h13 --&gt; h21&#xA;    h13 --&gt; h22&#xA;    h13 --&gt; h23&#xA;    h13 --&gt; h24&#xA;    h13 --&gt; h25&#xA;    h14 --&gt; h21&#xA;    h14 --&gt; h22&#xA;    h14 --&gt; h23&#xA;    h14 --&gt; h24&#xA;    h14 --&gt; h25&#xA;    h21 --&gt; o1&#xA;    h22 --&gt; o1&#xA;    h23 --&gt; o1&#xA;    h24 --&gt; o1&#xA;    h25 --&gt; o1&#xA;    h21 --&gt; o2&#xA;    h22 --&gt; o2&#xA;    h23 --&gt; o2&#xA;    h24 --&gt; o2&#xA;    h25 --&gt; o2&#xA;    h21 --&gt; o3&#xA;    h22 --&gt; o3&#xA;    h23 --&gt; o3&#xA;    h24 --&gt; o3&#xA;    h25 --&gt; o3&#xA;    o1 --&gt; END1:::hidden&#xA;    o2 --&gt; END2:::hidden&#xA;    o3 --&gt; END3:::hidden&#xA;    note left of x3 : Input layer&#xA;    note left of h14 : Hidden layer 1&#xA;    note left of h25 : Hidden layer 2&#xA;    note left of o1 : Output layer&#xA;    classDef hidden display: none;&#xA;&lt;/pre&gt;</description>
    </item>
    <item>
      <title>Normalization</title>
      <link>http://localhost:1313/docs/training/normalization/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>http://localhost:1313/docs/training/normalization/</guid>
      <description>&lt;h2 id=&#34;batch-normalization&#34;&gt;&#xA;  Batch normalization&#xA;  &lt;a class=&#34;anchor&#34; href=&#34;#batch-normalization&#34;&gt;#&lt;/a&gt;&#xA;&lt;/h2&gt;&#xA;&lt;img src=&#34;https://vikasraykar.github.io/deeplearning/batch.jpeg&#34; alt=&#34;Batch normalization&#34; width=&#34;400&#34;/&gt;&#xA;&lt;p&gt;In batch normalization the mean and variance are computed across the mini-batch separately for each feature/hidden unit. For a mini-batch of size B&#xA;&lt;span&gt;&#xA;  \[&#xA;\mu_i = \frac{1}{B} \sum_{n=1}^{B} a_{ni}&#xA;\]&#xA;&lt;/span&gt;&#xA;&#xA;&lt;span&gt;&#xA;  \[&#xA;\sigma_i^2 = \frac{1}{B} \sum_{n=1}^{B} (a_{ni}-\mu_i)^2&#xA;\]&#xA;&lt;/span&gt;&#xA;&#xA;We normalize the pre-activations as follows.&#xA;&lt;span&gt;&#xA;  \[&#xA;\hat{a}_{ni} = \frac{a_{ni}-\mu_i}{\sqrt{\sigma_i^2+\delta}}&#xA;\]&#xA;&lt;/span&gt;&#xA;&#xA;&lt;span&gt;&#xA;  \[&#xA;\tilde{a}_{ni} = \gamma_i \hat{a}_{ni} + \beta_i&#xA;\]&#xA;&lt;/span&gt;&#xA;&lt;/p&gt;&#xA;&lt;a  href=&#34;https://pytorch.org/docs/stable/generated/torch.nn.BatchNorm1d.html#torch.nn.BatchNorm1d&#34;   target=&#34;_blank&#34; rel=&#34;noopener&#34;  class=&#34;book-btn&#34;&gt;PyTorch&lt;/a&gt;&#xA;&#xA;&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#272822;background-color:#fafafa;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#111&#34;&gt;m&lt;/span&gt; &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; &lt;span style=&#34;color:#111&#34;&gt;nn&lt;/span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;&lt;span style=&#34;color:#111&#34;&gt;BatchNorm1d&lt;/span&gt;&lt;span style=&#34;color:#111&#34;&gt;(&lt;/span&gt;&lt;span style=&#34;color:#111&#34;&gt;num_features&lt;/span&gt;&lt;span style=&#34;color:#111&#34;&gt;)&lt;/span&gt;&#xA;&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;h2 id=&#34;layer-normalization&#34;&gt;&#xA;  Layer normalization&#xA;  &lt;a class=&#34;anchor&#34; href=&#34;#layer-normalization&#34;&gt;#&lt;/a&gt;&#xA;&lt;/h2&gt;&#xA;&lt;img src=&#34;https://vikasraykar.github.io/deeplearning/layer.jpeg&#34; alt=&#34;Layer normalization&#34; width=&#34;300&#34;/&gt;&#xA;&lt;p&gt;In layer normalization the mean and variance are computed across the feature/hidden unit for each example seprately.&#xA;&lt;span&gt;&#xA;  \[&#xA;\mu_n = \frac{1}{M} \sum_{i=1}^{M} a_{ni}&#xA;\]&#xA;&lt;/span&gt;&#xA;&#xA;&lt;span&gt;&#xA;  \[&#xA;\sigma_n^2 = \frac{1}{M} \sum_{i=1}^{M} (a_{ni}-\mu_i)^2&#xA;\]&#xA;&lt;/span&gt;&#xA;&#xA;We normalize the pre-activations as follows.&#xA;&lt;span&gt;&#xA;  \[&#xA;\hat{a}_{ni} = \frac{a_{ni}-\mu_n}{\sqrt{\sigma_n^2+\delta}}&#xA;\]&#xA;&lt;/span&gt;&#xA;&#xA;&lt;span&gt;&#xA;  \[&#xA;\tilde{a}_{ni} = \gamma_n \hat{a}_{ni} + \beta_n&#xA;\]&#xA;&lt;/span&gt;&#xA;&lt;/p&gt;</description>
    </item>
    <item>
      <title>Activation functions</title>
      <link>http://localhost:1313/docs/neural_networks/activations/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>http://localhost:1313/docs/neural_networks/activations/</guid>
      <description>&lt;h2 id=&#34;sigmoid&#34;&gt;&#xA;  Sigmoid&#xA;  &lt;a class=&#34;anchor&#34; href=&#34;#sigmoid&#34;&gt;#&lt;/a&gt;&#xA;&lt;/h2&gt;&#xA;&lt;p&gt;Sigmoid/Logistic&#xA;&lt;span&gt;&#xA;  \[&#xA;\sigma(z) = \frac{1}{1+\exp(-z)}&#xA;\]&#xA;&lt;/span&gt;&#xA;&#xA;The derivative is given by&#xA;&lt;span&gt;&#xA;  \[&#xA;\sigma&#39;(z) = \sigma(z)(1-\sigma(z))&#xA;\]&#xA;&lt;/span&gt;&#xA;&lt;/p&gt;&#xA;&lt;h2 id=&#34;relu&#34;&gt;&#xA;  ReLU&#xA;  &lt;a class=&#34;anchor&#34; href=&#34;#relu&#34;&gt;#&lt;/a&gt;&#xA;&lt;/h2&gt;&#xA;&lt;p&gt;Rectified Linear Unit (ReLU)&#xA;&lt;span&gt;&#xA;  \[&#xA;\text{ReLU}(z) = \max(z,0)&#xA;\]&#xA;&lt;/span&gt;&#xA;&lt;/p&gt;&#xA;&lt;blockquote&gt;&#xA;&lt;p&gt;Nair and Hinton, 2010&lt;/p&gt;&lt;/blockquote&gt;&#xA;&lt;h2 id=&#34;prelu&#34;&gt;&#xA;  pReLU&#xA;  &lt;a class=&#34;anchor&#34; href=&#34;#prelu&#34;&gt;#&lt;/a&gt;&#xA;&lt;/h2&gt;&#xA;&lt;p&gt;parameterized Rectified Linear Unit (pReLU)&#xA;&lt;span&gt;&#xA;  \[&#xA;\text{pReLU}(z) = \max(z,0) + \alpha \min(z,0)&#xA;\]&#xA;&lt;/span&gt;&#xA;&lt;/p&gt;&#xA;&lt;blockquote&gt;&#xA;&lt;p&gt;He et al., 2015&lt;/p&gt;&lt;/blockquote&gt;&#xA;&lt;h2 id=&#34;tanh&#34;&gt;&#xA;  Tanh&#xA;  &lt;a class=&#34;anchor&#34; href=&#34;#tanh&#34;&gt;#&lt;/a&gt;&#xA;&lt;/h2&gt;&#xA;&lt;p&gt;Hyperbolic tangent.&#xA;&lt;span&gt;&#xA;  \[&#xA;\text{tanh}(z) = \frac{1-\exp(-2z)}{1+\exp(-2z)}&#xA;\]&#xA;&lt;/span&gt;&#xA;&#xA;The derivative is given by&#xA;&lt;span&gt;&#xA;  \[&#xA;\text{tanh}&#39;(z)= 1- \text{tanh}^2(z)&#xA;\]&#xA;&lt;/span&gt;&#xA;&lt;/p&gt;</description>
    </item>
    <item>
      <title>Regularization</title>
      <link>http://localhost:1313/docs/training/regularization/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>http://localhost:1313/docs/training/regularization/</guid>
      <description>&lt;h2 id=&#34;dropout&#34;&gt;&#xA;  Dropout&#xA;  &lt;a class=&#34;anchor&#34; href=&#34;#dropout&#34;&gt;#&lt;/a&gt;&#xA;&lt;/h2&gt;&#xA;&lt;p&gt;Dropout is one of the most effective form of regularization that is widely used.&lt;/p&gt;&#xA;&lt;p&gt;The core idea is to delete nodes from the network, including their connections, at random during training.&lt;/p&gt;&#xA;&lt;p&gt;Dropout is applied to both hidden and input nodes, but not outputs. It is equivalent to setting the output of a dropped node to zero.&lt;/p&gt;&#xA;&lt;a  href=&#34;https://pytorch.org/docs/stable/generated/torch.nn.Dropout.html#torch.nn.Dropout&#34;   target=&#34;_blank&#34; rel=&#34;noopener&#34;  class=&#34;book-btn&#34;&gt;torch.nn.Dropout&lt;/a&gt;&#xA;&#xA;&lt;h2 id=&#34;early-stopping&#34;&gt;&#xA;  Early stopping&#xA;  &lt;a class=&#34;anchor&#34; href=&#34;#early-stopping&#34;&gt;#&lt;/a&gt;&#xA;&lt;/h2&gt;&#xA;&lt;p&gt;For good generalization training should be stopped at the point of smallest error with respect to the validation set.&lt;/p&gt;</description>
    </item>
    <item>
      <title>Entropy</title>
      <link>http://localhost:1313/docs/neural_networks/entropy/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>http://localhost:1313/docs/neural_networks/entropy/</guid>
      <description>&lt;blockquote&gt;&#xA;&lt;p&gt;A brief primer on entropy, cross-entropy and perplexity.&lt;/p&gt;&lt;/blockquote&gt;&#xA;&lt;h2 id=&#34;entropy&#34;&gt;&#xA;  Entropy&#xA;  &lt;a class=&#34;anchor&#34; href=&#34;#entropy&#34;&gt;#&lt;/a&gt;&#xA;&lt;/h2&gt;&#xA;&lt;p&gt;&lt;span&gt;&#xA;  \(\)&#xA;&lt;/span&gt;&#xA;The &lt;strong&gt;entropy&lt;/strong&gt; of a discrete random variable &lt;span&gt;&#xA;  \(X\)&#xA;&lt;/span&gt;&#xA; with &lt;span&gt;&#xA;  \(K\)&#xA;&lt;/span&gt;&#xA; states/categories with distribution &lt;span&gt;&#xA;  \(p_k = \text{Pr}(X=k)\)&#xA;&lt;/span&gt;&#xA; for &lt;span&gt;&#xA;  \(k=1,...,K\)&#xA;&lt;/span&gt;&#xA;  is a measure of uncertainty and is defined as follows.&#xA;&lt;span&gt;&#xA;  \[H(X) = \sum_{k=1}^{K} p_k \log_2 \frac{1}{p_k} = - \sum_{k=1}^{K} p_k \log_2 p_k \]&#xA;&lt;/span&gt;&#xA;&#xA;&lt;span&gt;&#xA;  \(\)&#xA;&lt;/span&gt;&#xA;&#xA;The term &lt;span&gt;&#xA;  \(\log_2\frac{1}{p}\)&#xA;&lt;/span&gt;&#xA; quantifies the notion or surprise or uncertainty and hence entropy is the average uncertainty.&lt;/p&gt;</description>
    </item>
    <item>
      <title>Training loop</title>
      <link>http://localhost:1313/docs/training/training_loop/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>http://localhost:1313/docs/training/training_loop/</guid>
      <description>&lt;h2 id=&#34;training-loop&#34;&gt;&#xA;  Training loop&#xA;  &lt;a class=&#34;anchor&#34; href=&#34;#training-loop&#34;&gt;#&lt;/a&gt;&#xA;&lt;/h2&gt;&#xA;&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#272822;background-color:#fafafa;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#75715e&#34;&gt;# Load the dataset.&lt;/span&gt;&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#111&#34;&gt;train_dataset&lt;/span&gt; &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; &lt;span style=&#34;color:#111&#34;&gt;SampleDataset&lt;/span&gt;&lt;span style=&#34;color:#111&#34;&gt;(&lt;/span&gt;&lt;span style=&#34;color:#111&#34;&gt;X_train&lt;/span&gt;&lt;span style=&#34;color:#111&#34;&gt;,&lt;/span&gt; &lt;span style=&#34;color:#111&#34;&gt;y_train&lt;/span&gt;&lt;span style=&#34;color:#111&#34;&gt;)&lt;/span&gt;&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#111&#34;&gt;test_dataset&lt;/span&gt; &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; &lt;span style=&#34;color:#111&#34;&gt;SampleDataset&lt;/span&gt;&lt;span style=&#34;color:#111&#34;&gt;(&lt;/span&gt;&lt;span style=&#34;color:#111&#34;&gt;X_test&lt;/span&gt;&lt;span style=&#34;color:#111&#34;&gt;,&lt;/span&gt; &lt;span style=&#34;color:#111&#34;&gt;y_test&lt;/span&gt;&lt;span style=&#34;color:#111&#34;&gt;)&lt;/span&gt;&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#75715e&#34;&gt;# Preparing your data for training with DataLoaders.&lt;/span&gt;&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#111&#34;&gt;batch_size&lt;/span&gt; &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; &lt;span style=&#34;color:#ae81ff&#34;&gt;64&lt;/span&gt;&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#111&#34;&gt;train_dataloader&lt;/span&gt; &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; &lt;span style=&#34;color:#111&#34;&gt;DataLoader&lt;/span&gt;&lt;span style=&#34;color:#111&#34;&gt;(&lt;/span&gt;&lt;span style=&#34;color:#111&#34;&gt;train_dataset&lt;/span&gt;&lt;span style=&#34;color:#111&#34;&gt;,&lt;/span&gt; &lt;span style=&#34;color:#111&#34;&gt;batch_size&lt;/span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#111&#34;&gt;batch_size&lt;/span&gt;&lt;span style=&#34;color:#111&#34;&gt;,&lt;/span&gt; &lt;span style=&#34;color:#111&#34;&gt;shuffle&lt;/span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#00a8c8&#34;&gt;True&lt;/span&gt;&lt;span style=&#34;color:#111&#34;&gt;)&lt;/span&gt;&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#111&#34;&gt;test_dataloader&lt;/span&gt; &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; &lt;span style=&#34;color:#111&#34;&gt;DataLoader&lt;/span&gt;&lt;span style=&#34;color:#111&#34;&gt;(&lt;/span&gt;&lt;span style=&#34;color:#111&#34;&gt;test_dataset&lt;/span&gt;&lt;span style=&#34;color:#111&#34;&gt;,&lt;/span&gt; &lt;span style=&#34;color:#111&#34;&gt;batch_size&lt;/span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#111&#34;&gt;batch_size&lt;/span&gt;&lt;span style=&#34;color:#111&#34;&gt;,&lt;/span&gt; &lt;span style=&#34;color:#111&#34;&gt;shuffle&lt;/span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#00a8c8&#34;&gt;True&lt;/span&gt;&lt;span style=&#34;color:#111&#34;&gt;)&lt;/span&gt;&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#75715e&#34;&gt;# Define the model class.&lt;/span&gt;&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#111&#34;&gt;model&lt;/span&gt; &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; &lt;span style=&#34;color:#111&#34;&gt;LogisticRegression&lt;/span&gt;&lt;span style=&#34;color:#111&#34;&gt;(&lt;/span&gt;&lt;span style=&#34;color:#111&#34;&gt;num_features&lt;/span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#111&#34;&gt;d&lt;/span&gt;&lt;span style=&#34;color:#111&#34;&gt;)&lt;/span&gt;&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#75715e&#34;&gt;# Loss fucntion.&lt;/span&gt;&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#111&#34;&gt;loss_fn&lt;/span&gt; &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; &lt;span style=&#34;color:#111&#34;&gt;nn&lt;/span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;&lt;span style=&#34;color:#111&#34;&gt;BCELoss&lt;/span&gt;&lt;span style=&#34;color:#111&#34;&gt;()&lt;/span&gt;&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#75715e&#34;&gt;# Optimizer.&lt;/span&gt;&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#111&#34;&gt;optimizer&lt;/span&gt; &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; &lt;span style=&#34;color:#111&#34;&gt;SGD&lt;/span&gt;&lt;span style=&#34;color:#111&#34;&gt;(&lt;/span&gt;&lt;span style=&#34;color:#111&#34;&gt;model&lt;/span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;&lt;span style=&#34;color:#111&#34;&gt;parameters&lt;/span&gt;&lt;span style=&#34;color:#111&#34;&gt;(),&lt;/span&gt; &lt;span style=&#34;color:#111&#34;&gt;lr&lt;/span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;0.01&lt;/span&gt;&lt;span style=&#34;color:#111&#34;&gt;,&lt;/span&gt; &lt;span style=&#34;color:#111&#34;&gt;momentum&lt;/span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;0.9&lt;/span&gt;&lt;span style=&#34;color:#111&#34;&gt;)&lt;/span&gt;&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#75715e&#34;&gt;# Learning rate scheduler.&lt;/span&gt;&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#111&#34;&gt;scheduler&lt;/span&gt; &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; &lt;span style=&#34;color:#111&#34;&gt;ExponentialLR&lt;/span&gt;&lt;span style=&#34;color:#111&#34;&gt;(&lt;/span&gt;&lt;span style=&#34;color:#111&#34;&gt;optimizer&lt;/span&gt;&lt;span style=&#34;color:#111&#34;&gt;,&lt;/span&gt; &lt;span style=&#34;color:#111&#34;&gt;gamma&lt;/span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;0.9&lt;/span&gt;&lt;span style=&#34;color:#111&#34;&gt;)&lt;/span&gt;&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#75715e&#34;&gt;# Run for a few epochs.&lt;/span&gt;&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#00a8c8&#34;&gt;for&lt;/span&gt; &lt;span style=&#34;color:#111&#34;&gt;epoch&lt;/span&gt; &lt;span style=&#34;color:#f92672&#34;&gt;in&lt;/span&gt; &lt;span style=&#34;color:#111&#34;&gt;range&lt;/span&gt;&lt;span style=&#34;color:#111&#34;&gt;(&lt;/span&gt;&lt;span style=&#34;color:#111&#34;&gt;n_epochs&lt;/span&gt;&lt;span style=&#34;color:#111&#34;&gt;):&lt;/span&gt;&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#75715e&#34;&gt;# Iterate through the DataLoader to access mini-batches.&lt;/span&gt;&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#00a8c8&#34;&gt;for&lt;/span&gt; &lt;span style=&#34;color:#111&#34;&gt;batch&lt;/span&gt;&lt;span style=&#34;color:#111&#34;&gt;,&lt;/span&gt; &lt;span style=&#34;color:#111&#34;&gt;(&lt;/span&gt;&lt;span style=&#34;color:#111&#34;&gt;input&lt;/span&gt;&lt;span style=&#34;color:#111&#34;&gt;,&lt;/span&gt; &lt;span style=&#34;color:#111&#34;&gt;target&lt;/span&gt;&lt;span style=&#34;color:#111&#34;&gt;)&lt;/span&gt; &lt;span style=&#34;color:#f92672&#34;&gt;in&lt;/span&gt; &lt;span style=&#34;color:#111&#34;&gt;enumerate&lt;/span&gt;&lt;span style=&#34;color:#111&#34;&gt;(&lt;/span&gt;&lt;span style=&#34;color:#111&#34;&gt;train_dataloader&lt;/span&gt;&lt;span style=&#34;color:#111&#34;&gt;):&lt;/span&gt;&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        &lt;span style=&#34;color:#75715e&#34;&gt;# Prediction.&lt;/span&gt;&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        &lt;span style=&#34;color:#111&#34;&gt;output&lt;/span&gt; &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; &lt;span style=&#34;color:#111&#34;&gt;model&lt;/span&gt;&lt;span style=&#34;color:#111&#34;&gt;(&lt;/span&gt;&lt;span style=&#34;color:#111&#34;&gt;input&lt;/span&gt;&lt;span style=&#34;color:#111&#34;&gt;)&lt;/span&gt;&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        &lt;span style=&#34;color:#75715e&#34;&gt;# Compute loss.&lt;/span&gt;&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        &lt;span style=&#34;color:#111&#34;&gt;loss&lt;/span&gt; &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; &lt;span style=&#34;color:#111&#34;&gt;loss_fn&lt;/span&gt;&lt;span style=&#34;color:#111&#34;&gt;(&lt;/span&gt;&lt;span style=&#34;color:#111&#34;&gt;output&lt;/span&gt;&lt;span style=&#34;color:#111&#34;&gt;,&lt;/span&gt; &lt;span style=&#34;color:#111&#34;&gt;target&lt;/span&gt;&lt;span style=&#34;color:#111&#34;&gt;)&lt;/span&gt;&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        &lt;span style=&#34;color:#75715e&#34;&gt;# Compute gradient.&lt;/span&gt;&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        &lt;span style=&#34;color:#111&#34;&gt;loss&lt;/span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;&lt;span style=&#34;color:#111&#34;&gt;backward&lt;/span&gt;&lt;span style=&#34;color:#111&#34;&gt;()&lt;/span&gt;&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        &lt;span style=&#34;color:#75715e&#34;&gt;# Gradient descent.&lt;/span&gt;&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        &lt;span style=&#34;color:#111&#34;&gt;optimizer&lt;/span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;&lt;span style=&#34;color:#111&#34;&gt;step&lt;/span&gt;&lt;span style=&#34;color:#111&#34;&gt;()&lt;/span&gt;&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        &lt;span style=&#34;color:#75715e&#34;&gt;# Prevent gradient accumulation&lt;/span&gt;&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        &lt;span style=&#34;color:#111&#34;&gt;optimizer&lt;/span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;&lt;span style=&#34;color:#111&#34;&gt;zero_grad&lt;/span&gt;&lt;span style=&#34;color:#111&#34;&gt;()&lt;/span&gt;&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#75715e&#34;&gt;# Adjust learning rate&lt;/span&gt;&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#111&#34;&gt;scheduler&lt;/span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;&lt;span style=&#34;color:#111&#34;&gt;step&lt;/span&gt;&lt;span style=&#34;color:#111&#34;&gt;()&lt;/span&gt;&#xA;&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;</description>
    </item>
    <item>
      <title>Quiz</title>
      <link>http://localhost:1313/docs/training/quiz/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>http://localhost:1313/docs/training/quiz/</guid>
      <description>&lt;h2 id=&#34;quiz&#34;&gt;&#xA;  Quiz&#xA;  &lt;a class=&#34;anchor&#34; href=&#34;#quiz&#34;&gt;#&lt;/a&gt;&#xA;&lt;/h2&gt;&#xA;&lt;span&gt;&#xA;  \(\)&#xA;&lt;/span&gt;&#xA;&#xA;&lt;blockquote&gt;&#xA;&lt;p&gt;Derive the gradient of the loss function for linear regression and logistic regression.&lt;/p&gt;&lt;/blockquote&gt;&#xA;&lt;blockquote&gt;&#xA;&lt;p&gt;Plot binary entropy.&lt;/p&gt;&lt;/blockquote&gt;&#xA;&lt;blockquote&gt;&#xA;&lt;p&gt;WHy don&amp;rsquo;t you use MSE loss for binary classification ?&lt;/p&gt;&lt;/blockquote&gt;&#xA;&lt;blockquote&gt;&#xA;&lt;p&gt;What is the most widely used optimizer ? What are the typically used parameters of the optimizer ?&lt;/p&gt;&lt;/blockquote&gt;&#xA;&lt;blockquote&gt;&#xA;&lt;p&gt;For SGD with momemtum show that it increases the effective learning rate from &lt;span&gt;&#xA;  \(\eta\)&#xA;&lt;/span&gt;&#xA; to &lt;span&gt;&#xA;  \(\frac{\eta}{(1-\mu)}\)&#xA;&lt;/span&gt;&#xA;.&lt;/p&gt;&lt;/blockquote&gt;&#xA;&lt;blockquote&gt;&#xA;&lt;p&gt;In &lt;a href=&#34;https://arxiv.org/abs/1706.03762&#34;&gt;Attention Is All You Need&lt;/a&gt; paper what is the optimizer and the learning rate scheduler used ?&lt;/p&gt;</description>
    </item>
    <item>
      <title>Coding</title>
      <link>http://localhost:1313/docs/training/coding/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>http://localhost:1313/docs/training/coding/</guid>
      <description>&lt;h2 id=&#34;coding-assignment&#34;&gt;&#xA;  Coding assignment&#xA;  &lt;a class=&#34;anchor&#34; href=&#34;#coding-assignment&#34;&gt;#&lt;/a&gt;&#xA;&lt;/h2&gt;&#xA;&lt;span&gt;&#xA;  \(\)&#xA;&lt;/span&gt;&#xA;&#xA;&lt;h3 id=&#34;setup&#34;&gt;&#xA;  Setup&#xA;  &lt;a class=&#34;anchor&#34; href=&#34;#setup&#34;&gt;#&lt;/a&gt;&#xA;&lt;/h3&gt;&#xA;&lt;p&gt;&lt;a href=&#34;https://github.com/vikasraykar/deeplearning-dojo/&#34;&gt;https://github.com/vikasraykar/deeplearning-dojo/&lt;/a&gt;&lt;/p&gt;&#xA;&lt;pre tabindex=&#34;0&#34;&gt;&lt;code&gt;git clone https://github.com/vikasraykar/deeplearning-dojo.git&#xA;cd deeplearning-dojo&#xA;&#xA;python -m venv .env&#xA;source .env/bin/activate&#xA;pip install -r requirements.txt&#xA;&lt;/code&gt;&lt;/pre&gt;&lt;h3 id=&#34;problem-1&#34;&gt;&#xA;  Problem 1&#xA;  &lt;a class=&#34;anchor&#34; href=&#34;#problem-1&#34;&gt;#&lt;/a&gt;&#xA;&lt;/h3&gt;&#xA;&lt;blockquote&gt;&#xA;&lt;p&gt;Linear Regression with numpy and batch gradient descent.&lt;/p&gt;&lt;/blockquote&gt;&#xA;&lt;p&gt;In the first coding assigment you will be implementing a basic &lt;strong&gt;Linear Regression&lt;/strong&gt; model from scratch using &lt;strong&gt;only &lt;code&gt;numpy&lt;/code&gt;&lt;/strong&gt;. You will be implementing a basic batch gradient descent optimizer.&lt;/p&gt;&#xA;&lt;blockquote class=&#34;book-hint danger&#34;&gt;&#xA;&lt;p&gt;You can use only &lt;code&gt;numpy&lt;/code&gt; and are not allowed to to use &lt;code&gt;torch&lt;/code&gt; or any other python libraries.&lt;/p&gt;</description>
    </item>
  </channel>
</rss>
