[{"id":0,"href":"/docs/rl/basics/","title":"Basics","section":"Reinforcement","content":" Reinforcement learning basics # slides Reinforcement Learning (RL) is a natural computational paradigm for agents learning from interaction to achieve a goal. Deep learning (DL) provides a powerful general-purpose representation learning framework. A combination of these two has recently emerged as a strong contender for artificial general intelligence. This tutorial will provide a gentle exposition of RL concepts and DL based RL with a focus on policy gradients.\n\\(\\) The agent-environment interaction # The reinforcement learning (RL) framework is an abstraction of the problem of goal directed learning from interaction.\nThe learner and the decision maker is called the agent. The thing it interacts with (everything outside the agent) is called the environment. The agent and the environment interact continually. In the RL framework any problem of learning goal-directed behavior is abstracted to three signals passing back and forth between the agent and the environment.\none signal to represent the choices made by the agent (the actions) one signal to represent the basis on which the choices are made (the states) one signal to define the agents goal (rewards) In general, actions can be any decisions we want to learn how to make, and the states can be anything we can know that might be useful in making them. More formally at each time step \\(t\\) The agent receives a representation of the environments state, \\(S_t \\in \\mathcal{S}\\) , where \\(\\mathcal{S}\\) is the set of possible states. On the basis of \\(S_t\\) the agent selects an action \\(A_t \\in \\mathcal{A}(S_t)\\) , where \\(\\mathcal{A}(S_t)\\) the set of actions available in state \\(S_t\\) . One time step later, as a result of the action \\(A_t\\) the agent receives a scalar reward, \\(R_{t+1} \\in \\mathcal{R} \\subset \\mathbb{R}\\) . The agent then observes a new state \\(S_{t+1}\\) . Policy # At each time step, the agent essentially has to implement a mapping (called the agents policy) from states to actions.\nThe agents (stochastic) policy is denoted by \\(\\pi_t\\) , where\n\\[ \\pi_t(a|s) = \\mathbb{P}[A_t=a|S_t=s]. \\] Markov Decision Processes # In the most general case the environment response may depend on everything that has happened earlier. \\[ Pr\\left\\{S_{t+1}=s^{'},R_{t+1}=r|S_0,A_0,R_1,...,S_{t-1},A_{t-1},R_t,S_t,A_t,\\right\\}. \\] If the state signal has Markov property then the response depends only on the state and action representations at \\(t\\) \\[ Pr\\left\\{S_{t+1}=s^{'},R_{t+1}=r|S_t,A_t,\\right\\}. \\] A RL task that satisfies the Markov property is called a Markov Decision Process (MDP).\nGoals and rewards # The agents goal is to maximize the total amount of cumulative reward it receives over the long run (and not the immediate reward). If we want the agent to achieve some goal, we must specify the rewards to in in such a way that in maximizing them the agent will also achieve our goals.\nEpisodic tasks have a natural notion of the final time step. The agent-environment interaction naturally breaks into sub sequences, which are called episodes, such as plays of a game, trips through a maze, etc. Each episode ends in a special state called the terminal state, followed by a reset to the standard starting state.\nFor continuing tasks the agent-environment interaction goes on continually without limit.\nLet \\(R\\_{t+1},R\\_{t+2},R\\_{t+3},...\\) be the sequence of rewards received after time step \\(t\\) .\nThe return \\(G_t\\) is a defined as some specific function (for example, the sum) of the reward sequence. For episodic tasks \\[ G_t = R_{t+1}+R_{t+2}+R_{t+3}+...+R_{T}, \\] where \\(T\\) is the final time step.\nFor continuing tasks we need an additional concept of discounting. The discounted return is given by \\[ G_t = R_{t+1}+\\gamma R_{t+2}+\\gamma^2 R_{t+3}+... = \\sum_{k=0}^{\\infty}\\gamma^k R_{t+k+1} = R_{t+1}+\\gamma G_{t+1}, \\] where \\(0 \\leq \\gamma \\leq 1\\) is a parameter called the discount rate.\nThe discount rate determines the present value of future rewards. A reward received \\(k\\) time steps in the future is worth only \\(\\gamma^{k-1}\\) times what it would have been worth if it were received immediately.\nThe discount rate determines the present value of future rewards.\nAs \\(\\gamma\\) approaches 1 the agent becomes more farsighted.\nAs \\(\\gamma\\) approaches 0 the agent becomes more myopic.\nThe agents goal is to choose the actions to maximize the expected discounted return.\nValue Functions # A value function is a prediction of future reward. Recall that, the agents (stochastic) policy is denoted by \\(\\pi\\) , where \\[ \\pi(a|s) = \\mathbb{P}[A_t=a|S_t=s]. \\] The value of state \\(s\\) under a policy \\(\\pi\\) is the expected return when starting in \\(s\\) and following the policy \\(\\pi\\) thereafter. \\[ v_{\\pi}(s) = \\mathbb{E}_{\\pi}[G_t|S_t=s] = \\mathbb{E}_{\\pi}[\\sum_{k=0}^{\\infty}\\gamma^k R_{t+k+1}|S_t=s]. \\] \\(v_{\\pi}\\) is called the state-value function for policy \\(\\pi\\) .\nHow much reward will I get from state \\(s\\) under policy \\(\\pi\\) ?\nThe value of action \\(a\\) in state \\(s\\) under a policy \\(\\pi\\) is the expected return starting in \\(s\\) , taking the action \\(a\\) , and thereafter following policy \\(\\pi\\) .\n\\[ q_{\\pi}(s,a) = \\mathbb{E}_{\\pi}[G_t|S_t=s,A_t=a] =\\mathbb{E}_{\\pi}[\\sum_{k=0}^{\\infty}\\gamma^k R_{t+k+1}|S_t=s,A_t=a]. \\] \\(q_{\\pi}\\) is called the action-value function or the Q-value function for policy \\(\\pi\\) .\nHow much reward will I get from action \\(a\\) in state \\(s\\) under policy \\(\\pi\\) ?\nValue functions decompose into a Bellman equation which specifies the relation between the value of \\(s\\) and the value of its possible successor states.\n\\[ v_{\\pi}(s) = \\mathbb{E}[r+\\gamma v_{\\pi}(s^{'})] \\] Optimal Value Functions # The agents goal is to find a policy to maximize the expected discounted return.\nWhy are we talking about value functions ?\nValue functions define a partial ordering over polices.\n\\[ \\pi \\geq \\pi{'} \\text{ if and only if } v_{\\pi}(s) \\geq v_{\\pi}(s^{'}) \\text{ for all } s \\in \\mathcal{S} \\] The optimal policy is the one which has the maximum state-value function.\noptimal state-value function\n\\[ v_{*}(s) = \\max_{\\pi} v_{\\pi}(s) \\text{ for all } s \\in \\mathcal{S} \\] Bellman\u0026rsquo;s optimality equation\n\\[ v_{*}(s) = \\max_{a \\in \\mathcal{A}(s)} \\sum_{s^{'},r} p(s^{'},r|s,a)[r+\\gamma v_{*}(s^{'}) ] \\] optimal action-value function\n\\[ q_{*}(s,a) = \\max_{\\pi} q_{\\pi}(s,a) \\text{ for all } s \\in \\mathcal{S} \\text{ and } a \\in \\mathcal{A} \\] Bellman\u0026rsquo;s optimality equation\n\\[ q_{*}(s,a) = \\sum_{s^{'},r} p(s^{'},r|s,a)[r+\\gamma \\max_{a^{'}} q_{*}(s^{'},a^{'}) ] \\] The value of the start state must equal the discounted value of the expected next state plus the reward expected along the way.\nPolicy gradient methods # The goal is to learn a parametrized policy that can select actions without consulting a value function. Note that a value function will still be used to learn the policy parameter, but is not required for action selection.\nLet \\(\\theta \\in \\mathbb{R}^{d}\\) represent the policy\u0026rsquo;s parameter vector. The parameterized policy is written as\n\\[ \\pi(a|s,\\theta) = \\text{Pr}[A_t=a|S_t=s,\\theta_t=\\theta] \\] This is the probability that action \\(a\\) is taken at time \\(t\\) given that the agent is in state \\(s\\) at time \\(t\\) with parameter \\(\\theta\\) .\nWe will estimate the policy parameter \\(\\theta\\) to maximize a performance measure \\(J(\\theta)\\) .\n\\[ \\widehat{\\theta} = \\arg \\max_{\\theta} J(\\theta) \\] As usual we will use stochastic gradient ascent\n\\[ \\theta_{t+1} = \\theta_(t) + \\alpha \\widehat{\\nabla J(\\theta_t)}, \\] where \\(\\widehat{\\nabla J(\\theta_t)}\\) is a stochastic estimate of the gradient whose expectation approximates the true gradient.\nFor the episodic case the performance is defined as the value of the start state under the parameterized policy.\n\\[ J(\\theta) = v_{\\pi_{\\theta}}(s_0) \\] Recall, the value of a state \\(s\\) under a policy \\(\\pi\\) is the expected return when starting in \\(s\\) and following the policy \\(\\pi\\) thereafter.\n\\[ v_{\\pi_\\theta}(s_0) = \\mathbb{E}_{\\pi_\\theta}[G_t|S_t=s_0] = \\mathbb{E}_{\\pi_\\theta}[\\sum_{k=0}^{\\infty}\\gamma^k R_{t+k+1}|S_t=s_0] \\] Log-Derivative trick If \\(x \\sim p\\_{\\theta}(.)\\) then \\[ \\nabla_\\theta \\mathbb{E}[f(x)] = \\nabla_\\theta \\int p_{\\theta}(x) f(x) dx = \\int \\frac{p_{\\theta}(x)}{p_{\\theta}(x)} \\nabla_\\theta p_{\\theta}(x) f(x) dx \\] \\[ \\nabla_\\theta \\mathbb{E}[f(x)] = \\int p_{\\theta}(x) \\nabla_\\theta \\log p_{\\theta}(x) f(x) dx = \\mathbb{E}[f(x)\\nabla_\\theta \\log p_{\\theta}(x)] \\] We also need the compute the gradient of the log probability of an episode\nGradient of the log probability of an episode\nLet \\(\\tau\\) be an episode of length \\(T\\) defined as \\[ \\tau=(s_0,a_0,r_1,s_1,a_1,r_2,....,a_{T-1},r_T,s_T). \\] Then \\[ \\nabla_{\\theta} \\log p_{\\theta}(\\tau) = \\nabla_{\\theta} \\log \\left(\\mu(s_0) \\prod_{t=0}^{T-1} \\pi_{\\theta}(a_t|s_t)\\text{Pr}(s_{t+1}|s_t,a_t) \\right) \\] \\[ \\nabla_{\\theta} \\log p_{\\theta}(\\tau) = \\nabla_{\\theta} \\left[\\log \\mu(s_0) + \\sum_{t=0}^{T-1} ( \\log \\pi_{\\theta}(a_t|s_t)+ \\log \\text{Pr}(s_{t+1}|s_t,a_t) ) \\right] \\] \\[ \\nabla_{\\theta} \\log p_{\\theta}(\\tau) = \\nabla_{\\theta} \\sum_{t=0}^{T-1} \\log \\pi_{\\theta}(a_t|s_t) \\] Observe that when taking gradients, the state dynamics disappear!\nUsing the above two tricks\n\\[ \\nabla_{\\theta} v_{\\pi_\\theta}(s_0) = \\mathbb{E}_{\\tau \\sim \\pi_\\theta}\\left[ G_{\\tau}\\nabla_{\\theta} \\sum_{t=0}^{T-1} \\log \\pi_{\\theta}(a_t|s_t)\\right] \\] References # http://karpathy.github.io/2016/05/31/rl/ https://www.davidsilver.uk/teaching/ http://incompleteideas.net/sutton/book/the-book-2nd.html http://rll.berkeley.edu/deeprlcourse/ https://gym.openai.com/ https://deepmind.com/blog/deep-reinforcement-learning/ http://www.scholarpedia.org/article/Policy_gradient_methods#Assumptions_and_Notation https://danieltakeshi.github.io/2017/03/28/going-deeper-into-reinforcement-learning-fundamentals-of-policy-gradients/ http://www.1-4-5.net/~dmm/ml/log_derivative_trick.pdf "},{"id":1,"href":"/docs/training/gradient_descent/","title":"Gradient Descent","section":"Training","content":" Gradient Descent # Steepest descent.\nLet \\(\\mathbf{w}\\) be a vector of all the parameters for a model.\nLet \\(L(\\mathbf{w})\\) be the loss function (or error function).\nWe need to choose the model parameters that optimizes (minimizes) the loss function.\n\\[ \\hat{\\mathbf{w}} = \\argmin_{\\mathbf{w}} L(\\mathbf{w}) \\] Let \\(\\nabla L(\\mathbf{w})\\) be the gradient vector, where each element is the partial derivative of the loss fucntion wrt each parameter.\nThe gradient vector points in the direction of the greatest rate of increase of the loss function.\nSo to mimimize the loss function we take small steps in the direction of \\(-\\nabla L(\\mathbf{w})\\) .\nAt the mimimum \\(\\nabla L(\\mathbf{w})=0\\) .\n\\(\\nabla L(\\mathbf{w})=0\\) .\nStationary points \\(\\nabla L(\\mathbf{w})=0\\) are knows as stationary points, which can be either a minima, maxima or a saddle point. The necessary and sufficient condition for a local minima is The gradient of the loss function should be zero. The Hessian matrix should be positive definite. For now we will assume the gradient is given. For deep neural networks the gradient can be computed efficiently via backpropagation (which we will revisit later).\nBatch Gradient Descent # We take a small step in the direction of the negative gradient.\n\\[ \\mathbf{w}^t \\leftarrow \\mathbf{w}^{t-1} - \\eta \\nabla L(\\mathbf{w}^{t-1}) \\] The parameter \\(\\eta \u003e 0\\) is called the learning rate and determines the step size at each iteration.\nThis update is repeated multiple times (till covergence).\nfor epoch in range(n_epochs): dw = gradient(loss, data, w) w = w - lr * dw Each step requires that the entire training data be processed to compute the gradient \\(\\nabla L(\\mathbf{w}^{t-1})\\) . For large datasets this is not comptationally efficient.\nStochastic Gradient Descent # In general most loss functions can be written as sum over each training instance. \\[ L(\\mathbf{w}) = \\sum_{i=1}^{N} L_i(\\mathbf{w}) \\] In Stochastic Gradient Descent (SGD) we update the parameters one data point at a time. \\[ \\mathbf{w}^t \\leftarrow \\mathbf{w}^{t-1} - \\eta \\nabla L_i(\\mathbf{w}^{t-1}) \\] A complete passthrough of the whole dataset is called an epoch. Training is done for multiple epochs depending on the size of the dataset.\nfor epoch in range(n_epochs): for i in range(n_data): dw = gradient(loss, data[i], w) w = w - lr * dw SGD is much faster and more computationally efficient, but it has noise in the estimation of the gradient. Since it updates the weight frequently, it can lead to big oscillations and that makes the training process highly unstable. Bottou, L. (2010). Large-Scale Machine Learning with Stochastic Gradient Descent. In: Lechevallier, Y., Saporta, G. (eds) Proceedings of COMPSTAT'2010. Physica-Verlag HD.\nMini-batch Stochastic Gradient Descent # Using a single example results in a very noisy estimate of the gradient. So we use a small random subset of data called mini-batch of size B (batch size) to compute the gradient.\n\\[ \\mathbf{w}^t \\leftarrow \\mathbf{w}^{t-1} - \\eta \\nabla L_{batch}(\\mathbf{w}^{t-1}) \\] for epoch in range(n_epochs): for mini_batch in get_batches(data, batch_size): dw = gradient(loss, mini_batch, w) w = w - lr * dw PyTorch optimizer = optim.SGD(model.parameters(), lr=1e-3) Mini-batch SGD is the most commonly used method and is sometimes refered to as just SGD.\nTypical choices of the batch size are B=32,64,128,256,.. In practice we do a random shuffle of the data per epoch. In practice, mini-batch SGD is the most frequently used variation because it is both computationally cheap and results in more robust convergence.\nAdding momentum # One of the basic improvements over SGD comes from adding a momentum term.\nAt every time step, we update velocity by decaying the previous velocity by a factor of \\(0 \\leq \\mu \\leq 1\\) (called the momentum parameter) and adding the current gradient update. \\[ \\mathbf{v}^{t-1} \\leftarrow \\mu \\mathbf{v}^{t-2} - \\eta \\nabla L(\\mathbf{w}^{t-1}) \\] Then, we update our weights in the direction of the velocity vector. \\[ \\mathbf{w}^t \\leftarrow \\mathbf{w}^{t-1} + \\mathbf{v}^{t-1} \\] for epoch in range(n_epochs): for mini_batch in get_batches(data, batch_size): dw = gradient(loss, mini_batch, w) # gradient v = momentum * v - lr * dw # velocity w = w + v PyTorch optimizer = optim.SGD(model.parameters(), lr=0.01, momentum=0.9) We now have two hyper-parameters learnign rate and momentum. Typically we set the momentum parameter to 0.9.\nEffective learning rate \\(\\) One interpretation of momentum to increase the effective learning rate from \\(\\eta\\) to \\(\\frac{\\eta}{(1-\\mu)}\\) . If we make the approximation that the gradient is unchanging then \\[ -\\eta \\nabla L \\{1+\\mu+\\mu^2+...\\} = - \\frac{\\eta}{1-\\mu} \\nabla L \\] By contrast, in a region of high curvature in which gradient descent is oscillatory, successive contributions from the momentum term will tend to cancel and effective learning rate will be close to \\(\\eta\\) . We can now escape local minima or saddle points because we keep moving downwards even though the gradient of the mini-batch might be zero. Momentum can also help us reduce the oscillation of the gradients because the velocity vectors can smooth out these highly changing landscapes. It reduces the noise of the gradients and follows a more direct walk down the landscape. Ilya Sutskever, James Martens, George Dahl, and Geoffrey Hinton. 2013. On the importance of initialization and momentum in deep learning. In Proceedings of the 30th International Conference on International Conference on Machine Learning - Volume 28 (ICML'13). JMLR.org, III–1139–III–1147.\nAdaptive Learning Rates # Different learning rate for each parameter.\nAdagrad # Adaptive gradient.\nAdaGrad reduces each learning rate parameter over time by using the accumulated sum of squares of all the derivates calculated for that parameter. \\[ \\mathbf{w}^t \\leftarrow \\mathbf{w}^{t-1} - \\frac{\\eta}{\\sqrt{\\mathbf{r}^{t}}+\\delta} \\odot \\nabla L(\\mathbf{w}^{t-1}) \\] where \\(\\mathbf{r}^t\\) is the running sum of the squares of the gradients and \\(\\delta\\) is a small constant to ensure numerical stability. \\[ \\mathbf{r}^t = \\mathbf{r}^{t-1} + \\left(\\nabla L(\\mathbf{w}^{t})\\right)^2 \\] for epoch in range(n_epochs): for mini_batch in get_batches(data, batch_size): dw = gradient(loss, mini_batch, w) # gradient r += dw*dw # Accumulated squared gradients w = w - lr * dw / (r.sqrt() + delta) PyTorch optimizer = torch.optim.Adagrad(model.parameters(), lr=0.01, eps=1e-10) We can see that when the gradient is changing very fast, the learning rate will be smaller. When the gradient is changing slowly, the learning rate will be bigger.\nA drawback of Adagrad is that as time goes by, the learning rate becomes smaller and smaller due to the monotonic increment of the running squared sum.\nJohn Duchi, Elad Hazan, and Yoram Singer. 2011. Adaptive Subgradient Methods for Online Learning and Stochastic Optimization. J. Mach. Learn. Res. 12, null (2/1/2011), 2121–2159.\nRMSProp # Root Mean Square Propagation, Leaky AdaGrad\nSince AdaGrad accumulates the squared gradients from the beginning, the associatied weight updates can become very small as training progresses.\nRMSProp essentially replaces it with an exponentialy weighted average. \\[ \\mathbf{r}^t = \\alpha \\mathbf{r}^{t-1} + (1-\\alpha) \\left(\\nabla L(\\mathbf{w}^{t})\\right)^2 \\] where \\(0 \u003c \\alpha \u003c 1\\) . \\[ \\mathbf{w}^t \\leftarrow \\mathbf{w}^{t-1} - \\frac{\\eta}{\\sqrt{\\mathbf{r}^{t}}+\\delta} \\odot \\nabla L(\\mathbf{w}^{t-1}) \\] \\(\\) Typically we set the \\(\\alpha=0.9\\) . for epoch in range(n_epochs): for mini_batch in get_batches(data, batch_size): dw = gradient(loss, mini_batch, w) # gradient r += alpha * r + (1-alpha) * dw*dw # Accumulated squared gradients w = w - lr * dw / (r.sqrt() + delta) PyTorch optimizer = torch.optim.RMSProp(model.parameters(), lr=0.01, alpha=0.99, eps=1e-8) Hinton, 2012. Neural Networks for Machine Learning. Lecture 6a.\nAdam # Adaptive moments.\nIf we combine RMSProp with momentum we ontain the most popular Adam optimization method.\nAdam maintains an exponentially weighted average of the first and the second moments. \\[ \\mathbf{s}^t = \\beta_1 \\mathbf{s}^{t-1} + (1-\\beta_1) \\left(\\nabla L(\\mathbf{w}^{t})\\right) \\] \\[ \\mathbf{r}^t = \\beta_2 \\mathbf{r}^{t-1} + (1-\\beta_2) \\left(\\nabla L(\\mathbf{w}^{t})\\right)^2 \\] We correct for the bias introduced by initializing \\(\\mathbf{s}^0\\) and \\(\\mathbf{r}^0\\) to zero. \\[ \\hat{\\mathbf{s}}^t = \\frac{\\mathbf{s}^t}{1-\\beta_1^t} \\] \\[ \\hat{\\mathbf{r}}^t = \\frac{\\mathbf{r}^t}{1-\\beta_2^t} \\] The updates are given as follows. \\[ \\mathbf{w}^t \\leftarrow \\mathbf{w}^{t-1} - \\frac{\\eta}{\\sqrt{\\hat{\\mathbf{r}}^{t}}+\\delta} \\odot \\hat{\\mathbf{s}}^t \\] \\(\\) Typically we set the \\(\\beta_1=0.9\\) and \\(\\beta_2=0.99\\) . for epoch in range(n_epochs): for mini_batch in get_batches(data, batch_size): dw = gradient(loss, mini_batch, w) # gradient s += beta1 * s + (1-beta1) * dw # Accumulated gradients r += beta2 * r + (1-beta2) * dw*dw # Accumulated squared gradients s_hat = s /(1-beta1**t) r_hat = r /(1-beta2**t) w = w - lr * s_hat / (r_hat.sqrt() + delta) PyTorch optimizer = optim.Adam(model.parameters(), lr=0.001, betas=(0.9,0.99), eps=1e-08) Adam: A method for stochastic optimization, D. P. Kingma, D.P. and J. Ba, ICLR 2015.\nAdamW # AdamW proposes a modification to Adam that improves regularization by adding weight decay. At each iteration we pull the parameters towards zero.\n\\[ \\mathbf{w}^t \\leftarrow \\mathbf{w}^{t} - \\eta \\lambda \\mathbf{w}^{t} \\] PyTorch optimizer = optim.AdamW(model.parameters(), lr=0.001, betas=(0.9,0.99), eps=1e-08, weight_decay=0.01) Ilya Loshchilov, Frank Hutter, Decoupled Weight Decay Regularization, ICLR 2019.\nAdam and AdamW are the most widely used optimizers.\nLearning rate schedule # A small learning rate leads to slow convergence while a large learning rate leads to instability (due to divergent oscillations).\nIn practice we start with a large learning rate and and then reduce itover time.\n\\[ \\mathbf{w}^t \\leftarrow \\mathbf{w}^{t-1} - \\eta \\nabla^{t-1} L(\\mathbf{w}^{t-1}) \\] Linear \\[ \\mathbf{\\eta}^t = \\left(1-\\frac{t}{K}\\right) \\mathbf{\\eta}^0 + \\left(\\frac{t}{K}\\right) \\mathbf{\\eta}^K \\] The learning rate reduces linearly over K steps, after which its value is held constant.\nPower \\[ \\mathbf{\\eta}^t = \\mathbf{\\eta}^0 \\left(1+\\frac{t}{s}\\right)^c \\] Exponential \\[ \\mathbf{\\eta}^t = \\mathbf{\\eta}^0 c^\\frac{t}{s}\\] from torch.optim import SGD from torch.optim.lr_scheduler import ExponentialLR optimizer = SGD(model.parameters(), lr=0.01, momentum=0.9) scheduler = ExponentialLR(optimizer, gamma=0.9) PyTorch Collateral # https://pytorch.org/docs/stable/optim.html "},{"id":2,"href":"/docs/neural_networks/linear_regression/","title":"Linear Regression","section":"Neural Networks","content":" Linear Regression # Linear Regression is a single layer neural network for regression.\nstateDiagram-v2 direction LR z1: $$x_1$$ z2: $$x_2$$ zi: $$x_i$$ zM: $$x_d$$ aj: $$a=\\sum_i w_{i} x_i$$ zj: $$z=h(a)=a$$ z1 --\u003e aj:$$w_{1}$$ z2 --\u003e aj:$$w_{2}$$ zi --\u003e aj:$$w_{i}$$ zM --\u003e aj:$$w_{d}$$ aj --\u003e zj zj --\u003e END:::hidden note left of zM : Inputs note left of aj : Pre-activation note left of zj : Activation note left of END : Output classDef hidden display: none; Model # Linear Regression assumes a linear relationship between the target \\(y \\in \\mathbb{R}\\) and the features \\(\\mathbf{x}\\in \\mathbb{R}^d\\) . \\[ y = f(\\mathbf{x}) = w_1 x_1 + w_2 x_2 + ... + w_d x_d + b = \\mathbf{w}^T\\mathbf{x} + b, \\] where \\(\\mathbf{w}\\in \\mathbb{R}^d\\) is the \\(d\\) -dimensional weight vector and \\(b \\in \\mathbb{R}\\) is the bias term.\nWithout loss of generalization we can ignore the bias term as it can be subsumed into the design matrix by appending a column of ones.\n\\[ y = f(\\mathbf{x}) = \\mathbf{w}^T\\mathbf{x} \\] We often stack all the \\(n\\) examples into a design matrix \\(\\mathbf{X} \\in \\mathbb{R^{n \\times d}}\\) , where each row is one instance. The predictions for all the \\(n\\) instances \\(\\mathbf{y} \\in \\mathbb{R}^n\\) can be written conveniently as a matrix-vector product. \\[ \\mathbf{y} = \\mathbf{X}\\mathbf{w} \\] Statistical model # The probability of \\(y\\) for a given feature vector ( \\(\\mathbf{x}\\in \\mathbb{R}^d\\) ) is modelled as \\[ \\text{Pr}[y|\\mathbf{x},\\mathbf{w}] = \\mathcal{N}(y|\\mathbf{w}^T\\mathbf{x},\\sigma^2) \\] where \\(\\mathbf{w}\\in \\mathbb{R}^d\\) are the weights/parameters of the model and \\(\\mathcal{N}\\) is the normal distribution with mean \\(\\mathbf{w}^T\\mathbf{x}\\) and variance \\(\\sigma^2\\) . The model prediction is given by \\[ \\text{E}[y|\\mathbf{x},\\mathbf{w}] = \\mathbf{w}^T\\mathbf{x} \\] Likelihood # Given a dataset \\(\\mathcal{D}=\\{\\mathbf{x}_i \\in \\mathbb{R}^d,\\mathbf{y}_i \\in \\mathbb{R}\\}_{i=1}^N\\) containing \\(n\\) examples we need to estimate the parameter vector \\(\\mathbf{w}\\) by maximizing the likelihood of data.\nIn practice we minimize the negative log likelihood.\nLet \\( \\mu_i = \\mathbf{w}^T\\mathbf{x}_i\\) be the model prediction for each example in the training dataset. The negative log likelihood (NLL) is given by \\[ \\begin{align} L(\\mathbf{w}) \u0026= - \\sum_{i=1}^{N} \\log \\left[\\text{Pr}[y_i|\\mathbf{x}_i,\\mathbf{w}]\\right] \\nonumber \\\\ \u0026= \\frac{N}{2} \\log(2\\pi\\sigma^2) + \\frac{1}{2\\sigma^2} \\sum_{i=1}^{N} (y_i-\\mu_i)^2 \\nonumber \\\\ \\end{align} \\] This is equivalent to minimizing the Mean Squared Error (MSE) loss. \\[ \\begin{align} L(\\mathbf{w}) \u0026= \\frac{1}{2N} \\sum_{i=1}^{N} (y_i-\\mu_i)^2 \\nonumber \\\\ \\end{align} \\] We need to choose the model parameters that optimizes (minimizes) the loss function. \\[ \\hat{\\mathbf{w}} = \\argmin_{\\mathbf{w}} L(\\mathbf{w}) \\] Loss function # Mean Squared Error (MSE) \\[ L(\\mathbf{w}) = \\frac{1}{2N} \\sum_{i=1}^{N} \\left(y_i - \\mathbf{w}^T\\mathbf{x}_i \\right)^2 \\] torch.nn.MSELoss Gradient # The loss function using matrix notation can be written as follows. \\[ L(\\mathbf{w}) = \\frac{1}{2} \\| \\mathbf{y} - \\mathbf{X}\\mathbf{w} \\|^2 \\] The gradient of the loss function if given by \\[ \\nabla_{\\mathbf{w}} L(\\mathbf{w}) = \\mathbf{X}^T(\\mathbf{X}\\mathbf{w}-\\mathbf{y}) \\] Analytic solution # Taking the derivative of the loss with respect to \\(\\mathbf{w}\\) and setting it to zero yields: \\[ \\hat{\\mathbf{w}} = \\left(\\mathbf{X}^T\\mathbf{X}\\right)^{-1}\\mathbf{X}^T\\mathbf{y}. \\] This is unique when the design matrix is full rank, that columns of the design matrix are linearly independent or no features is linearly dependent on the others.\nIn practice we will use mini-batch stochastic gradient descent.\n"},{"id":3,"href":"/docs/neural_networks/","title":"Neural Networks","section":"Docs","content":" Single Layer Networks # For simplicity for this chapter we will mainly introduce single layer networks (for regression and classification).\nstateDiagram-v2 direction LR z1: $$x_1$$ z2: $$x_2$$ zi: $$x_i$$ zM: $$x_d$$ aj: $$a=\\sum_i w_{i} x_i$$ zj: $$z=h(a)$$ z1 --\u003e aj:$$w_{1}$$ z2 --\u003e aj:$$w_{2}$$ zi --\u003e aj:$$w_{i}$$ zM --\u003e aj:$$w_{d}$$ aj --\u003e zj zj --\u003e END:::hidden note left of zM : Inputs note left of aj : Pre-activation note left of zj : Activation note left of END : Output classDef hidden display: none; "},{"id":4,"href":"/docs/transformers/tokenizers/","title":"Tokenizers","section":"Transformers","content":"class Tokenizer: \u0026#34;\u0026#34;\u0026#34;Tokenizer\u0026#34;\u0026#34;\u0026#34; def encode(self, string: str) -\u0026gt; list[int]: \u0026#34;\u0026#34;\u0026#34;Convert a string to a sequence of integers (tokens).\u0026#34;\u0026#34;\u0026#34; raise NotImplementedError def decode(self, indices: list[int]) -\u0026gt; str: \u0026#34;\u0026#34;\u0026#34;Convert a sequence of integers (tokens) to a string.\u0026#34;\u0026#34;\u0026#34; raise NotImplementedError Tokenization pipeline\nNormalization Pre-tokenization Model Post-processor SentencePiece # https://github.com/google/sentencepiece\nSentencePiece is a tokenization algorithm for the preprocessing of text that you can use with either BPE, WordPiece, or Unigram model.\nIt considers the text as a sequence of Unicode characters, and replaces spaces with a special character, ▁. Used in conjunction with the Unigram algorithm, it doesn’t require a pre-tokenization step, which is very useful for languages where the space character is not used (like Chinese or Japanese). SentencePiece is reversible tokenization: since there is no special treatment of spaces, decoding the tokens is done simply by concatenating them and replacing the _s with spaces — this results in the normalized text. BPE # GPT-2\nByte-Pair Encoding (BPE)\nNeural Machine Translation of Rare Words with Subword Units, Rico Sennrich, Barry Haddow, Alexandra Birch, ACL 2016.\nWordPiece # BERT\nUnigram # T5\nTokenizer-free approaches # Use bytes directly, promising, but have not yet been scaled up to the frontier.\nhttps://arxiv.org/abs/2105.13626\nhttps://arxiv.org/pdf/2305.07185\nhttps://arxiv.org/abs/2412.09871\nhttps://arxiv.org/abs/2406.19223\n"},{"id":5,"href":"/docs/training/","title":"Training","section":"Docs","content":"The goal of training is to find the value of the parameters of a model to make effective predictions.\nWe choose the model parameters by optimizing a loss funcntion.\n"},{"id":6,"href":"/docs/training/backpropagation/","title":"Backpropagation","section":"Training","content":" Backpropagation # Backpropagation, Error Backpropagation, Backprop.\nBackpropagation (or backprop) is an efficient technique to compute the gradient of the loss function.\nIt boils down to a local message passing scheme in which information is sent backwards through the network.\nForward propagation # stateDiagram-v2 direction LR z1: $$z_1$$ z2: $$z_2$$ zi: $$z_i$$ zM: $$...$$ aj: $$a_j=\\sum_i w_{ji} z_i$$ zj: $$z_j=h(a_j)$$ START1:::hidden --\u003e z1 START2:::hidden --\u003e z2 STARTi:::hidden --\u003e zi STARTM:::hidden --\u003e zM z1 --\u003e aj z2 --\u003e aj zi --\u003e aj:$$w_{ji}$$ zM --\u003e aj aj --\u003e zj zj --\u003e END:::hidden note left of aj : Pre-activation note left of zj : Activation classDef hidden display: none; Let\u0026rsquo;s consider a hidden unit in a general feed forward neural network. \\[ a_j=\\sum_i w_{ji} z_i \\] where \\(z_i\\) is the activation of another unit or an input that sends an connection of unit \\(j\\) and \\(w_{ji}\\) is the weight associated with that connection. The sum \\(a_j\\) is known as pre-activation and is transformed by a non-linear activation function \\(h()\\) to give the activation \\(z_j\\) of unit \\(j\\) . \\[ z_j=h(a_j) \\] For any given data point in the training set, we can pass the input and compute the activations of all the hidden and output units. This process is called forward propagation since it is the forward flow of information through the network.\nBackward propagation # In general most loss functions can be written as sum over each training instance. \\[ L(\\mathbf{w}) = \\sum_{n=1}^{N} L_n(\\mathbf{w}) \\] Hence we will consider evaluating the gradient of \\(L_n(\\mathbf{w})\\) with respect to the weight parameters \\(w_{ji}\\) . We will now use chain rule to derive the gradient of the loss function. \\[ \\frac{\\partial L_n}{\\partial w_{ji}} = \\frac{\\partial L_n}{\\partial a_{j}} \\frac{\\partial a_j}{\\partial w_{ji}} = \\delta_j z_i \\] where \\(\\delta_j\\) are referred to as errors \\[ \\frac{\\partial L_n}{\\partial a_{j}} := \\delta_j \\] and \\[ \\frac{\\partial a_j}{\\partial w_{ji}} = z_i \\] So we now have \\[ \\frac{\\partial L_n}{\\partial w_{ji}} = \\delta_j z_i \\] The required derivative for \\(w_{ij}\\) is simply obtained by multiplying the value of \\(\\delta_j\\) for the unit at the output end of the weight by the value of \\(z_i\\) for the unit at the input end of the weight. This can be seen as a local computation involving the error signal at the output end with the activation signal at the input end.\nstateDiagram-v2 direction LR zi: $$z_i$$ zj: $$z_j$$ zi --\u003e zj:$$w_{ji}$$ note left of zi : $$\\delta_i$$ note right of zj : $$\\delta_j$$ So this now boils down to computing \\(\\delta_j\\) for all the hidden and the output units. \\(\\delta\\) for the output units are based on the loss function. For example for the MSE loss \\[ \\delta_k = y_{nk} - t_{nk} \\] To evaluate the \\(\\delta\\) for the hidden units we again make use of the the chain rule for partial derivatives. \\[ \\delta_j := \\frac{\\partial L_n}{\\partial a_{j}} = \\sum_{k} \\frac{\\partial L_n}{\\partial a_{k}} \\frac{\\partial a_k}{\\partial a_{j}} \\] where the sum runs over all the units k to which j sends connections. This gives rise to the final backpropagation formula \\[ \\delta_j = h^{'}(a_j)\\sum_{k} w_{kj} \\delta_k \\] stateDiagram-v2 direction LR zi: $$z_i$$ zj: $$z_j$$ z1: $$z_1$$ zk: $$z_k$$ zi --\u003e zj:$$w_{ji}$$ zj --\u003e zk:$$w_{kj}$$ zj --\u003e z1:$$w_{1j}$$ zk --\u003e zj z1 --\u003e zj note right of zj : $$\\delta_j$$ note right of zk : $$\\delta_k$$ note right of z1 : $$\\delta_1$$ This tells us that the value of \\(\\delta\\) for a particular hidden unit can be obtained by propagating the \\(\\delta\\) backward from units higher up in the network.\nAlgorithm # Forward propagation\nFor all hidden and output units compute in forward order\n\\[ a_j \\leftarrow \\sum_i w_{ji} z_i \\] \\[ z_j \\leftarrow h(a_j) \\] Error evaluation\nFor all output units compute\n\\[ \\delta_k \\leftarrow \\frac{\\partial L_n}{\\partial a_k} \\] Backward propagation\nFor all hidden units compute in reverse order\n\\[ \\delta_j \\leftarrow h^{'}(a_j)\\sum_{k} w_{kj} \\delta_k \\] \\[ \\frac{\\partial L_n}{\\partial w_{ji}} \\leftarrow \\delta_j z_i \\] stateDiagram-v2 direction LR z1: $$z_1$$ z2: $$z_2$$ zi: $$z_i$$ zM: $$...$$ delta1: $$\\delta_1$$ delta2: $$\\delta_2$$ deltak: $$\\delta_k$$ deltaM: $$...$$ aj: $$a_j$$ zj: $$z_j$$ START1:::hidden --\u003e z1 START2:::hidden --\u003e z2 STARTi:::hidden --\u003e zi STARTM:::hidden --\u003e zM z1 --\u003e aj z2 --\u003e aj zi --\u003e aj:$$w_{ji}$$ zM --\u003e aj aj --\u003e zj zj --\u003e delta1 zj --\u003e delta2 zj --\u003e deltak:$$w_{kj}$$ zj --\u003e deltaM delta1 --\u003e zj delta2 --\u003e zj deltak --\u003e zj deltaM --\u003e zj delta1 --\u003e START11:::hidden delta2 --\u003e START22:::hidden deltak --\u003e STARTii:::hidden deltaM --\u003e STARTMM:::hidden note left of aj : Pre-activation note left of zj : Activation note right of deltak : Errors classDef hidden display: none; "},{"id":7,"href":"/docs/neural_networks/logistic_regression/","title":"Logistic Regression","section":"Neural Networks","content":" Logistic Regression # Logistic Regression is a single layer neural network for binary classification.\nstateDiagram-v2 direction LR z1: $$x_1$$ z2: $$x_2$$ zi: $$x_i$$ zM: $$x_d$$ aj: $$a=\\sum_i w_{i} x_i$$ zj: $$z=\\sigma(a)$$ z1 --\u003e aj:$$w_{1}$$ z2 --\u003e aj:$$w_{2}$$ zi --\u003e aj:$$w_{i}$$ zM --\u003e aj:$$w_{d}$$ aj --\u003e zj zj --\u003e END:::hidden note left of zM : Inputs note left of aj : Pre-activation note left of zj : Activation note left of END : Output classDef hidden display: none; Statistical model # The probability of the positive class ( \\(y=1\\) ) for a given feature vector ( \\(\\mathbf{x}\\in \\mathbb{R}^d\\) ) is given by \\[ \\text{Pr}[y=1|\\mathbf{x},\\mathbf{w}] = \\sigma(\\mathbf{w}^T\\mathbf{x}) \\] where \\(\\mathbf{w}\\in \\mathbb{R}^d\\) are the weights/parameters of the model and \\(\\sigma\\) is the sigmoid activation function defined as \\[ \\sigma(x) = \\frac{1}{1-e^{-z}} \\] Without loss of generalization we ignore the bias term as it can be incorporated into the feature vector by appending a column of ones to the feature matrix.\nWe often stack all the \\(n\\) examples into a design matrix \\(\\mathbf{X} \\in \\mathbb{R^{n \\times d}}\\) , where each row is one instance. The predictions for all the \\(n\\) instances \\(\\mathbf{y} \\in \\mathbb{R}^n\\) can be written conveniently as a matrix-vector product. \\[ \\mathbf{y} = \\sigma(\\mathbf{X}\\mathbf{w}) \\] Likelihood # Given a dataset \\(\\mathcal{D}=\\{\\mathbf{x}_i \\in \\mathbb{R}^d,\\mathbf{y}_i \\in [0,1]\\}_{i=1}^N\\) containing \\(n\\) examples we need to estimate the parameter vector \\(\\mathbf{w}\\) by maximizing the likelihood of data.\nIn practice we minimize the negative log likelihood.\nLet \\( \\mu_i = \\text{Pr}[y_i=1|\\mathbf{x}_i,\\mathbf{w}] = \\sigma(\\mathbf{w}^T\\mathbf{x}_i)\\) be the model prediction for each example in the training dataset. The the negative log likelihood (NLL) is given by \\[ \\begin{align} L(\\mathbf{w}) \u0026= - \\sum_{i=1}^{N} \\log\\left[\\mu_i^{y_i}(1-\\mu_i)^{1-y_i}\\right] \\nonumber \\\\ \u0026= - \\sum_{i=1}^{N} \\left[ y_i\\log(\\mu_i) + (1-y_i)\\log(1-\\mu_i) \\right] \\nonumber \\\\ \\end{align} \\] This is referred to as the Binary Cross Entropy (BCE) loss. We need to choose the model parameters that optimizes (minimizes) the loss function. \\[ \\hat{\\mathbf{w}} = \\argmin_{\\mathbf{w}} L(\\mathbf{w}) \\] Loss function # Binary Cross Entropy (BCE) loss \\[ L(\\mathbf{w}) - \\sum_{i=1}^{N} \\left[ y_i\\log(\\mu_i) + (1-y_i)\\log(1-\\mu_i) \\right] \\] torch.nn.BCELoss torch.nn.BCEWithLogitsLoss Gradient # The gradient of the loss function if given by \\[ \\nabla_{\\mathbf{w}} L(\\mathbf{w}) = \\mathbf{X}^T\\left(\\sigma(\\mathbf{X}\\mathbf{w})-\\mathbf{y}\\right) \\] Collateral # https://pytorch.org/docs/stable/nn.html#loss-functions\n"},{"id":8,"href":"/docs/transformers/","title":"Transformers","section":"Docs","content":"Transformer architecture deep dive.\n"},{"id":9,"href":"/docs/transformers/transformers101/","title":"Transformers101","section":"Transformers","content":" Transformers 101 # A transformer has 3 main components.\nMulti-head scaled self-attention. MLP with residual connections and layer normalization. Positional encodings. TLDR # Transformer Description \\(\\mathbf{X}\\) embedding matrix \\(\\mathbf{X} = \\mathbf{X} + \\mathbf{R}\\) position encoding matrix \\(\\mathbf{Y} = \\text{TransformerLayer}[\\mathbf{X}]\\) transformer \\(\\mathbf{Y} = \\text{SoftMax}[\\mathbf{X}\\mathbf{X}^{T}] \\mathbf{X}\\) dot-product self-attention \\(\\mathbf{Y} = \\text{SoftMax}[\\mathbf{X} \\mathbf{W}^{q}\\mathbf{W}^{kT}\\mathbf{X}^{T}] \\mathbf{X} \\mathbf{W}^{v}\\) Query, Key, Value matrices \\(\\mathbf{Y} = \\text{SoftMax}[\\mathbf{Q}\\mathbf{K}^{T}] \\mathbf{V} \\) Query, Key, Value matrices \\(\\mathbf{Y} = \\text{SoftMax}[\\frac{\\mathbf{Q}\\mathbf{K}^{T}}{\\sqrt{D}}] \\mathbf{V} \\) Scaled dot-product self attention \\(\\mathbf{Y} = \\text{Concat}[\\mathbf{H}_1,...,\\mathbf{H}_H]\\mathbf{W}^o \\) where \\(\\mathbf{H}_h = \\text{SoftMax}\\left[\\frac{\\mathbf{Q_h}\\mathbf{K_h}^{T}}{\\sqrt{D_k}}\\right] \\mathbf{V_h}\\) Multi-head attention \\(\\mathbf{Z} = \\text{LayerNorm}\\left[\\mathbf{Y}(\\mathbf{X})+\\mathbf{X}\\right]\\) layer normalization and residual connection \\(\\mathbf{X^*} = \\text{LayerNorm}\\left[\\text{MLP}(\\mathbf{Z})+\\mathbf{Z}\\right]\\) MLP layer Parameter Count Description \\(\\mathbf{E}\\) \\(VD\\) The token embedding matrix. \\(V\\) is the size of the vocabulary and \\(D\\) is the dimensions of the embeddings. \\(\\mathbf{W}^q_h\\) \\(\\mathbf{W}^k_h\\) \\(\\mathbf{W}^v_h\\) \\(3HD^2\\) The query, key and the value matrices each of dimension \\(D \\times D \\) for the \\(H\\) heads. \\(\\mathbf{W}^o\\) \\(HD^2\\) The output matrix of dimension \\(HD \\times D \\) . \\(\\mathbf{W}^{ff}_{1}\\) \\(\\mathbf{W}^{ff}_{2}\\) \\(2DD_{ff}\\) The parameters of the two-layer MLP. \\(8D^2\\) Typically \\(D_{ff} = 4 D\\) \\((4H+8)D^2\\) total parameters Multi-head scaled self-attention. # Tokens # We will start with the concept of tokens. As token can be\nword sub-word image patch amino acid etc. Token embeddings # Let \\(\\mathbf{x}_n \\in \\mathbb{R}^D\\) be a column vector of \\(D\\) features corresponding to a token \\(n\\) .\nThis corresponds to the \\(D\\) -dimensional embedding vector of the token.\nEmbedding matrix # We can stack all the embedding vectors \\(\\left\\{\\mathbf{x}_n\\right\\}_{n=1}^{N}\\) for a sequence of \\(N\\) tokens as rows into an embedding matrix \\(\\mathbf{X}\\) .\n\\[\\mathbf{X}_{\\text{N (tokens)} \\times \\text{D (features)}}\\] Transformer layer # A transformer transforms the embedding matrix \\(\\mathbf{X}\\) to another matrix \\(\\mathbf{Y}\\) of the same dimension.\n\\[ \\mathbf{Y}_{N \\times D} = \\text{TransformerLayer}[\\mathbf{X}_{N \\times D}] \\] The goal of transformation is that the new space \\(\\mathbf{Y}\\) will have a richer internal representation that is better suited to solve downstream tasks.\nThe embeddings are trained to capture elementary semantic properties, words with similar meaning should map to nearby locations in the embedding space.\nA transformer can be viewed as a richer form of embedding in which the embedding vector for a token is mapped to a location that depends on the embedding vectors of other tokens in the sequence.\nI swam across the river to get to the other bank. (bank~water) I walked across the road to get cash from the bank. (bank~money) Attention # We do this via the notion of attention.\nTo determine the appropriate interpretation of the token bank the transformer processing a sentence should attend to (or give more importance to) specific words from the rest of the sequence.\nOriginally developed by Bahdanau, Cho, and Bengio, 2015 1 as an enhancement to RNNs for machine translation. Vaswani et al, 2017 2 later completely eliminated the recurrence structure and instead focussed only on the attention mechanism.\nWe will do this via the notion of attention where we generate the output transformed vector \\(\\mathbf{y}_n\\) via a linear combination of all the input vectors, that is, by attending to all the input vectors.\n\\[ \\mathbf{y}_n = \\sum_{m=1}^{N} a_{nm} \\mathbf{x}_m \\] \\(a_{nm}\\) are called attention weights/coefficients.\nThe attention coefficients should satisfy the following two properties.\n\\(a_{nm} \\geq 0\\) \\(\\sum_{m=1}^N a_{nm} =1\\) Partition of unity ( \\(0 \\leq a_{nm} \\leq 1\\) ).\nSelf-attention # We want to capture the notion of how similar a token is to other tokens.\nThis can be done via a dot product between the query vector ( \\(\\mathbf{x}_{n}\\) ) and the key vector ( \\(\\mathbf{x}_{m}\\) ).\n\\[ a_{nm} \\propto (\\mathbf{x}_{n}^T\\mathbf{x}_m) \\] The attention coefficients should satisfy the following two properties.\n\\(a_{nm} \\geq 0\\) \\(\\sum_{m=1}^N a_{nm} =1\\) This can be achieved by a soft-max of the dot products.\n\\[ a_{nm} = \\text{SoftMax}(\\mathbf{x}_{n}^T\\mathbf{x}_m) = \\frac{\\exp(\\mathbf{x}_{n}^T\\mathbf{x}_m)}{\\sum_{m'=1}^{N} \\exp(\\mathbf{x}_{n}^T\\mathbf{x}_m')} \\] \\[ \\mathbf{y}_n = \\sum_{m=1}^{N} \\text{SoftMax}(\\mathbf{x}_{n}^T\\mathbf{x}_m) \\mathbf{x}_m \\] Query, Key, Value # A bit of terminology taken from the IR literature.\nQuery The search query that the user types on a search engine. Key The feature representation of each document. Value The actual document. The query is attending to a particular value whose key closely matches the query (hard attention). \\[ \\mathbf{y}_n = \\sum_{m=1}^{N} \\text{SoftMax}(\\mathbf{x}_{n}^T\\mathbf{x}_m) \\mathbf{x}_m \\] \\(\\mathbf{x}_{n}\\) is the query. \\(\\mathbf{x}_m\\) ( \\(m=1,...N\\) ) are the keys. \\(\\mathbf{x}_m\\) ( \\(m=1,...N\\) ) are the values. Dot-product self-attention # So we now have the first definition of the transformer layer.\n\\[ \\mathbf{Y}_{N \\times D} = \\text{TransformerLayer}[\\mathbf{X}_{N \\times D}] \\] \\[\\mathbf{Y} = \\text{SoftMax}[\\mathbf{X}\\mathbf{X}^{T}] \\mathbf{X}\\] \\[\\mathbf{Y}_{N \\times D} = \\text{SoftMax}[\\mathbf{X}_{N \\times D} \\mathbf{X}^{T}_{D \\times N} ] \\mathbf{X}_{N \\times D} \\] Other than the embedding matrix this has no no learnable parameters yet.\nParameter Count Description \\(\\mathbf{E}\\) \\(VD\\) The token embedding matrix. \\(V\\) is the size of the vocabulary and \\(D\\) is the dimensions of the mebeddings. \\(\\mathcal{O}(2N^2D)\\) computations.\nNetwork parameters # The above has no learnable parameters.\nEach feature value \\(x_{ni}\\) is equally important in the dot product.\nWe will introduce a \\(D \\times D\\) matrix \\(\\mathbf{U}\\) of learnable weights.\n\\[\\mathbf{X^{*}}_{N \\times D}=\\mathbf{X}_{N \\times D}\\mathbf{U}_{D \\times D}\\] With this we have the second iteration of the transformer layer.\n\\[\\mathbf{Y} = \\text{SoftMax}[\\mathbf{X}\\mathbf{U}\\mathbf{U}^{T}\\mathbf{X}^{T}] \\mathbf{X}\\mathbf{U}\\] We now have \\(D^2\\) learnable parameters. \\(\\mathcal{O}(2N^2D+ND^2)\\) computations\nWhile this has more flexibility the matrix \\(\\mathbf{X}\\mathbf{U}\\mathbf{U}^{T}\\mathbf{X}^{T}\\) is symmetric, whereas we would like the attention matrix to support significant asymmetry.\nThe word chisel should be strongly associated with tool since every chisel is a tool. The word tool should be weakly associated with the word chisel since there are many other kinds of tools besides chisel. Query, Key, Value matrices # To overcome these limitations, we introduce separate Query, Key, Value matrices each having their own independent linear transformations.\n\\[\\mathbf{Q} = \\mathbf{X} \\mathbf{W}^{q}\\] \\[\\mathbf{K} = \\mathbf{X} \\mathbf{W}^{k}\\] \\[\\mathbf{V} = \\mathbf{X} \\mathbf{W}^{v}\\] Let\u0026rsquo;s check the dimensions.\n\\[\\mathbf{Q}_{N \\times D_q} = \\mathbf{X}_{N \\times D} \\mathbf{W}^{q}_{D \\times D_q}\\] \\[\\mathbf{K}_{N \\times D_k} = \\mathbf{X}_{N \\times D} \\mathbf{W}^{k}_{D \\times D_k}\\] \\[\\mathbf{V}_{N \\times D_v} = \\mathbf{X}_{N \\times D} \\mathbf{W}^{v}_{D \\times D_v}\\] Typically \\(D_q=D_k\\) \\[\\mathbf{Q}_{N \\times D_k} = \\mathbf{X}_{N \\times D} \\mathbf{W}^{q}_{D \\times D_k}\\] \\[\\mathbf{K}_{N \\times D_k} = \\mathbf{X}_{N \\times D} \\mathbf{W}^{k}_{D \\times D_k}\\] \\[\\mathbf{V}_{N \\times D_v} = \\mathbf{X}_{N \\times D} \\mathbf{W}^{v}_{D \\times D_v}\\] With this we have the third iteration of the transformer layer.\n\\[\\mathbf{Y} = \\text{SoftMax}[\\mathbf{Q}\\mathbf{K}^{T}] \\mathbf{V}\\] Let\u0026rsquo;s check the dimensions once.\n\\[\\mathbf{Y}_{N \\times D_v} = \\text{SoftMax}[\\mathbf{Q}_{N \\times D_k}\\mathbf{K}^{T}_{D_k \\times N}] \\mathbf{V}_{N \\times D_v}\\] A common choice is \\(D_k=D_v=D\\) . This also makes the output dimension same as the input and helps later to add residual connections.\n\\[\\mathbf{Y}_{N \\times D} = \\text{SoftMax}[\\mathbf{Q}_{N \\times D}\\mathbf{K}^{T}_{D \\times N}] \\mathbf{V}_{N \\times D}\\] We now have \\(3D^2\\) learnable parameters.\nParameter Count Description \\(\\mathbf{E}\\) \\(VD\\) The token embedding matrix. \\(V\\) is the size of the vocabulary and \\(D\\) is the dimensions of the embeddings. \\(\\mathbf{W}^q\\) \\(\\mathbf{W}^k\\) \\(\\mathbf{W}^v\\) \\(3D^2\\) The query, key and the value matrices each of dimension \\(D \\times D\\) \\(\\mathcal{O}(2N^2D+3ND^2)\\) computations\nRecap # Transformer Description \\(\\mathbf{Y} = \\text{TransformerLayer}[\\mathbf{X}]\\) transformer \\(\\mathbf{Y} = \\text{SoftMax}[\\mathbf{X}\\mathbf{X}^{T}] \\mathbf{X}\\) dot-product self-attention \\(\\mathbf{Y} = \\text{SoftMax}[\\mathbf{X}\\mathbf{U}\\mathbf{U}^{T}\\mathbf{X}^{T}] \\mathbf{X}\\mathbf{U}\\) learnable parameters \\(\\mathbf{Y} = \\text{SoftMax}[\\mathbf{X} \\mathbf{W}^{q}\\mathbf{W}^{kT}\\mathbf{X}^{T}] \\mathbf{X} \\mathbf{W}^{v}\\) Query, Key, Value matrices \\(\\mathbf{Y} = \\text{SoftMax}[\\mathbf{Q}\\mathbf{K}^{T}] \\mathbf{V} \\) Query, Key, Value matrices Scaled dot-product self-attention # The gradients of the soft-max become exponentially small for inputs of high magnitude.\nHence we scale it as follows.\n\\[\\mathbf{Y} = \\text{SoftMax}\\left[\\frac{\\mathbf{Q}\\mathbf{K}^{T}}{\\sqrt{D_k}}\\right] \\mathbf{V}\\] \\(D_k\\) which is the variance of the dot-product.\nIf the elements of the query and key vectors were all independent random variables with zero mean and unit variance, then the variance of the dot product would be \\(D_k\\) Single attention head # This is known a s single attention head. \\[\\mathbf{Y} = \\text{SoftMax}\\left[\\frac{\\mathbf{Q}\\mathbf{K}^{T}}{\\sqrt{D_k}}\\right] \\mathbf{V}\\] Multi-head attention # Capture multiple patterns of attention (for example, tense, vocabulary etc.).\nSort of similar to using multiple different filters in each layer of a convolutional neural network.\nWe have \\(H\\) attention heads indexed by \\(h=1,...,H\\) .\n\\[\\mathbf{H}_h = \\text{Attention}(\\mathbf{Q_h},\\mathbf{K_h},\\mathbf{V_h}) = \\text{SoftMax}\\left[\\frac{\\mathbf{Q_h}\\mathbf{K_h}^{T}}{\\sqrt{D_k}}\\right] \\mathbf{V_h}\\] \\[\\mathbf{Q}_h = \\mathbf{X} \\mathbf{W}^{q}_h\\] \\[\\mathbf{K}_h = \\mathbf{X} \\mathbf{W}^{k}_h\\] \\[\\mathbf{V}_h = \\mathbf{X} \\mathbf{W}^{v}_h\\] The output from each heads are first concatenated into a single matrix and then linearly transformed using another matrix.\n\\[\\mathbf{Y}(\\mathbf{X}) =\\text{Concat}[\\mathbf{H}_1,...,\\mathbf{H}_H]\\mathbf{W}^o\\] Let\u0026rsquo;s check the dimensions\n\\[\\mathbf{Y}_{N \\times D} =\\text{Concat}[\\mathbf{H}_1,...,\\mathbf{H}_H]_{N \\times HD_v} \\mathbf{W}^o_{HD_v \\times D}\\] Typically \\(D_v = D/H\\) .\nWe now have \\(3HD^2\\) learnable parameters.\nParameter Count Description \\(\\mathbf{E}\\) \\(VD\\) The token embedding matrix. \\(V\\) is the size of the vocabulary and \\(D\\) is the dimensions of the embeddings. \\(\\mathbf{W}^q_h\\) \\(\\mathbf{W}^k_h\\) \\(\\mathbf{W}^v_h\\) \\(3HD^2\\) The query, key and the value matrices each of dimension \\(D \\times D \\) for the \\(H\\) heads. \\(\\mathbf{W}^o\\) \\(HD^2\\) The output matrix of dimension \\(HD \\times D \\) . \\(\\mathcal{O}(2HN^2D+4HND^2)\\) computations\nRecap # Transformer Description \\(\\mathbf{Y} = \\text{TransformerLayer}[\\mathbf{X}]\\) transformer \\(\\mathbf{Y} = \\text{SoftMax}[\\mathbf{X}\\mathbf{X}^{T}] \\mathbf{X}\\) dot-product self-attention \\(\\mathbf{Y} = \\text{SoftMax}[\\mathbf{X}\\mathbf{U}\\mathbf{U}^{T}\\mathbf{X}^{T}] \\mathbf{X}\\mathbf{U}\\) learnable parameters \\(\\mathbf{Y} = \\text{SoftMax}[\\mathbf{X} \\mathbf{W}^{q}\\mathbf{W}^{kT}\\mathbf{X}^{T}] \\mathbf{X} \\mathbf{W}^{v}\\) Query, Key, Value matrices \\(\\mathbf{Y} = \\text{SoftMax}[\\mathbf{Q}\\mathbf{K}^{T}] \\mathbf{V} \\) Query, Key, Value matrices \\(\\mathbf{Y} = \\text{SoftMax}[\\frac{\\mathbf{Q}\\mathbf{K}^{T}}{\\sqrt{D}}] \\mathbf{V} \\) Scaled dot-product self attention \\(\\mathbf{Y} = \\text{Concat}[\\mathbf{H}_1,...,\\mathbf{H}_H]\\mathbf{W}^o \\) where \\(\\mathbf{H}_h = \\text{SoftMax}\\left[\\frac{\\mathbf{Q_h}\\mathbf{K_h}^{T}}{\\sqrt{D_k}}\\right] \\mathbf{V_h}\\) Multi-head attention MLP layers # Residual connections # To improve training efficiency we introduce residual connections that bypass the multi-head structure.\n\\[\\mathbf{Y}(\\mathbf{X}) = \\text{TransformerLayer}[\\mathbf{X}]\\] \\[\\mathbf{Z} = \\mathbf{Y}(\\mathbf{X})+\\mathbf{X}\\] Layer normalization # Layer normalization is then added also to improve training efficiency.\nLayer Normalization, J. L. Bao, J. R. Kiros, and G. E. Hinton, 2016\nPost-norm # \\[\\mathbf{Z} = \\text{LayerNorm}\\left[\\mathbf{Y}(\\mathbf{X})+\\mathbf{X}\\right]\\] Pre-norm # \\[\\mathbf{Z} = \\mathbf{Y}(\\text{LayerNorm}\\left[\\mathbf{X}\\right])+\\mathbf{X}\\] Pre-norm is most widely used these days while the original paper used post-norm.\nMLP layer # We further add a MLP layer, for example, a two layer fully connected network with ReLU hidden units (typically bias is excluded).\n\\[\\mathbf{X^*} = \\text{MLP}(\\mathbf{Z})=\\text{R/GeLU}(\\mathbf{Z}\\mathbf{W}^{ff}_{1})\\mathbf{W}^{ff}_2\\] Let\u0026rsquo;s check the dimensions.\n\\[\\mathbf{X^*} = \\text{R/GeLU}(\\mathbf{Z}_{N\\times D}{\\mathbf{W}^{ff1}_{1}}_{D \\times D_{ff}}){\\mathbf{W}^{ff}_{2}}_{D_{ff} \\times D}\\] Typically \\(D_{ff} = 4 D\\) Residual connection again # \\[\\mathbf{X^*} = \\text{MLP}(\\mathbf{Z})+\\mathbf{Z}\\] Layer normalization again # \\[\\mathbf{X^*} = \\text{LayerNorm}\\left[\\text{MLP}(\\mathbf{Z})+\\mathbf{Z}\\right]\\] We now have \\(2DD_{ff}\\) learnable parameters.\nParameter Count Description \\(\\mathbf{E}\\) \\(VD\\) The token embedding matrix. \\(V\\) is the size of the vocabulary and \\(D\\) is the dimensions of the embeddings. \\(\\mathbf{W}^q_h\\) \\(\\mathbf{W}^k_h\\) \\(\\mathbf{W}^v_h\\) \\(3HD^2\\) The query, key and the value matrices each of dimension \\(D \\times D \\) for the \\(H\\) heads. \\(\\mathbf{W}^o\\) \\(HD^2\\) The output matrix of dimension \\(HD \\times D \\) . \\(\\mathbf{W}^{ff}_{1}\\) \\(\\mathbf{W}^{ff}_{2}\\) \\(2DD_{ff}\\) The parameters of the two-layer MLP. \\(8D^2\\) Typically \\(D_{ff} = 4 D\\) \\(\\mathcal{O}(2HN^2D+4HND^2+2NDD_{ff})\\) computations\n\\(\\mathcal{O}(2HN^2D+4HND^2+8ND^2)\\) computations\nRecap # Transformer Description \\(\\mathbf{Y} = \\text{TransformerLayer}[\\mathbf{X}]\\) transformer \\(\\mathbf{Y} = \\text{SoftMax}[\\mathbf{X}\\mathbf{X}^{T}] \\mathbf{X}\\) dot-product self-attention \\(\\mathbf{Y} = \\text{SoftMax}[\\mathbf{X} \\mathbf{W}^{q}\\mathbf{W}^{kT}\\mathbf{X}^{T}] \\mathbf{X} \\mathbf{W}^{v}\\) Query, Key, Value matrices \\(\\mathbf{Y} = \\text{SoftMax}[\\mathbf{Q}\\mathbf{K}^{T}] \\mathbf{V} \\) Query, Key, Value matrices \\(\\mathbf{Y} = \\text{SoftMax}[\\frac{\\mathbf{Q}\\mathbf{K}^{T}}{\\sqrt{D}}] \\mathbf{V} \\) Scaled dot-product self attention \\(\\mathbf{Y} = \\text{Concat}[\\mathbf{H}_1,...,\\mathbf{H}_H]\\mathbf{W}^o \\) where \\(\\mathbf{H}_h = \\text{SoftMax}\\left[\\frac{\\mathbf{Q_h}\\mathbf{K_h}^{T}}{\\sqrt{D_k}}\\right] \\mathbf{V_h}\\) Multi-head attention \\(\\mathbf{Z} = \\text{LayerNorm}\\left[\\mathbf{Y}(\\mathbf{X})+\\mathbf{X}\\right]\\) layer normalization and residual connection \\(\\mathbf{X^*} = \\text{LayerNorm}\\left[\\text{MLP}(\\mathbf{Z})+\\mathbf{Z}\\right]\\) MLP layer Positional encodings # A transformer is equivariant with respect to input permutation, that is, it does not depend on the order of the tokens.\nThe food was bad, not good at all. The food was good, not bad at all. We need to find a way to inject the token order information.\nWe add a position encoding vector ( \\(\\mathbf{r}_n\\) ) to each token vector ( \\(\\mathbf{x}_n\\) ).\n\\[\\mathbf{x}_n^* = \\mathbf{x}_n + \\mathbf{r}_n\\] We could associate an integer 1, 2, 3\u0026hellip; with each position.\nMagnitude of the value may increase without bound and corrupt the embedding vector. Will not generalize well the new input sequences longer than the training data. Desiderata (Dufter, Schmitt, and Schutze, 2021)\nUnique representation for each token. Should be bounded. Generalize to longer sequences. Compute relative position of tokens. Sinusoidal functions # For a given position \\(n\\) the position encoding vector has components \\(\\mathbf{r}_{ni}\\) given by sinusoids of steadily increasing wavelengths.\n\\[\\mathbf{r}_{ni} = \\text{sin}\\left(\\frac{n}{L^{i/D}}\\right) \\text{if i is even}\\] \\[\\mathbf{r}_{ni} = \\text{cos}\\left(\\frac{n}{L^{(i-1)/D}}\\right) \\text{if i is odd}\\] Sort of binary representation of numbers.\nRoPE # https://arxiv.org/pdf/2104.09864\nSummary # Transformer Description \\(\\mathbf{X}\\) embedding matrix \\(\\mathbf{X} = \\mathbf{X} + \\mathbf{R}\\) position encoding matrix \\(\\mathbf{Y} = \\text{TransformerLayer}[\\mathbf{X}]\\) transformer \\(\\mathbf{Y} = \\text{SoftMax}[\\mathbf{X}\\mathbf{X}^{T}] \\mathbf{X}\\) dot-product self-attention \\(\\mathbf{Y} = \\text{SoftMax}[\\mathbf{X} \\mathbf{W}^{q}\\mathbf{W}^{kT}\\mathbf{X}^{T}] \\mathbf{X} \\mathbf{W}^{v}\\) Query, Key, Value matrices \\(\\mathbf{Y} = \\text{SoftMax}[\\mathbf{Q}\\mathbf{K}^{T}] \\mathbf{V} \\) Query, Key, Value matrices \\(\\mathbf{Y} = \\text{SoftMax}[\\frac{\\mathbf{Q}\\mathbf{K}^{T}}{\\sqrt{D}}] \\mathbf{V} \\) Scaled dot-product self attention \\(\\mathbf{Y} = \\text{Concat}[\\mathbf{H}_1,...,\\mathbf{H}_H]\\mathbf{W}^o \\) where \\(\\mathbf{H}_h = \\text{SoftMax}\\left[\\frac{\\mathbf{Q_h}\\mathbf{K_h}^{T}}{\\sqrt{D_k}}\\right] \\mathbf{V_h}\\) Multi-head attention \\(\\mathbf{Z} = \\text{LayerNorm}\\left[\\mathbf{Y}(\\mathbf{X})+\\mathbf{X}\\right]\\) layer normalization and residual connection \\(\\mathbf{X^*} = \\text{LayerNorm}\\left[\\text{MLP}(\\mathbf{Z})+\\mathbf{Z}\\right]\\) MLP layer Final model # --- title: Transformer --- stateDiagram-v2 direction BT Input: Sequence of token ids Output: Output layers TE: Token Embedding PE: Position Embedding T1: Transformer layer T2: Transformer layer T3: ... T4: Transformer layer Add: Add Dropout: Dropout Norm: Norm Input --\u003e TE Input --\u003e PE TE --\u003e Add PE --\u003e Add Add --\u003e Dropout Dropout --\u003e T1 T1 --\u003e T2 T2 --\u003e T3 T3 --\u003e T4 T4 --\u003e Norm Norm --\u003e Output --- title: Transformer layer (pre-norm variant) --- stateDiagram-v2 direction BT X: $$\\mathbf{X}_{N \\times D}$$ Y: $$\\mathbf{X}^*_{N \\times D}$$ MHSA: Multi-head self-attention (MHSA) MLP: Multi-layer perceptron (MLP) Norm1: Norm Norm2: Norm Dropout1: Dropout Dropout2: Dropout Add1: Add Add2: Add X --\u003e Norm1 Norm1 --\u003e MHSA MHSA --\u003e Dropout1 Dropout1 --\u003e Add1 X --\u003e Add1 Add1 --\u003e Norm2 Norm2 --\u003e MLP MLP --\u003e Dropout2 Dropout2 --\u003e Add2 Add1 --\u003e Add2 Add2 --\u003e Y note right of Add1 : $$Z = \\text{MHSA}(\\text{Norm}\\left[\\mathbf{X}\\right])+\\mathbf{X}$$ note right of Add2 : $$X^* = \\text{MLP}(\\text{Norm}\\left[\\mathbf{Z}\\right])+\\mathbf{Z}$$ References # Neural Machine Translation by Jointly Learning to Align and Translate, D. Bahdanau, K. Cho, Y. Bengio, ICLR 2015.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nAttention Is All You Need, A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones, A. N. Gomez, L. Kaiser, I. Polosukhin, NeurIPS 2017.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n"},{"id":10,"href":"/docs/training/autograd/","title":"AutoDiff","section":"Training","content":" Automatic differentiation # Algorithmic differentiation, autodiff, autograd\nThere are broadly 4 appoaches to compute derivatives.\nApproach Pros Cons Manual derivation of backprop equations. If done carefully can result in efficient code. Manual process, prone to errors and not easy to iterate on models Numerical evaluation of gradients via finite differences. Sometimes used to check for correctness of other methods. Limited by computational accuracy. Scales poorly with the size of the network. Symbolic differentiation using packages like sympy Closed form needed. Resulting expression can be very long (expression swell). Automatic differentiation Most preferred. Atılım Günes Baydin, Barak A. Pearlmutter, Alexey Andreyevich Radul, and Jeffrey Mark Siskind. 2017. Automatic differentiation in machine learning: a survey. J. Mach. Learn. Res. 18, 1 (January 2017), 5595–5637.\nForward-mode automatic differentiation # We augment each intermediate variable \\(z_i\\) (known as primal variable) with an additional variable representing the value of some derivative of that variable, which we denote as \\(\\dot{z}_i\\) , known as tangent variable. The tangent variables are generated automatically.\nConsider the following function. \\[ f(x_1,x_2) = x_1x_2 + \\exp(x_1x_2) - \\sin(x_2) \\] When implemented in software the code consists of a sequence of operations than can be expressed as an evaluation trace of the underlying elementary operations. This trace can be visualized as a computation graph with respect to the following 7 primal variables. stateDiagram-v2 direction LR x1: $$x_1$$ x2: $$x_2$$ v1: $$v_1 = x_1$$ v2: $$v_2 = x_2$$ v3: $$v_3 = v_1v_2$$ v4: $$v_4 = \\sin(v_2)$$ v5: $$v_5 = \\exp(v_3)$$ v6: $$v_6 = v_3 - v_4$$ v7: $$v_7 = v_5 + v_6$$ f: $$f = v_5 + v_6$$ x1 --\u003e v1 x2 --\u003e v2 v1 --\u003e v3 v2 --\u003e v4 v2 --\u003e v3 v3 --\u003e v5 v4 --\u003e v6 v3 --\u003e v6 v5 --\u003e v7 v6 --\u003e v7 v7 --\u003e f We first write code to implement the evaluation of the primal variables. \\[ v_1 = x_1 \\] \\[ v_2 = x_2 \\] \\[ v_3 = v_1v_2 \\] \\[ v_4 = \\sin(v_2) \\] \\[ v_5 = \\exp(v_3) \\] \\[ v_6 = v_3 - v_4 \\] \\[ v_7 = v_5 + v_6 \\] Now say we wish to evaluate the derivative \\(\\partial f/\\partial x_1\\) . First we define the tangent variables by \\[\\dot{v}_i = \\frac{\\partial v_i}{\\partial x_1}\\] Expressions for evaluating these can be constructed automatically using the chain rule of calculus. \\[ \\dot{v}_i = \\frac{\\partial v_i}{\\partial x_1} = \\sum_{j\\in\\text{parents}(i)} \\frac{\\partial v_i}{\\partial v_j} \\frac{\\partial v_j}{\\partial x_1} = \\sum_{j\\in\\text{parents}(i)} \\dot{v}_j \\frac{\\partial v_i}{\\partial v_j} \\] where \\(\\text{parents}(i)\\) denotes the set of parents of node \\(i\\) in the evaluation trace diagram.\nThe associated equations and corresponding code for evaluating the tangent variables are generated automatically. \\[ \\dot{v}_1 = 1 \\] \\[ \\dot{v}_2 = 0 \\] \\[ \\dot{v}_3 = v_1\\dot{v}_2+\\dot{v}_1v_2 \\] \\[ \\dot{v}_4 = \\dot{v}_2\\cos(v_2) \\] \\[ \\dot{v}_5 = \\dot{v}_3\\exp(v_3) \\] \\[ \\dot{v}_6 = \\dot{v}_3 - \\dot{v}_4 \\] \\[ \\dot{v}_7 = \\dot{v}_5 + \\dot{v}_6 \\] To evaluate the derivative \\(\\frac{\\partial f}{\\partial x_1}\\) we input specific values of \\(x_1\\) and \\(x_2\\) and the code then executes the primal and tangent equations, numerically evaluating the tuples \\((v_i,\\dot{v}_i)\\) in forward order until we obtain the required derivative.\nThe forward mode with slight modifications can handle multiple outputs in the same pass but the process has to be repeated for every parameter that we need the derivative. Since we are often in the regime of one output with millions of parameters this is not scalable for modern deep neural networks. We therefore turn to an alternative version based on the backwards flow of derivative data through the evaluation trace graph.\nReverse-mode automatic differentiation # Reverse-mode automatic differentiation is a generalization of the error backpropagation procedure we discussed earlier.\nAs with forward mode, we augment each primal variable \\(v_i\\) with an additional variable called adjoint variable, denoted as \\(\\bar{v}_i\\) . \\[\\bar{v}_i = \\frac{\\partial f}{\\partial v_i}\\] Expressions for evaluating these can be constructed automatically using the chain rule of calculus. \\[ \\bar{v}_i = \\frac{\\partial f}{\\partial v_i} = \\sum_{j\\in\\text{children}(i)} \\frac{\\partial f}{\\partial v_j} \\frac{\\partial v_j}{\\partial v_i} = \\sum_{j\\in\\text{children}(i)} \\bar{v}_j \\frac{\\partial v_j}{\\partial v_i} \\] where \\(\\text{children}(i)\\) denotes the set of children of node i in the evaluation trace diagram.\nThe successive evaluation of the adjoint variables represents a flow of information backwards through the graph. For multiple parameters a single backward pass is enough. Reverse mode is more memory intensive than forward mode.\n\\[ \\bar{v}_7 = 1 \\] \\[ \\bar{v}_6 = \\bar{v}_7 \\] \\[ \\bar{v}_5 = \\bar{v}_7 \\] \\[ \\bar{v}_4 = -\\bar{v}_6 \\] \\[ \\bar{v}_3 = \\bar{v}_5v_5+\\bar{v}_6 \\] \\[ \\bar{v}_2 = \\bar{v}_2v_1+\\bar{v}_4\\cos(v_2) \\] \\[ \\bar{v}_1 = \\bar{v}_3v_2 \\] Autograd in pytorch # A Gentle Introduction to torch.autograd The Fundamentals of Autograd "},{"id":11,"href":"/docs/training/initialization/","title":"Initialization","section":"Training","content":" Parameter initialization # Initialization before starting the gradient descent.\nAvoid all parameters set to same value. (symmetry breaking)\nUniform distribution in the range \\([-\\epsilon,\\epsilon]\\) Zero-mean Gaussian \\(\\mathcal{N}(0,\\epsilon^2)\\) nn.init "},{"id":12,"href":"/docs/rl/","title":"Reinforcement","section":"Docs","content":" Reinforcement Learning basics.\n"},{"id":13,"href":"/docs/neural_networks/softmax_regression/","title":"Softmax Regression","section":"Neural Networks","content":" Softmax Regression # Softmax Regression is a single layer neural network for multi-class classification.\nstateDiagram-v2 direction LR x1: $$x_1$$ x2: $$x_2$$ x3: $$x_3$$ xd: $$x_d$$ a1: $$a_1=\\sum_i w_{1i} x_i$$ a2: $$a_2=\\sum_i w_{2i} x_i$$ ak: $$a_k=\\sum_i w_{ki} x_i$$ z1: $$z_1=\\text{softmax}(\\mathbf{a})_1$$ z2: $$z_2=\\text{softmax}(\\mathbf{a})_2$$ zk: $$z_k=\\text{softmax}(\\mathbf{a})_k$$ x1 --\u003e a1:$$w_{11}$$ x2 --\u003e a1:$$w_{12}$$ x3 --\u003e a1:$$w_{13}$$ xd --\u003e a1:$$w_{1d}$$ x1 --\u003e a2:$$w_{21}$$ x2 --\u003e a2:$$w_{22}$$ x3 --\u003e a2:$$w_{23}$$ xd --\u003e a2:$$w_{2d}$$ x1 --\u003e ak:$$w_{k1}$$ x2 --\u003e ak:$$w_{k2}$$ x3 --\u003e ak:$$w_{k3}$$ xd --\u003e ak:$$w_{kd}$$ a1 --\u003e z1 a2 --\u003e z2 ak --\u003e zk z1 --\u003e END1:::hidden z2 --\u003e END2:::hidden zk --\u003e END:::hidden note left of xd : Inputs note right of a1 : Pre-activations note left of zk : Activations note left of END : Outputs classDef hidden display: none; Statistical model # Given \\(k\\) classes the probability of class \\(i\\) for a given feature vector ( \\(\\mathbf{x}\\in \\mathbb{R}^d\\) ) is given by \\[ \\text{Pr}[y=i|\\mathbf{x},(\\mathbf{w}_1,\\mathbf{w}_2,...,\\mathbf{w}_k)] = \\frac{\\exp(\\mathbf{w}_i^T\\mathbf{x})}{\\sum_{j=1}^{k} \\exp(\\mathbf{w}_j^T\\mathbf{x})} \\] where \\(\\mathbf{w}_1,\\mathbf{w}_2,...,\\mathbf{w}_k\\in \\mathbb{R}^d\\) are the weight vector or parameters of the model for each class.\nStacking the weights vectors \\(\\mathbf{w}_1,\\mathbf{w}_2,...,\\mathbf{w}_k\\in \\mathbb{R}^d\\) into a weight matrix \\(\\mathbf{W} \\in \\mathbb{R}^{d \\times k}\\) we can write the pre-activation vector \\(\\mathbf{a} \\in \\mathbb{R}^{k}\\) as follows. \\[ \\mathbf{a} = \\mathbf{W}^T\\mathbf{x} \\] The activation vector \\(\\mathbf{z} \\in \\mathbb{R}^{k}\\) is given by \\[ \\mathbf{z} = \\text{softmax}(\\mathbf{a}) \\] and the softmax activation function is defined as \\[ \\text{softmax}(\\mathbf{a})_i = \\frac{\\exp(\\mathbf{a}_i)}{\\sum_{j=1}^{k} \\exp(\\mathbf{a}_j)} \\] Hence \\[ \\text{Pr}[y=i|\\mathbf{x},\\mathbf{W}] = \\text{softmax}(\\mathbf{W}^T\\mathbf{x})_i \\] We often stack all the \\(n\\) examples into a design matrix \\(\\mathbf{X} \\in \\mathbb{R^{n \\times d}}\\) , where each row is one instance. The predictions for all the \\(n\\) instances \\(\\mathbf{y} \\in \\mathbb{R}^{n \\times K }\\) can be written conveniently as a matrix-vector product. \\[ \\mathbf{y} = \\text{softmax}(\\mathbf{X}\\mathbf{W}) \\] Likelihood # Given a dataset \\(\\mathcal{D}=\\{\\mathbf{x}_i \\in \\mathbb{R}^d,\\mathbf{y}_i \\in [1,2,..,k]\\}_{i=1}^n\\) containing \\(n\\) examples we need to estimate the parameter vector \\(\\mathbf{W}\\) by maximizing the likelihood of data.\nIn practice we minimize the negative log likelihood.\nLet \\(\\mu_i^j\\) be the model prediction for class j for each example i in the training dataset. \\[ \\mu_i^j = \\text{Pr}[y_i=j|\\mathbf{x}_i,\\mathbf{W}] = \\text{softmax}(\\mathbf{W}^T\\mathbf{x}_i)_j \\] Let \\(y_i^j\\) be the corresponding true label. The negative log likelihood (NLL) is given by \\[ L(\\mathbf{W}) = - \\sum_{i=1}^{n} \\sum_{j=1}^{k} y_i^j \\log \\mu_i^j \\] This is referred to as the Cross Entropy loss. We need to choose the model parameters that optimizes (minimizes) the loss function. \\[ \\hat{\\mathbf{W}} = \\argmin_{\\mathbf{W}} L(\\mathbf{W}) \\] Loss function # Cross Entropy loss \\[ L(\\mathbf{W}) = - \\sum_{i=1}^{n} \\sum_{j=1}^{k} y_i^j \\log \\mu_i^j \\] Compare to the earlier Binary Cross Entropy loss \\[ L(\\mathbf{w}) - \\sum_{i=1}^{n} \\left[ y_i\\log(\\mu_i) + (1-y_i)\\log(1-\\mu_i) \\right] \\] torch.nn.CrossEntropyLoss "},{"id":14,"href":"/docs/transformers/transformers102/","title":"Transformers102","section":"Transformers","content":" Transformer Language Models # category task example sample use case Decoder vec2seq GPT chat, image captioning Encoder seq2vec BERT sentiment analysis Encoder-Decoder seq2seq T5 machine translation Auto-regressive models # Decompose the distribution over sequence of tokens into a product of conditional distributions.\n\\[p(x_1,...,x_N)=\\prod_{n=1}^{N} p(x_n|x_1,...,x_{n-1})\\] Markov models # Assume the conditional distribution is independent of all previous tokens except the \\(L\\) most recent tokens (known as \\(n\\) -gram models).\nBi-gram model # \\(L=1\\) bi-gram model\n\\[p(x_1,...,x_N)=p(x_1)p(x_2|x_1)\\prod_{n=3}^{N} p(x_n|x_{n-1})\\] Tri-gram model # \\(L=2\\) tri-gram model\n\\[p(x_1,...,x_N)=p(x_1)p(x_2|x_1)p(x_3|x_1,x_2)\\prod_{n=4}^{N} p(x_n|x_{n-1},x_{n-2})\\] Large Language Models in Machine Translation, Thorsten Brants, Ashok C. Popat, Peng Xu, Franz J. Och, Jeffrey Dean, 2007.\nDecoder transformers # vec2seq - Generative models that output sequence of tokens. GPT (Generative Pre-trained Transformer)\nUse the transformer architecture to construct an auto-regressive model.\n\\[p(x_1,...,x_N)=\\prod_{n=1}^{N} p(x_n|x_1,...,x_{n-1})\\] The conditional distribution \\(p(x_n|x_1,...,x_{n-1})\\) is modelled using a transformer.\nDecoding at a high level # The model takes as input a sequence of the first \\(n-1\\) tokens. The output represents the conditional distribution for token \\(n\\) . Draw a sample from this distribution to extend the sequence now to \\(n\\) tokens. This new sequence can now be fed back to the model to generate a distribution over token \\(n+1\\) . \u0026hellip; Add a linear-softmax layer # Input is a sequence of \\(N\\) tokens \\(x_1,...,x_N\\) each of dimensionality \\(D\\) .\nTransformer layer \\[\\mathbf{\\widetilde{X}} = \\text{TransformerLayer}[\\mathbf{X}]\\] Output is a sequence of \\(N\\) tokens \\(\\widetilde{x}_1,...,\\widetilde{x}_N\\) each of dimensionality \\(D\\) .\nEach output needs to be a probability distribution over the vocabulary of \\(K\\) tokens.\nAdd a linear-softmax layer.\n\\[\\mathbf{Y}_{N \\times K} = \\text{SoftMax}(\\mathbf{\\widetilde{X}}_{N \\times D} {\\mathbf{W}^p}_{D \\times K})\\] \\(\\mathbf{Y}\\) is matrix whose \\(n\\) th row is \\(y_n^{T}\\) .\nEach softmax output unit has an associated cross-entropy loss.\nSelf-supervised training # The model can be trained using a large corpus of unlabelled natural language data in a self-supervised fashion.\n(input) \\(x_1,...,x_n\\) -\u0026gt; (output) \\(x_{n+1}\\) Each example can be processed independently.\nWe can actually process the entire sequence. Shift the input sequence right by one step so that each token acts both as a target value for the sequence of previous tokens and as an input value for subsequent tokens,\nINPUT \u0026lt;start\u0026gt; \\(x_1\\) \\(x_2\\) \\(x_3\\) \\(...\\) \\(x_n\\) OUPUT \\(y_1\\) \\(y_2\\) \\(y_3\\) \\(y_4\\) \\(...\\) \\(y_{n+1}\\) TARGET \\(x_1\\) \\(x_2\\) \\(x_3\\) \\(x_4\\) \\(...\\) \\(x_{n+1}\\) Masked attention # The transformer can simply learn to copy the next input directly to the output, which is not available during decoding.\nSet to zero all attention weights that correspond to a token attending to any later token in the sequence.\n\\[\\mathbf{Y} = \\text{SoftMax}[\\mathbf{Q}\\mathbf{K}^{T}] \\mathbf{V}\\] In practice we set the pre-activation values to \\(-\\infty\\) so that softmax evaluates to zero for the associated outputs. and also takes care of normalization across non-zero outputs.\nSampling strategies # The output of the trained model is a probability distribution over the space of tokens, given by the softmax activation function, which represents the probability of the next token given the current token sequence.\nGreedy search # Select the token with the highest probability.\nDeterministic.\n\\(\\mathcal{O}(KN)\\) Beam search # Instead of choosing the single most probable token value at each step, we maintain a set of \\(B\\) (beam width) hypothesis, each consisting of a sequence of token values up to step \\(n\\) . We then feed all these sequences through the network and for each sequence we find the \\(B\\) most probable token values, thereby creating \\(B^2\\) possible hypothesis. This list is then pruned by selecting the most probable \\(B\\) hypotheses according to the total probability of the extended sequence. \\(\\mathcal{O}(BKN)\\) Sampling from softmax # Sampling from the softmax distribution at each step.\nCan lead to nonsensical sequences sometime due to large vocabulary\nTop-k sampling # Consider only the states having a top- \\(k\\) probabilities and sample from these according to their renormalized probabilities.\nTop-p sampling/Nucleus sampling # Calculates the cumulative probability of the top outputs until a threshold is reached and then samples from this restricted set of tokens.\nSoft top-k sampling # Introduce a parameter \\(T\\) called temperature into the definition of softmax function.\n\\[y_i=\\frac{\\exp(a_i/T)}{\\sum_j \\exp(a_j/T)}\\] \\(T=0\\) Greedy sampling\n\\(T=1\\) Softmax sampling\nEncoder transformers # seq2vec - Generative models that output sequence of tokens. BERT (Bidirectional Encoder Representations from Transformers)\nTake sequence of tokens as input and produce fixed-length vectors to be used for various downstream tasks.\nPre-training Fine-tuning The first token of every input sequence is given by a special token \u0026lt;class\u0026gt;.\nPre-training # A randomly chosen subset of tokens (say 15%) is replaced with a special token denoted by \u0026lt;mask\u0026gt;.\nThe transformer is trained to predict the missing tokens at the corresponding output nodes.\nOf the masked tokens\n80% are replaced with \u0026lt;mask\u0026gt; 10% are replaced with a random token. 10% the original token is retained. Fine-tuning # Once the encoder is trained it can then be fine-tuned for a task.\nTypically a new layer is built on top of the embedding for the \u0026lt;class\u0026gt; token.\nEncoder-Decoder transformers # seq2seq T5\nAn encoder is used to map the input token sequence to a suitable internal representation.\nCross-attention - Same as self-attention but the key and the value vectors come from the encoder representation.\n"},{"id":15,"href":"/docs/transformers/alignment/","title":"Alignment","section":"Transformers","content":" Alignment # LLMs are typically trained for next-token prediction.\nPre-trained LLMs may not be able to follow user instructions because they were not trained to do so.\nPre-trained LLMs may generate harmful content or perpetuate biases inherent in their training data.\n--- title: LLM training stages --- flowchart LR subgraph Pre-training A[Pre-training] end subgraph Post-training B[\"Instruction Alignment (SFT)\"] C[\"Preference Alignment (RLHF)\"] end subgraph Inference D[Prompt engineering] end A--\u003eB B--\u003eC C--\u003eD Fine tune LLMs with labelled data # Supervised Fine Tuning (SFT) # Training data is task specific instructions paired with their expected outputs.\nDuring backward pass we the force the loss corresponding to the instruction to be zero.\nParameter Efficient Fine Tuning (PEFT) # LoRA # QLoRA # Soft prompts # Fine tune LLMs with reward models # Alignment during inference # Prompting\n"},{"id":16,"href":"/docs/neural_networks/mlp/","title":"Multilayer perceptron","section":"Neural Networks","content":" Multilayer perceptron # A 3-layer multilayer perceptron. \\[ \\begin{align} \\mathbf{X} \u0026= \\mathbf{X} \\nonumber \\\\ \\mathbf{H}^{(1)} \u0026= g_1\\left(\\mathbf{X}\\mathbf{W}^{(1)}+\\mathbf{b}^{(1)}\\right) \\nonumber \\\\ \\mathbf{H}^{(2)} \u0026= g_2\\left(\\mathbf{H}^{(1)}\\mathbf{W}^{(2)}+\\mathbf{b}^{(2)}\\right) \\nonumber \\\\ \\mathbf{O} \u0026= \\mathbf{H}^{(2)}\\mathbf{W}^{(3)}+\\mathbf{b}^{(3)} \\nonumber \\\\ \\end{align} \\] \\(g\\) is a nonlinear activation function\nstateDiagram-v2 direction LR x1: $$x_1$$ x2: $$x_2$$ x3: $$x_3$$ h11: $$h^1_1$$ h12: $$h^1_2$$ h13: $$h^1_3$$ h14: $$h^1_4$$ h21: $$h^2_1$$ h22: $$h^2_2$$ h23: $$h^2_3$$ h24: $$h^2_4$$ h25: $$h^2_5$$ o1: $$o_1$$ o2: $$o_2$$ o3: $$o_3$$ x1 --\u003e h11 x1 --\u003e h12 x1 --\u003e h13 x1 --\u003e h14 x2 --\u003e h11 x2 --\u003e h12 x2 --\u003e h13 x2 --\u003e h14 x3 --\u003e h11 x3 --\u003e h12 x3 --\u003e h13 x3 --\u003e h14 h11 --\u003e h21 h11 --\u003e h22 h11 --\u003e h23 h11 --\u003e h24 h11 --\u003e h25 h12 --\u003e h21 h12 --\u003e h22 h12 --\u003e h23 h12 --\u003e h24 h12 --\u003e h25 h13 --\u003e h21 h13 --\u003e h22 h13 --\u003e h23 h13 --\u003e h24 h13 --\u003e h25 h14 --\u003e h21 h14 --\u003e h22 h14 --\u003e h23 h14 --\u003e h24 h14 --\u003e h25 h21 --\u003e o1 h22 --\u003e o1 h23 --\u003e o1 h24 --\u003e o1 h25 --\u003e o1 h21 --\u003e o2 h22 --\u003e o2 h23 --\u003e o2 h24 --\u003e o2 h25 --\u003e o2 h21 --\u003e o3 h22 --\u003e o3 h23 --\u003e o3 h24 --\u003e o3 h25 --\u003e o3 o1 --\u003e END1:::hidden o2 --\u003e END2:::hidden o3 --\u003e END3:::hidden note left of x3 : Input layer note left of h14 : Hidden layer 1 note left of h25 : Hidden layer 2 note left of o1 : Output layer classDef hidden display: none; "},{"id":17,"href":"/docs/training/normalization/","title":"Normalization","section":"Training","content":" Batch normalization # In batch normalization the mean and variance are computed across the mini-batch separately for each feature/hidden unit. For a mini-batch of size B \\[ \\mu_i = \\frac{1}{B} \\sum_{n=1}^{B} a_{ni} \\] \\[ \\sigma_i^2 = \\frac{1}{B} \\sum_{n=1}^{B} (a_{ni}-\\mu_i)^2 \\] We normalize the pre-activations as follows. \\[ \\hat{a}_{ni} = \\frac{a_{ni}-\\mu_i}{\\sqrt{\\sigma_i^2+\\delta}} \\] \\[ \\tilde{a}_{ni} = \\gamma_i \\hat{a}_{ni} + \\beta_i \\] PyTorch m = nn.BatchNorm1d(num_features) Layer normalization # In layer normalization the mean and variance are computed across the feature/hidden unit for each example seprately. \\[ \\mu_n = \\frac{1}{M} \\sum_{i=1}^{M} a_{ni} \\] \\[ \\sigma_n^2 = \\frac{1}{M} \\sum_{i=1}^{M} (a_{ni}-\\mu_i)^2 \\] We normalize the pre-activations as follows. \\[ \\hat{a}_{ni} = \\frac{a_{ni}-\\mu_n}{\\sqrt{\\sigma_n^2+\\delta}} \\] \\[ \\tilde{a}_{ni} = \\gamma_n \\hat{a}_{ni} + \\beta_n \\] Layer Normalization, J. L. Bao, J. R. Kiros, and G. E. Hinton, 2016\nPyTorch layer_norm = nn.LayerNorm(enormalized_shape) Collateral # https://pytorch.org/docs/stable/nn.html#normalization-layers\n"},{"id":18,"href":"/docs/neural_networks/activations/","title":"Activation functions","section":"Neural Networks","content":" Sigmoid # Sigmoid/Logistic \\[ \\sigma(z) = \\frac{1}{1+\\exp(-z)} \\] The derivative is given by \\[ \\sigma'(z) = \\sigma(z)(1-\\sigma(z)) \\] ReLU # Rectified Linear Unit (ReLU) \\[ \\text{ReLU}(z) = \\max(z,0) \\] Nair and Hinton, 2010\npReLU # parameterized Rectified Linear Unit (pReLU) \\[ \\text{pReLU}(z) = \\max(z,0) + \\alpha \\min(z,0) \\] He et al., 2015\nTanh # Hyperbolic tangent. \\[ \\text{tanh}(z) = \\frac{1-\\exp(-2z)}{1+\\exp(-2z)} \\] The derivative is given by \\[ \\text{tanh}'(z)= 1- \\text{tanh}^2(z) \\] GeLU # Gaussian error Linear Unit/Smooth ReLU \\[ \\text{GeLU}(z) = z \\Phi(z) \\] where \\(\\Phi(z)\\) is the standard Gaussian cumulative distribution.\nHendrycks and Gimpel, 2016\nSwish # \\[ \\text{Swish}_{\\beta}(z) = z \\sigma(\\beta z) \\] Ramachandran et al. 2017\nGLU # Gated Liner Unit \\[ \\text{GLU}(z) = z \\odot \\sigma(wz+b) \\] SwiGLU # Swish Gated Liner Unit \\[ \\text{Swish}(z) = z \\odot \\text{Swish}_{\\beta}(wz+b) \\] GLU Variants Improve Transformer, Noam Shazeer, 2020.\n🤷 We offer no explanation as to why these architectures seem to work; we attribute their success, as all else, to divine benevolence.\nCollateral # https://pytorch.org/docs/stable/nn.html#non-linear-activations-weighted-sum-nonlinearity\n"},{"id":19,"href":"/docs/transformers/models/","title":"Frontier models","section":"Transformers","content":"https://crfm.stanford.edu/ecosystem-graphs/index.html?mode=table\nClosed models # API access only\nOpen-weight models # weights available\nOpen-source models # weights, data, training details available\nBERT # year architecture type open # parameters who checkpoint tokenizer 2018 Oct BERT encoder ✅ 340M Google WordPiece ALBERT encoder ✅ ELECTRA encoder ✅ RoBERTa encoder ✅ 2019 Oct DistilBERT encoder ✅ 66 M HuggingFace 2019 Oct BART encoder-decoder 400M Meta 2019 Oct T5 encoder-decoder 11B Google SentencePiece GPT # Open AI, closed, decoder\nhttps://platform.openai.com/docs/models\nyear architecture parameters tokenizer notes 2018 Jun GPT 110 M 2019 Feb GPT-2 1.5 B BPE fluent text, first signs of zero-shot, staged release 2020 May GPT-3 175 B in-context learning 2024 GPT-4 1.8 T 2025 Jan OpenAI o3 OpenAI o4-mini Gemini # Gemini 2.5 - https://blog.google/technology/google-deepmind/gemini-model-thinking-updates-march-2025/\nPaLM # Google\nyear architecture # parameters tokenizer notes 2018 Jun PaLM 540 B massive scale, undertrained Llama # Meta\nhttps://arxiv.org/pdf/2302.13971\nLLama2 https://arxiv.org/pdf/2307.09288\nLlama3 https://arxiv.org/abs/2407.21783\nLlama3.3 https://ai.meta.com/blog/meta-llama-3/\nQwen # ALibaba\nhttps://arxiv.org/abs/2412.15115\nQwen2.5-Max https://qwenlm.github.io/blog/qwen2.5-max/\nDeepseek # https://arxiv.org/pdf/2401.02954\nDeepSeek-v2 https://arxiv.org/abs/2405.04434\nDeepSeek-v3 https://arxiv.org/pdf/2412.19437\nDeepSeek-R1 https://arxiv.org/pdf/2501.12948\nOLMo # AI2\nhttps://arxiv.org/pdf/2402.00838\nOLMo2 https://arxiv.org/abs/2501.00656\nMistral # Claude # Anthropic\nClaude 3.7 Sonnet https://www.anthropic.com/news/claude-3-7-sonnet\nPhi # Grok # xAI\nGrok3 - https://x.ai/news/grok-3\n"},{"id":20,"href":"/docs/transformers/moe/","title":"MoE","section":"Transformers","content":"https://arxiv.org/pdf/1701.06538\n"},{"id":21,"href":"/docs/training/regularization/","title":"Regularization","section":"Training","content":" Dropout # Dropout is one of the most effective form of regularization that is widely used.\nThe core idea is to delete nodes from the network, including their connections, at random during training.\nDropout is applied to both hidden and input nodes, but not outputs. It is equivalent to setting the output of a dropped node to zero.\ntorch.nn.Dropout Early stopping # For good generalization training should be stopped at the point of smallest error with respect to the validation set.\n"},{"id":22,"href":"/docs/transformers/scaling/","title":"Scaling","section":"Transformers","content":"https://arxiv.org/pdf/2001.08361\nhttps://arxiv.org/pdf/2203.15556\n"},{"id":23,"href":"/docs/neural_networks/entropy/","title":"Entropy","section":"Neural Networks","content":" A brief primer on entropy, cross-entropy and perplexity.\nEntropy # \\(\\) The entropy of a discrete random variable \\(X\\) with \\(K\\) states/categories with distribution \\(p_k = \\text{Pr}(X=k)\\) for \\(k=1,...,K\\) is a measure of uncertainty and is defined as follows. \\[H(X) = \\sum_{k=1}^{K} p_k \\log_2 \\frac{1}{p_k} = - \\sum_{k=1}^{K} p_k \\log_2 p_k \\] \\(\\) The term \\(\\log_2\\frac{1}{p}\\) quantifies the notion or surprise or uncertainty and hence entropy is the average uncertainty.\nThe unit is bits ( \\(\\in [0,\\log_2 K]\\) ) (or nats incase of natural log).\nThe discrete distribution with maximum entropy ( \\(\\log_2 K\\) ) is uniform.\nThe discrete distribution with minimum entropy ( \\(0\\) ) is any delta function which puts all mass on one state/category.\nPrediction and Entropy of Printed English, C. E. Shannon, 1950.\nBinary entropy # \\(\\) For a binary random variable \\(X \\in {0,1}\\) with \\(\\text{Pr}(X=1) = \\theta\\) and \\(\\text{Pr}(X=0) = 1-\\theta\\) the entropy is as follows.\n\\[H(\\theta) = - [ \\theta \\log_2 \\theta + (1-\\theta) \\log_2 (1-\\theta) ] \\] The range is \\(H(\\theta) \\in [0,1]\\) and is maximum when \\(\\theta=0.5\\) .\nCross entropy # \\(\\) Cross entropy is the average number of bits needed to encode the data from from a source \\(p\\) when we model it using \\(q\\) .\n\\[H(p,q) = - \\sum_{k=1}^{K} p_k \\log_2 q_k \\] Perplexity # \\[\\text{PPL}(p,q) = 2^{H(p,q)}\\] \\[\\text{PPL}(p,q) = e^{H(p,q)}\\] KL Divergence # The Kullback-Leibler (KL) divergence or relative entropy measures the dissimilarity between two probability distributions \\(p\\) and \\(q\\) .\n\\[KL(p,q) = \\sum_{k=1}^{K} p_k \\log_2 \\frac{p_k}{q_k}\\] \\[KL(p,q) = H(p,q) - H(p,p) \\geq 0\\] Mutual Information # \\(I(X,Y) = KL(P(X,Y)\\|P(X)P(Y))\\) "},{"id":24,"href":"/docs/training/training_loop/","title":"Training loop","section":"Training","content":" Training loop # # Load the dataset. train_dataset = SampleDataset(X_train, y_train) test_dataset = SampleDataset(X_test, y_test) # Preparing your data for training with DataLoaders. batch_size = 64 train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True) test_dataloader = DataLoader(test_dataset, batch_size=batch_size, shuffle=True) # Define the model class. model = LogisticRegression(num_features=d) # Loss fucntion. loss_fn = nn.BCELoss() # Optimizer. optimizer = SGD(model.parameters(), lr=0.01, momentum=0.9) # Learning rate scheduler. scheduler = ExponentialLR(optimizer, gamma=0.9) # Run for a few epochs. for epoch in range(n_epochs): # Iterate through the DataLoader to access mini-batches. for batch, (input, target) in enumerate(train_dataloader): # Prediction. output = model(input) # Compute loss. loss = loss_fn(output, target) # Compute gradient. loss.backward() # Gradient descent. optimizer.step() # Prevent gradient accumulation optimizer.zero_grad() # Adjust learning rate scheduler.step() "},{"id":25,"href":"/docs/transformers/datasets/","title":"Datasets","section":"Transformers","content":"Pile - https://arxiv.org/pdf/2101.00027\n"},{"id":26,"href":"/docs/neural_networks/metrics/","title":"Metrics","section":"Neural Networks","content":" Some core evaluation metrics. To be revised.\nBinary Classification # True Positive Rate # Recall, Sensitivity.\n\\[\\text{TPR} = \\text{Pr}[y_{pred}=1|y=1] = \\frac{TP}{TP+FN} = \\frac{TP}{N_{+}}\\] False Positive Rate # 1-Specificity\n\\[\\text{FPR} = \\text{Pr}[y_{pred}=1|y=0] = \\frac{FP}{FP+TN} = \\frac{FP}{N_{-}}\\] ROC Curve # x-axis is FPR and y-axis is TPR\nAUC # Area under the ROC Curve.\n\\[\\text{AUC}=\\text{Pr}[y^{+} \u003e y^{-}]\\] Accuracy # \\[\\text{Accuracy}=\\frac{TP+TN}{TP+FP+TN+FN}\\] Precision # Precision is the fraction of predicted positives that are actually positive.\n\\[\\text{Precision} = \\text{Pr}[y=1|y_{pred}=1] = \\frac{TP}{TP+FP}\\] Recall # Precision is the fraction of positives that are correctly predicted as positive.\n\\[\\text{Recall} = \\text{Pr}[y_{pred}=1|y=1] = \\frac{TP}{TP+FN}\\] F1 Score # F1 Score is the harmonic mean of precision and recall.\n\\[F1=2 \\times \\frac{\\text{Precision}\\times\\text{Recall}}{\\text{Precision}+\\text{Recall}}\\] PR Curve # Retrieval # Precision@k # Average Precision # Mean Average Precision # MRR # Mean Reciprocal Rank (MRR) is the average of the reciprocal ranks of the ground truth contexts in the retrieved contexts.\n\\[MRR = \\frac{1}{Q} \\sum_{i=1}^{Q} \\frac{1}{\\text{rank}_i}\\] nDCG # Normalized Cumulative Discounted Gain is the ratio of the Discounted Cumulative Gain and the ideal DCG.\nDCG is the sum of the discounted relevance scores of the ground truth contexts.\n\\[\\text{DCG}=\\sum_{i}^{N} \\frac{\\text{rel}_i}{\\log_2(i+1)}\\] For a query given the set of relevant offers and the retrieved offers we would like to compute the following offline metrics. The metrics are then average over the set of Q queries.\nRecall # For a given query q, recall(q) is the fraction of the offers that are relevant to the query that are successfully retrieved.\n\\[\\text{recall(q)} = \\frac{|\\{\\text{relevant offers for query q}\\} \\cap \\{\\text{retrieved offers for query q}\\}|}{|\\{\\text{relevant offers for query q}\\}|}\\] It can also be evaluated considering only the top-k results returned by the system using recall@k.\nPrecision # For a given query q, precision(q) is the fraction of the offers retrieved that are relevant to the user\u0026rsquo;s query.\n\\[\\text{precision(q)} = \\frac{|\\{\\text{relevant offers for query q}\\} \\cap \\{\\text{retrieved offers for query q}\\}|}{|\\{\\text{retrieved offers for query q}\\}|}\\] It can also be evaluated considering only the top-k results returned by the system using precision@k.\nPR Curve # By computing the precision and recall at every position in the ranked sequence of documents (by varying \\(k=1,...,n\\) where \\(n\\) is the total number of retrieved offers), one can plot the Precision-Recall (PR) Curve by plotting precision \\(p(r)\\) on the y-axis as a function of recall \\(r\\) on the x-axis.\nAveP - Average Precision # Average precision computes the average value of \\(p(r)\\) over the interval from \\(r=0\\) to \\(r=1\\) . This essentially the Area under the PR Curve (PR AUC) computes as \\[\\text{AveP}=\\sum_{k=1}^{n}p(k)\\Delta r(k)\\] where \\(k\\) is the rank in the sequence of retrieved offers, \\(n\\) is the number of retrieved offers, \\(p(k)\\) is the precision at cut-off \\(k\\) in the list, and \\(\\Delta r(k)\\) is the change in recall from items \\(k-1\\) to \\(k\\) .\nMAP - Mean Average Precision # Mean average precision (MAP) for a set of queries is the mean of the average precision scores for each query.\n\\[MAP=\\frac{\\sum_{q=1}^{Q} \\text{AveP}(q)}{Q}\\] "},{"id":27,"href":"/docs/training/quiz/","title":"Quiz","section":"Training","content":" Quiz # \\(\\) Derive the gradient of the loss function for linear regression and logistic regression.\nPlot binary entropy.\nWHy don\u0026rsquo;t you use MSE loss for binary classification ?\nWhat is the most widely used optimizer ? What are the typically used parameters of the optimizer ?\nFor SGD with momemtum show that it increases the effective learning rate from \\(\\eta\\) to \\(\\frac{\\eta}{(1-\\mu)}\\) .\nIn Attention Is All You Need paper what is the optimizer and the learning rate scheduler used ?\nWhat is the disadvantage of forward-mode automatic differentiation ?\nWhat is the difference between batch and layer normalization ?\n"},{"id":28,"href":"/docs/training/coding/","title":"Coding","section":"Training","content":" Coding assignment # \\(\\) Setup # https://github.com/vikasraykar/deeplearning-dojo/\ngit clone https://github.com/vikasraykar/deeplearning-dojo.git cd deeplearning-dojo python -m venv .env source .env/bin/activate pip install -r requirements.txt Problem 1 # Linear Regression with numpy and batch gradient descent.\nIn the first coding assigment you will be implementing a basic Linear Regression model from scratch using only numpy. You will be implementing a basic batch gradient descent optimizer.\nYou can use only numpy and are not allowed to to use torch or any other python libraries.\nReview the linear regression model and its loss function. Given a feature matrix \\(\\mathbf{X}\\) as a \\(N \\times d\\) numpy.ndarray write the prediction and the loss function using matrix notation and carefully check for dimensions. Accont for the bias by appending the feature matrix with a column of ones. Derive the expression for the gradient of the loss function. Implement a basic batch gradient descent optimizer with a fixed learning rate first. Track the loss every few epochs and check the actual and estimated parameters. Check the MSE loss on the train and the test set. Implement a simple learning rate decay as follows lr=lr/(1+decay_factor*epoch). A sample stub is provided in the repo as below. Your task is to implement the predict and the fit function.\nLinearRegressionNumpy.py \u0026#34;\u0026#34;\u0026#34;Basic implementation of Linear Regression using only numpy. \u0026#34;\u0026#34;\u0026#34; import numpy as np class LinearRegression: \u0026#34;\u0026#34;\u0026#34;Linear Regression.\u0026#34;\u0026#34;\u0026#34; def __init__(self): pass def predict(self, X: np.ndarray) -\u0026gt; np.ndarray: \u0026#34;\u0026#34;\u0026#34;Predction. Args: X (np.ndarray): Features matrix. (N,d) add_colum_vector_for_bias (bool, optional): Add a column vector of ones to model the bias term. Defaults to True. Returns: y (np.ndarray): Prediction vector. (N,) \u0026#34;\u0026#34;\u0026#34; pass def fit( self, X_train: np.ndarray, y_train: np.ndarray, learning_rate: float = 1e-3, learning_rate_decay: bool = True, learning_rate_decay_factor: float = 1.0, num_epochs: int = 100, track_loss_num_epochs: int = 100, ): \u0026#34;\u0026#34;\u0026#34;Training. Args: X_train (np.ndarray): Features matrix. (N,d) y_train (np.ndarray): Target vector. (N,) learning_rate (float, optional): Learning rate. Defaults to 0.001. learning_rate_decay (bool, optional): If True does learning rate deacy. Defaults to True. learning_rate_decay_factor (float, optional): The deacay factor (lr=lr/(1+decay_factor*epoch)). Defaults to 1.0. num_epochs (int, optional): Number of epochs. Defaults to 100. track_loss_num_epochs (int, optional): Compute loss on training set once in k epochs. Defaults to 100. \u0026#34;\u0026#34;\u0026#34; pass Problem 2 # Logistic Regression with numpy and batch gradient descent.\nIn the second coding assigment you will be implementing a basic Logisitc Regression model from scratch using only numpy. You will be implementing a basic batch gradient descent optimizer.\nYou can use only numpy and are not allowed to to use torch or any other python libraries.\nReview the logistic regression model and its loss function. Given a feature matrix \\(\\mathbf{X}\\) as a \\(N \\times d\\) numpy.ndarray write the prediction and the loss function using matrix notation and carefully check for dimensions. Accont for the bias by appending the feature matrix with a column of ones. Derive the expression for the gradient of the loss function. Modularize it so that it should look exactly like the derivative you got for linear regression. Implement a basic batch gradient descent optimizer with a fixed learning rate first. Track the loss every few epochs and check the actual and estimated parameters. Check the accuracy on the train and the test set. Implement a simple learning rate decay as follows lr=lr/(1+decay_factor*epoch). A sample stub is provided in the repo as below. Your task is to implement the predict_proba, predict and the fit function.\nLogisticRegressionNumpy.py \u0026#34;\u0026#34;\u0026#34;Basic implementation of Logistic Regression using numpy only.\u0026#34;\u0026#34;\u0026#34; import numpy as np class LogisticRegression: \u0026#34;\u0026#34;\u0026#34;Logistic Regression\u0026#34;\u0026#34;\u0026#34; def __init__(self): pass def fit( self, X_train: np.ndarray, y_train: np.ndarray, learning_rate: float = 1e-3, learning_rate_decay: bool = True, learning_rate_decay_factor: float = 1.0, num_epochs: int = 100, track_loss_num_epochs: int = 10, ): \u0026#34;\u0026#34;\u0026#34;Train. Args: X_train (np.ndarray): Feature matrix. (N,d) y_train (np.ndarray): Target labels (0,1). (N,) learning_rate (float, optional): The initial learning rate. Defaults to 1e-3. learning_rate_decay (bool, optional): If True enables learning rate decay. Defaults to True. learning_rate_decay_factor (float, optional): The learning rate decay factor (1/(1+decay_factor*epoch)). Defaults to 1.0. num_epochs (int, optional): The number of epochs to train. Defaults to 100. track_loss_num_epochs (int, optional): Compute loss on training set once in k epochs. Defaults to 10. \u0026#34;\u0026#34;\u0026#34; pass def predict_proba(self, X: np.ndarray): \u0026#34;\u0026#34;\u0026#34;Predict the probability of the positive class (Pr(y=1)). Args: X (np.ndarray): Feature matrix. (N,d) Returns: y_pred_proba (np.ndarray): Predicted probabilities. (N,) \u0026#34;\u0026#34;\u0026#34; pass def predict( self, X: np.ndarray, threshold: float = 0.5, ): \u0026#34;\u0026#34;\u0026#34;Predict the label(0,1). Args: X (np.ndarray): Feature matrix. (N,d) add_colum_vector_for_bias (bool, optional): Add a column vector of ones to model the bias term. Defaults to True. threshold (float, optional): The threshold on the probabilit. Defaults to 0.5. Returns: y (np.ndarray): Prediction vector. (N,) \u0026#34;\u0026#34;\u0026#34; pass Problem 3 # Logistic Regression with torch and min-batch SGD.\nReview Datasets \u0026amp; DataLoaders in pytorch. Experiment withe different optimizers covered in this lectures and plot the learning curve for different optimizers (SGD, SGD with momentum, AdaGrad, RMSProp, Adam), AdamW. Tune the learning rate for a optimizer. A sample stub is provided in the repo as below.\nLogisticRegressionPytorch.py Problem 4 # Logistic Regression with torch and min-batch SGD on a publicly avaiable dataset.\nChosee one publicly avaiable large dataset and implement custom datsets and loaders and learn either a linear regression model.\nReal Estate Data UAE\nBonus problem # AdamW implementation\nImplement the AdamW optimizer as a subclass of torch.optim.Optimizer.\nAn Optimizer subclass much implemnt two methods.\nimport torch class AdamW(torch.optim.Optimizer): def __init__(self,params, ...): pass def step(self): pass The PyTorch optimizer API has a few subtleties and study how some optimizers are written.\n"}]