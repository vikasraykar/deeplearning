<!DOCTYPE html>
<html lang="en-us" dir="ltr">
<head><script src="/livereload.js?mindelay=10&amp;v=2&amp;port=1313&amp;path=livereload" data-no-instant defer></script>
  <meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1.0">
<meta name="description" content="
  Softmax Regression
  #

Softmax Regression is a single layer neural network for multi-class classification.




  





stateDiagram-v2
    direction LR
    x1: $$x_1$$
    x2: $$x_2$$
    x3: $$x_3$$
    xd: $$x_d$$
    a1: $$a_1=\sum_i w_{1i} x_i$$
    a2: $$a_2=\sum_i w_{2i} x_i$$
    ak: $$a_k=\sum_i w_{ki} x_i$$
    z1: $$z_1=\text{softmax}(\mathbf{a})_1$$
    z2: $$z_2=\text{softmax}(\mathbf{a})_2$$
    zk: $$z_k=\text{softmax}(\mathbf{a})_k$$
    x1 --&gt; a1:$$w_{11}$$
    x2 --&gt; a1:$$w_{12}$$
    x3 --&gt; a1:$$w_{13}$$
    xd --&gt; a1:$$w_{1d}$$
    x1 --&gt; a2:$$w_{21}$$
    x2 --&gt; a2:$$w_{22}$$
    x3 --&gt; a2:$$w_{23}$$
    xd --&gt; a2:$$w_{2d}$$
    x1 --&gt; ak:$$w_{k1}$$
    x2 --&gt; ak:$$w_{k2}$$
    x3 --&gt; ak:$$w_{k3}$$
    xd --&gt; ak:$$w_{kd}$$
    a1 --&gt; z1
    a2 --&gt; z2
    ak --&gt; zk
    z1 --&gt; END1:::hidden
    z2 --&gt; END2:::hidden
    zk --&gt; END:::hidden
    note left of xd : Inputs
    note right of a1 : Pre-activations
    note left of zk : Activations
    note left of END : Outputs
    classDef hidden display: none;



  Statistical model
  #

Given 



  \(k\)

 classes the probability of class 
  \(i\)

 for a given feature vector (
  \(\mathbf{x}\in \mathbb{R}^d\)

) is given by

  \[
\text{Pr}[y=i|\mathbf{x},(\mathbf{w}_1,\mathbf{w}_2,...,\mathbf{w}_k)] = \frac{\exp(\mathbf{w}_i^T\mathbf{x})}{\sum_{j=1}^{k} \exp(\mathbf{w}_j^T\mathbf{x})}
\]


where 
  \(\mathbf{w}_1,\mathbf{w}_2,...,\mathbf{w}_k\in \mathbb{R}^d\)

 are the weight vector or parameters of the model for each class.">
<meta name="theme-color" media="(prefers-color-scheme: light)" content="#ffffff">
<meta name="theme-color" media="(prefers-color-scheme: dark)" content="#343a40">
<meta name="color-scheme" content="light dark"><meta property="og:url" content="http://localhost:1313/docs/neural_networks/softmax_regression/">
  <meta property="og:site_name" content="Deep Learning">
  <meta property="og:title" content="Softmax Regression">
  <meta property="og:description" content="Softmax Regression # Softmax Regression is a single layer neural network for multi-class classification.
stateDiagram-v2 direction LR x1: $$x_1$$ x2: $$x_2$$ x3: $$x_3$$ xd: $$x_d$$ a1: $$a_1=\sum_i w_{1i} x_i$$ a2: $$a_2=\sum_i w_{2i} x_i$$ ak: $$a_k=\sum_i w_{ki} x_i$$ z1: $$z_1=\text{softmax}(\mathbf{a})_1$$ z2: $$z_2=\text{softmax}(\mathbf{a})_2$$ zk: $$z_k=\text{softmax}(\mathbf{a})_k$$ x1 --&gt; a1:$$w_{11}$$ x2 --&gt; a1:$$w_{12}$$ x3 --&gt; a1:$$w_{13}$$ xd --&gt; a1:$$w_{1d}$$ x1 --&gt; a2:$$w_{21}$$ x2 --&gt; a2:$$w_{22}$$ x3 --&gt; a2:$$w_{23}$$ xd --&gt; a2:$$w_{2d}$$ x1 --&gt; ak:$$w_{k1}$$ x2 --&gt; ak:$$w_{k2}$$ x3 --&gt; ak:$$w_{k3}$$ xd --&gt; ak:$$w_{kd}$$ a1 --&gt; z1 a2 --&gt; z2 ak --&gt; zk z1 --&gt; END1:::hidden z2 --&gt; END2:::hidden zk --&gt; END:::hidden note left of xd : Inputs note right of a1 : Pre-activations note left of zk : Activations note left of END : Outputs classDef hidden display: none; Statistical model # Given \(k\) classes the probability of class \(i\) for a given feature vector ( \(\mathbf{x}\in \mathbb{R}^d\) ) is given by \[ \text{Pr}[y=i|\mathbf{x},(\mathbf{w}_1,\mathbf{w}_2,...,\mathbf{w}_k)] = \frac{\exp(\mathbf{w}_i^T\mathbf{x})}{\sum_{j=1}^{k} \exp(\mathbf{w}_j^T\mathbf{x})} \] where \(\mathbf{w}_1,\mathbf{w}_2,...,\mathbf{w}_k\in \mathbb{R}^d\) are the weight vector or parameters of the model for each class.">
  <meta property="og:locale" content="en_us">
  <meta property="og:type" content="article">
    <meta property="article:section" content="docs">
<title>Softmax Regression | Deep Learning</title>
<link rel="icon" href="/favicon.png" >
<link rel="manifest" href="/manifest.json">
<link rel="canonical" href="http://localhost:1313/docs/neural_networks/softmax_regression/">
<link rel="stylesheet" href="/book.min.6c8b9d2a1fc95075ed7da46ca81060b39add8fff6741ac51259f768929281e2c.css" integrity="sha256-bIudKh/JUHXtfaRsqBBgs5rdj/9nQaxRJZ92iSkoHiw=" crossorigin="anonymous">
  <script defer src="/fuse.min.js"></script>
  <script defer src="/en.search.min.cde1b0268c897b03006007114aaa023eb2c065656657b3f18b7964f1bf55f8c1.js" integrity="sha256-zeGwJoyJewMAYAcRSqoCPrLAZWVmV7Pxi3lk8b9V&#43;ME=" crossorigin="anonymous"></script>
<!--
Made with Book Theme
https://github.com/alex-shpak/hugo-book
-->
  
</head>
<body dir="ltr">
  <input type="checkbox" class="hidden toggle" id="menu-control" />
  <input type="checkbox" class="hidden toggle" id="toc-control" />
  <main class="container flex">
    <aside class="book-menu">
      <div class="book-menu-content">
        
  <nav>
<h2 class="book-brand">
  <a class="flex align-center" href="/"><span>Deep Learning</span>
  </a>
</h2>


<div class="book-search hidden">
  <input type="text" id="book-search-input" placeholder="Search" aria-label="Search" maxlength="64" data-hotkeys="s/" />
  <div class="book-search-spinner hidden"></div>
  <ul id="book-search-results"></ul>
</div>
<script>document.querySelector(".book-search").classList.remove("hidden")</script>















  
  <ul>
    
      
        <li class="book-section-flat" >
          
  
  

  
    <a href="/docs/neural_networks/" class="">Neural Networks</a>
  

          
  <ul>
    
      
        <li>
          
  
  

  
    <a href="/docs/neural_networks/linear_regression/" class="">Linear Regression</a>
  

        </li>
      
    
      
        <li>
          
  
  

  
    <a href="/docs/neural_networks/logistic_regression/" class="">Logistic Regression</a>
  

        </li>
      
    
      
        <li>
          
  
  

  
    <a href="/docs/neural_networks/softmax_regression/" class="active">Softmax Regression</a>
  

        </li>
      
    
      
        <li>
          
  
  

  
    <a href="/docs/neural_networks/mlp/" class="">Multilayer perceptron</a>
  

        </li>
      
    
      
        <li>
          
  
  

  
    <a href="/docs/neural_networks/activations/" class="">Activation functions</a>
  

        </li>
      
    
      
        <li>
          
  
  

  
    <a href="/docs/neural_networks/entropy/" class="">Entropy</a>
  

        </li>
      
    
  </ul>

        </li>
      
    
      
        <li class="book-section-flat" >
          
  
  

  
    <a href="/docs/training/" class="">Training neural networks</a>
  

          
  <ul>
    
      
        <li>
          
  
  

  
    <a href="/docs/training/gradient_descent/" class="">Gradient Descent</a>
  

        </li>
      
    
      
        <li>
          
  
  

  
    <a href="/docs/training/backpropagation/" class="">Backpropagation</a>
  

        </li>
      
    
      
        <li>
          
  
  

  
    <a href="/docs/training/autograd/" class="">Automatic Differentiation</a>
  

        </li>
      
    
      
        <li>
          
  
  

  
    <a href="/docs/training/initialization/" class="">Initialization</a>
  

        </li>
      
    
      
        <li>
          
  
  

  
    <a href="/docs/training/normalization/" class="">Normalization</a>
  

        </li>
      
    
      
        <li>
          
  
  

  
    <a href="/docs/training/regularization/" class="">Regularization</a>
  

        </li>
      
    
      
        <li>
          
  
  

  
    <a href="/docs/training/training_loop/" class="">Training loop</a>
  

        </li>
      
    
      
        <li>
          
  
  

  
    <a href="/docs/training/quiz/" class="">Quiz</a>
  

        </li>
      
    
      
        <li>
          
  
  

  
    <a href="/docs/training/coding/" class="">Coding</a>
  

        </li>
      
    
  </ul>

        </li>
      
    
      
        <li class="book-section-flat" >
          
  
  

  
    <a href="/docs/transformers/" class="">Transformers</a>
  

          
  <ul>
    
      
        <li>
          
  
  

  
    <a href="/docs/transformers/transformers101/" class="">Transformers</a>
  

        </li>
      
    
      
        <li>
          
  
  

  
    <a href="/docs/transformers/alignment/" class="">Alignment</a>
  

        </li>
      
    
  </ul>

        </li>
      
    
      
        <li class="book-section-flat" >
          
  
  

  
    <a href="/docs/rl/" class="">Reinforcement Learning</a>
  

          
  <ul>
    
      
        <li>
          
  
  

  
    <a href="/docs/rl/basics/" class="">Basics</a>
  

        </li>
      
    
  </ul>

        </li>
      
    
  </ul>














</nav>




  <script>(function(){var e=document.querySelector("aside .book-menu-content");addEventListener("beforeunload",function(){localStorage.setItem("menu.scrollTop",e.scrollTop)}),e.scrollTop=localStorage.getItem("menu.scrollTop")})()</script>


 
      </div>
    </aside>

    <div class="book-page">
      <header class="book-header">
        
  <div class="flex align-center justify-between">
  <label for="menu-control">
    <img src="/svg/menu.svg" class="book-icon" alt="Menu" />
  </label>

  <h3>Softmax Regression</h3>

  <label for="toc-control">
    
    <img src="/svg/toc.svg" class="book-icon" alt="Table of Contents" />
    
  </label>
</div>


  
  <aside class="hidden clearfix">
    
  
<nav id="TableOfContents">
  <ul>
    <li><a href="#softmax-regression">Softmax Regression</a></li>
    <li><a href="#statistical-model">Statistical model</a></li>
    <li><a href="#likelihood">Likelihood</a></li>
    <li><a href="#loss-function">Loss function</a></li>
  </ul>
</nav>



  </aside>
  
 
      </header>

      
      
  <article class="markdown book-article"><h2 id="softmax-regression">
  Softmax Regression
  <a class="anchor" href="#softmax-regression">#</a>
</h2>
<p>Softmax Regression is a single layer neural network for multi-class classification.</p>


<script src="/mermaid.min.js"></script>

  <script>mermaid.initialize({
  "flowchart": {
    "useMaxWidth":true
  },
  "theme": "default"
}
)</script>




<pre class="mermaid">
stateDiagram-v2
    direction LR
    x1: $$x_1$$
    x2: $$x_2$$
    x3: $$x_3$$
    xd: $$x_d$$
    a1: $$a_1=\sum_i w_{1i} x_i$$
    a2: $$a_2=\sum_i w_{2i} x_i$$
    ak: $$a_k=\sum_i w_{ki} x_i$$
    z1: $$z_1=\text{softmax}(\mathbf{a})_1$$
    z2: $$z_2=\text{softmax}(\mathbf{a})_2$$
    zk: $$z_k=\text{softmax}(\mathbf{a})_k$$
    x1 --> a1:$$w_{11}$$
    x2 --> a1:$$w_{12}$$
    x3 --> a1:$$w_{13}$$
    xd --> a1:$$w_{1d}$$
    x1 --> a2:$$w_{21}$$
    x2 --> a2:$$w_{22}$$
    x3 --> a2:$$w_{23}$$
    xd --> a2:$$w_{2d}$$
    x1 --> ak:$$w_{k1}$$
    x2 --> ak:$$w_{k2}$$
    x3 --> ak:$$w_{k3}$$
    xd --> ak:$$w_{kd}$$
    a1 --> z1
    a2 --> z2
    ak --> zk
    z1 --> END1:::hidden
    z2 --> END2:::hidden
    zk --> END:::hidden
    note left of xd : Inputs
    note right of a1 : Pre-activations
    note left of zk : Activations
    note left of END : Outputs
    classDef hidden display: none;
</pre>

<h2 id="statistical-model">
  Statistical model
  <a class="anchor" href="#statistical-model">#</a>
</h2>
<p>Given 
<link rel="stylesheet" href="/katex/katex.min.css" />
<script defer src="/katex/katex.min.js"></script>
<script defer src="/katex/auto-render.min.js" onload="renderMathInElement(document.body);"></script><span>
  \(k\)
</span>
 classes the probability of class <span>
  \(i\)
</span>
 for a given feature vector (<span>
  \(\mathbf{x}\in \mathbb{R}^d\)
</span>
) is given by
<span>
  \[
\text{Pr}[y=i|\mathbf{x},(\mathbf{w}_1,\mathbf{w}_2,...,\mathbf{w}_k)] = \frac{\exp(\mathbf{w}_i^T\mathbf{x})}{\sum_{j=1}^{k} \exp(\mathbf{w}_j^T\mathbf{x})}
\]
</span>

where <span>
  \(\mathbf{w}_1,\mathbf{w}_2,...,\mathbf{w}_k\in \mathbb{R}^d\)
</span>
 are the weight vector or <strong>parameters</strong> of the model for each class.</p>
<p>Stacking the weights vectors <span>
  \(\mathbf{w}_1,\mathbf{w}_2,...,\mathbf{w}_k\in \mathbb{R}^d\)
</span>
 into a <strong>weight matrix</strong> <span>
  \(\mathbf{W} \in \mathbb{R}^{d \times k}\)
</span>
 we can write the <strong>pre-activation</strong> vector <span>
  \(\mathbf{a} \in \mathbb{R}^{k}\)
</span>
 as follows.
<span>
  \[
\mathbf{a} = \mathbf{W}^T\mathbf{x}
\]
</span>

The <strong>activation</strong> vector <span>
  \(\mathbf{z} \in \mathbb{R}^{k}\)
</span>
 is given by
<span>
  \[
\mathbf{z} = \text{softmax}(\mathbf{a})
\]
</span>

and the <strong>softmax</strong> activation function is defined as
<span>
  \[
\text{softmax}(\mathbf{a})_i = \frac{\exp(\mathbf{a}_i)}{\sum_{j=1}^{k} \exp(\mathbf{a}_j)}
\]
</span>

Hence
<span>
  \[
\text{Pr}[y=i|\mathbf{x},\mathbf{W}] = \text{softmax}(\mathbf{W}^T\mathbf{x})_i
\]
</span>

We often stack all the <span>
  \(n\)
</span>
 examples into a <em>design matrix</em> <span>
  \(\mathbf{X} \in \mathbb{R^{n \times d}}\)
</span>
, where each row is one instance. The predictions for all the <span>
  \(n\)
</span>
 instances <span>
  \(\mathbf{y} \in \mathbb{R}^{n \times K }\)
</span>
 can be written conveniently as a matrix-vector product.
<span>
  \[
\mathbf{y} = \text{softmax}(\mathbf{X}\mathbf{W})
\]
</span>
</p>
<h2 id="likelihood">
  Likelihood
  <a class="anchor" href="#likelihood">#</a>
</h2>
<p>Given a dataset <span>
  \(\mathcal{D}=\{\mathbf{x}_i \in \mathbb{R}^d,\mathbf{y}_i \in [1,2,..,k]\}_{i=1}^n\)
</span>
 containing <span>
  \(n\)
</span>
 examples we need to estimate the parameter vector <span>
  \(\mathbf{W}\)
</span>
 by maximizing the likelihood of data.</p>
<blockquote>
<p>In practice we minimize the <strong>negative log likelihood</strong>.</p></blockquote>
<p>Let <span>
  \(\mu_i^j\)
</span>
 be the model prediction for class j for each example i in the training dataset.
<span>
  \[
\mu_i^j = \text{Pr}[y_i=j|\mathbf{x}_i,\mathbf{W}] = \text{softmax}(\mathbf{W}^T\mathbf{x}_i)_j
\]
</span>

Let <span>
  \(y_i^j\)
</span>
 be the corresponding true label.
The negative log likelihood (NLL) is given by
<span>
  \[
L(\mathbf{W}) = - \sum_{i=1}^{n} \sum_{j=1}^{k} y_i^j \log \mu_i^j
\]
</span>

This is referred to as the <strong>Cross Entropy</strong> loss. We need to choose the model parameters that optimizes (minimizes) the loss function.
<span>
  \[
\hat{\mathbf{W}} = \argmin_{\mathbf{W}} L(\mathbf{W})
\]
</span>
</p>
<h2 id="loss-function">
  Loss function
  <a class="anchor" href="#loss-function">#</a>
</h2>
<p><strong>Cross Entropy</strong> loss
<span>
  \[
L(\mathbf{W}) = - \sum_{i=1}^{n} \sum_{j=1}^{k} y_i^j \log \mu_i^j
\]
</span>

Compare to the earlier <strong>Binary Cross Entropy</strong> loss
<span>
  \[
L(\mathbf{w}) - \sum_{i=1}^{n} \left[ y_i\log(\mu_i) + (1-y_i)\log(1-\mu_i) \right]
\]
</span>
</p>
<a  href="https://pytorch.org/docs/stable/generated/torch.nn.CrossEntropyLoss.html#torch.nn.CrossEntropyLoss"   target="_blank" rel="noopener"  class="book-btn">torch.nn.CrossEntropyLoss</a>

</article>
 
      

      <footer class="book-footer">
        
  <div class="flex flex-wrap justify-between">





</div>



  <script>(function(){function e(e){const t=window.getSelection(),n=document.createRange();n.selectNodeContents(e),t.removeAllRanges(),t.addRange(n)}document.querySelectorAll("pre code").forEach(t=>{t.addEventListener("click",function(){if(window.getSelection().toString())return;e(t.parentElement),navigator.clipboard&&navigator.clipboard.writeText(t.parentElement.textContent)})})})()</script>


 
        
      </footer>

      
  
  <div class="book-comments">

</div>
  
 

      <label for="menu-control" class="hidden book-menu-overlay"></label>
    </div>

    
    <aside class="book-toc">
      <div class="book-toc-content">
        
  
<nav id="TableOfContents">
  <ul>
    <li><a href="#softmax-regression">Softmax Regression</a></li>
    <li><a href="#statistical-model">Statistical model</a></li>
    <li><a href="#likelihood">Likelihood</a></li>
    <li><a href="#loss-function">Loss function</a></li>
  </ul>
</nav>


 
      </div>
    </aside>
    
  </main>

  
</body>
</html>












