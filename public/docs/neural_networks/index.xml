<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Neural Networks on Deep Learning</title>
    <link>http://localhost:1313/docs/neural_networks/</link>
    <description>Recent content in Neural Networks on Deep Learning</description>
    <generator>Hugo</generator>
    <language>en-us</language>
    <atom:link href="http://localhost:1313/docs/neural_networks/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>Linear Regression</title>
      <link>http://localhost:1313/docs/neural_networks/linear_regression/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>http://localhost:1313/docs/neural_networks/linear_regression/</guid>
      <description>&lt;h2 id=&#34;linear-regression&#34;&gt;&#xA;  Linear Regression&#xA;  &lt;a class=&#34;anchor&#34; href=&#34;#linear-regression&#34;&gt;#&lt;/a&gt;&#xA;&lt;/h2&gt;&#xA;&lt;p&gt;Linear Regression is a single layer neural network for regression.&lt;/p&gt;&#xA;&#xA;&#xA;&lt;script src=&#34;http://localhost:1313/mermaid.min.js&#34;&gt;&lt;/script&gt;&#xA;&#xA;  &lt;script&gt;mermaid.initialize({&#xA;  &#34;flowchart&#34;: {&#xA;    &#34;useMaxWidth&#34;:true&#xA;  },&#xA;  &#34;theme&#34;: &#34;default&#34;&#xA;}&#xA;)&lt;/script&gt;&#xA;&#xA;&#xA;&#xA;&#xA;&lt;pre class=&#34;mermaid&#34;&gt;&#xA;stateDiagram-v2&#xA;    direction LR&#xA;    z1: $$x_1$$&#xA;    z2: $$x_2$$&#xA;    zi: $$x_i$$&#xA;    zM: $$x_d$$&#xA;    aj: $$a=\sum_i w_{i} x_i$$&#xA;    zj: $$z=h(a)=a$$&#xA;    z1 --&gt; aj:$$w_{1}$$&#xA;    z2 --&gt; aj:$$w_{2}$$&#xA;    zi --&gt; aj:$$w_{i}$$&#xA;    zM --&gt; aj:$$w_{d}$$&#xA;    aj --&gt; zj&#xA;    zj --&gt; END:::hidden&#xA;    note left of zM : Inputs&#xA;    note left of aj : Pre-activation&#xA;    note left of zj : Activation&#xA;    note left of END : Output&#xA;    classDef hidden display: none;&#xA;&lt;/pre&gt;&#xA;&#xA;&lt;h2 id=&#34;model&#34;&gt;&#xA;  Model&#xA;  &lt;a class=&#34;anchor&#34; href=&#34;#model&#34;&gt;#&lt;/a&gt;&#xA;&lt;/h2&gt;&#xA;&lt;p&gt;Linear Regression assumes a &lt;strong&gt;linear relationship&lt;/strong&gt; between the target &#xA;&lt;link rel=&#34;stylesheet&#34; href=&#34;http://localhost:1313/katex/katex.min.css&#34; /&gt;&#xA;&lt;script defer src=&#34;http://localhost:1313/katex/katex.min.js&#34;&gt;&lt;/script&gt;&#xA;&lt;script defer src=&#34;http://localhost:1313/katex/auto-render.min.js&#34; onload=&#34;renderMathInElement(document.body);&#34;&gt;&lt;/script&gt;&lt;span&gt;&#xA;  \(y \in \mathbb{R}\)&#xA;&lt;/span&gt;&#xA; and the features &lt;span&gt;&#xA;  \(\mathbf{x}\in \mathbb{R}^d\)&#xA;&lt;/span&gt;&#xA;.&#xA;&lt;span&gt;&#xA;  \[&#xA;y = f(\mathbf{x}) = w_1 x_1 + w_2 x_2 + ... + w_d x_d + b = \mathbf{w}^T\mathbf{x} + b,&#xA;\]&#xA;&lt;/span&gt;&#xA;&#xA;where &lt;span&gt;&#xA;  \(\mathbf{w}\in \mathbb{R}^d\)&#xA;&lt;/span&gt;&#xA; is the &lt;span&gt;&#xA;  \(d\)&#xA;&lt;/span&gt;&#xA;-dimensional &lt;em&gt;weight vector&lt;/em&gt; and &lt;span&gt;&#xA;  \(b \in \mathbb{R}\)&#xA;&lt;/span&gt;&#xA; is the &lt;em&gt;bias term&lt;/em&gt;.&lt;/p&gt;</description>
    </item>
    <item>
      <title>Logistic Regression</title>
      <link>http://localhost:1313/docs/neural_networks/logistic_regression/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>http://localhost:1313/docs/neural_networks/logistic_regression/</guid>
      <description>&lt;h2 id=&#34;logistic-regression&#34;&gt;&#xA;  Logistic Regression&#xA;  &lt;a class=&#34;anchor&#34; href=&#34;#logistic-regression&#34;&gt;#&lt;/a&gt;&#xA;&lt;/h2&gt;&#xA;&lt;p&gt;Logistic Regression is a single layer neural network for binary classification.&lt;/p&gt;&#xA;&#xA;&#xA;&lt;script src=&#34;http://localhost:1313/mermaid.min.js&#34;&gt;&lt;/script&gt;&#xA;&#xA;  &lt;script&gt;mermaid.initialize({&#xA;  &#34;flowchart&#34;: {&#xA;    &#34;useMaxWidth&#34;:true&#xA;  },&#xA;  &#34;theme&#34;: &#34;default&#34;&#xA;}&#xA;)&lt;/script&gt;&#xA;&#xA;&#xA;&#xA;&#xA;&lt;pre class=&#34;mermaid&#34;&gt;&#xA;stateDiagram-v2&#xA;    direction LR&#xA;    z1: $$x_1$$&#xA;    z2: $$x_2$$&#xA;    zi: $$x_i$$&#xA;    zM: $$x_d$$&#xA;    aj: $$a=\sum_i w_{i} x_i$$&#xA;    zj: $$z=\sigma(a)$$&#xA;    z1 --&gt; aj:$$w_{1}$$&#xA;    z2 --&gt; aj:$$w_{2}$$&#xA;    zi --&gt; aj:$$w_{i}$$&#xA;    zM --&gt; aj:$$w_{d}$$&#xA;    aj --&gt; zj&#xA;    zj --&gt; END:::hidden&#xA;    note left of zM : Inputs&#xA;    note left of aj : Pre-activation&#xA;    note left of zj : Activation&#xA;    note left of END : Output&#xA;    classDef hidden display: none;&#xA;&lt;/pre&gt;&#xA;&#xA;&lt;h2 id=&#34;statistical-model&#34;&gt;&#xA;  Statistical model&#xA;  &lt;a class=&#34;anchor&#34; href=&#34;#statistical-model&#34;&gt;#&lt;/a&gt;&#xA;&lt;/h2&gt;&#xA;&lt;p&gt;The probability of the positive class (&#xA;&lt;link rel=&#34;stylesheet&#34; href=&#34;http://localhost:1313/katex/katex.min.css&#34; /&gt;&#xA;&lt;script defer src=&#34;http://localhost:1313/katex/katex.min.js&#34;&gt;&lt;/script&gt;&#xA;&lt;script defer src=&#34;http://localhost:1313/katex/auto-render.min.js&#34; onload=&#34;renderMathInElement(document.body);&#34;&gt;&lt;/script&gt;&lt;span&gt;&#xA;  \(y=1\)&#xA;&lt;/span&gt;&#xA;) for a given feature vector (&lt;span&gt;&#xA;  \(\mathbf{x}\in \mathbb{R}^d\)&#xA;&lt;/span&gt;&#xA;) is given by&#xA;&lt;span&gt;&#xA;  \[&#xA;\text{Pr}[y=1|\mathbf{x},\mathbf{w}] = \sigma(\mathbf{w}^T\mathbf{x})&#xA;\]&#xA;&lt;/span&gt;&#xA;&#xA;where &lt;span&gt;&#xA;  \(\mathbf{w}\in \mathbb{R}^d\)&#xA;&lt;/span&gt;&#xA; are the weights/&lt;strong&gt;parameters&lt;/strong&gt; of the model and &lt;span&gt;&#xA;  \(\sigma\)&#xA;&lt;/span&gt;&#xA; is the &lt;strong&gt;sigmoid&lt;/strong&gt; activation function defined as&#xA;&lt;span&gt;&#xA;  \[&#xA;\sigma(x) = \frac{1}{1-e^{-z}}&#xA;\]&#xA;&lt;/span&gt;&#xA;&lt;/p&gt;</description>
    </item>
    <item>
      <title>Softmax Regression</title>
      <link>http://localhost:1313/docs/neural_networks/softmax_regression/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>http://localhost:1313/docs/neural_networks/softmax_regression/</guid>
      <description>&lt;h2 id=&#34;softmax-regression&#34;&gt;&#xA;  Softmax Regression&#xA;  &lt;a class=&#34;anchor&#34; href=&#34;#softmax-regression&#34;&gt;#&lt;/a&gt;&#xA;&lt;/h2&gt;&#xA;&lt;p&gt;Softmax Regression is a single layer neural network for multi-class classification.&lt;/p&gt;&#xA;&#xA;&#xA;&lt;script src=&#34;http://localhost:1313/mermaid.min.js&#34;&gt;&lt;/script&gt;&#xA;&#xA;  &lt;script&gt;mermaid.initialize({&#xA;  &#34;flowchart&#34;: {&#xA;    &#34;useMaxWidth&#34;:true&#xA;  },&#xA;  &#34;theme&#34;: &#34;default&#34;&#xA;}&#xA;)&lt;/script&gt;&#xA;&#xA;&#xA;&#xA;&#xA;&lt;pre class=&#34;mermaid&#34;&gt;&#xA;stateDiagram-v2&#xA;    direction LR&#xA;    x1: $$x_1$$&#xA;    x2: $$x_2$$&#xA;    xi: $$x_i$$&#xA;    xM: $$x_d$$&#xA;    a1: $$a_1=\sum_i w_{1i} x_i$$&#xA;    a2: $$a_2=\sum_i w_{2i} x_i$$&#xA;    ak: $$a_k=\sum_i w_{ki} x_i$$&#xA;    z1: $$z_1=\text{softmax}(a_1)$$&#xA;    z2: $$z_2=\text{softmax}(a_2)$$&#xA;    zk: $$z_k=\text{softmax}(a_k)$$&#xA;    x1 --&gt; a1:$$w_{11}$$&#xA;    x2 --&gt; a1:$$w_{12}$$&#xA;    xi --&gt; a1:$$w_{1i}$$&#xA;    xM --&gt; a1:$$w_{1d}$$&#xA;    x1 --&gt; a2:$$w_{21}$$&#xA;    x2 --&gt; a2:$$w_{22}$$&#xA;    xi --&gt; a2:$$w_{2i}$$&#xA;    xM --&gt; a2:$$w_{2d}$$&#xA;    x1 --&gt; ak:$$w_{k1}$$&#xA;    x2 --&gt; ak:$$w_{k2}$$&#xA;    xi --&gt; ak:$$w_{ki}$$&#xA;    xM --&gt; ak:$$w_{kd}$$&#xA;    a1 --&gt; z1&#xA;    a2 --&gt; z2&#xA;    ak --&gt; zk&#xA;    z1 --&gt; END1:::hidden&#xA;    z2 --&gt; END2:::hidden&#xA;    zk --&gt; END:::hidden&#xA;    note left of xM : Inputs&#xA;    note left of ak : Pre-activation&#xA;    note left of zk : Activation&#xA;    note right of END : Output&#xA;    classDef hidden display: none;&#xA;&lt;/pre&gt;&#xA;&#xA;&lt;h2 id=&#34;statistical-model&#34;&gt;&#xA;  Statistical model&#xA;  &lt;a class=&#34;anchor&#34; href=&#34;#statistical-model&#34;&gt;#&lt;/a&gt;&#xA;&lt;/h2&gt;&#xA;&lt;p&gt;Given &#xA;&lt;link rel=&#34;stylesheet&#34; href=&#34;http://localhost:1313/katex/katex.min.css&#34; /&gt;&#xA;&lt;script defer src=&#34;http://localhost:1313/katex/katex.min.js&#34;&gt;&lt;/script&gt;&#xA;&lt;script defer src=&#34;http://localhost:1313/katex/auto-render.min.js&#34; onload=&#34;renderMathInElement(document.body);&#34;&gt;&lt;/script&gt;&lt;span&gt;&#xA;  \(k\)&#xA;&lt;/span&gt;&#xA; classes the probability of class &lt;span&gt;&#xA;  \(i\)&#xA;&lt;/span&gt;&#xA; for a given feature vector (&lt;span&gt;&#xA;  \(\mathbf{x}\in \mathbb{R}^d\)&#xA;&lt;/span&gt;&#xA;) is given by&#xA;&lt;span&gt;&#xA;  \[&#xA;\text{Pr}[y=i|\mathbf{x},(\mathbf{w}_1,\mathbf{w}_2,...,\mathbf{w}_k)] = \frac{\exp(\mathbf{w}_i^T\mathbf{x})}{\sum_{j=1}^{k} \exp(\mathbf{w}_j^T\mathbf{x})}&#xA;\]&#xA;&lt;/span&gt;&#xA;&#xA;where &lt;span&gt;&#xA;  \(\mathbf{w}_1,\mathbf{w}_2,...,\mathbf{w}_k\in \mathbb{R}^d\)&#xA;&lt;/span&gt;&#xA; are the weight vector or &lt;strong&gt;parameters&lt;/strong&gt; of the model for each class.&lt;/p&gt;</description>
    </item>
    <item>
      <title>Multi-layer perceptrons</title>
      <link>http://localhost:1313/docs/neural_networks/mlp/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>http://localhost:1313/docs/neural_networks/mlp/</guid>
      <description>&lt;h2 id=&#34;multi-layer-perceptrons&#34;&gt;&#xA;  Multi-layer perceptrons&#xA;  &lt;a class=&#34;anchor&#34; href=&#34;#multi-layer-perceptrons&#34;&gt;#&lt;/a&gt;&#xA;&lt;/h2&gt;</description>
    </item>
    <item>
      <title>Activation functions</title>
      <link>http://localhost:1313/docs/neural_networks/activations/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>http://localhost:1313/docs/neural_networks/activations/</guid>
      <description>&lt;h2 id=&#34;sigmoid&#34;&gt;&#xA;  Sigmoid&#xA;  &lt;a class=&#34;anchor&#34; href=&#34;#sigmoid&#34;&gt;#&lt;/a&gt;&#xA;&lt;/h2&gt;&#xA;&lt;h2 id=&#34;relu&#34;&gt;&#xA;  ReLU&#xA;  &lt;a class=&#34;anchor&#34; href=&#34;#relu&#34;&gt;#&lt;/a&gt;&#xA;&lt;/h2&gt;&#xA;&lt;h2 id=&#34;gelu&#34;&gt;&#xA;  GeLU&#xA;  &lt;a class=&#34;anchor&#34; href=&#34;#gelu&#34;&gt;#&lt;/a&gt;&#xA;&lt;/h2&gt;&#xA;&lt;h2 id=&#34;glu&#34;&gt;&#xA;  GLU&#xA;  &lt;a class=&#34;anchor&#34; href=&#34;#glu&#34;&gt;#&lt;/a&gt;&#xA;&lt;/h2&gt;&#xA;&lt;h2 id=&#34;swish&#34;&gt;&#xA;  Swish&#xA;  &lt;a class=&#34;anchor&#34; href=&#34;#swish&#34;&gt;#&lt;/a&gt;&#xA;&lt;/h2&gt;&#xA;&lt;h2 id=&#34;swiglu&#34;&gt;&#xA;  SwiGLU&#xA;  &lt;a class=&#34;anchor&#34; href=&#34;#swiglu&#34;&gt;#&lt;/a&gt;&#xA;&lt;/h2&gt;&#xA;&lt;p&gt;&lt;a href=&#34;https://arxiv.org/pdf/2002.05202&#34;&gt;GLU Variants Improve Transformer&lt;/a&gt;&lt;/p&gt;&#xA;&lt;blockquote&gt;&#xA;&lt;p&gt;We offer no explanation as to why these architectures seem to work; we attribute their success, as all else, to divine benevolence&lt;/p&gt;&lt;/blockquote&gt;</description>
    </item>
    <item>
      <title>Entropy</title>
      <link>http://localhost:1313/docs/neural_networks/entropy/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>http://localhost:1313/docs/neural_networks/entropy/</guid>
      <description>&lt;blockquote&gt;&#xA;&lt;p&gt;A brief primer on entropy, cross-entropy and perplexity.&lt;/p&gt;&lt;/blockquote&gt;&#xA;&lt;h2 id=&#34;entropy&#34;&gt;&#xA;  Entropy&#xA;  &lt;a class=&#34;anchor&#34; href=&#34;#entropy&#34;&gt;#&lt;/a&gt;&#xA;&lt;/h2&gt;&#xA;&lt;p&gt;&#xA;&lt;link rel=&#34;stylesheet&#34; href=&#34;http://localhost:1313/katex/katex.min.css&#34; /&gt;&#xA;&lt;script defer src=&#34;http://localhost:1313/katex/katex.min.js&#34;&gt;&lt;/script&gt;&#xA;&lt;script defer src=&#34;http://localhost:1313/katex/auto-render.min.js&#34; onload=&#34;renderMathInElement(document.body);&#34;&gt;&lt;/script&gt;&lt;span&gt;&#xA;  \(\)&#xA;&lt;/span&gt;&#xA;The &lt;strong&gt;entropy&lt;/strong&gt; of a discrete random variable &lt;span&gt;&#xA;  \(X\)&#xA;&lt;/span&gt;&#xA; with &lt;span&gt;&#xA;  \(K\)&#xA;&lt;/span&gt;&#xA; states/categories with distribution &lt;span&gt;&#xA;  \(p_k = \text{Pr}(X=k)\)&#xA;&lt;/span&gt;&#xA; for &lt;span&gt;&#xA;  \(k=1,...,K\)&#xA;&lt;/span&gt;&#xA;  is a measure of uncertainty and is defined as follows.&#xA;&lt;span&gt;&#xA;  \[H(X) = \sum_{k=1}^{K} p_k \log_2 \frac{1}{p_k} = - \sum_{k=1}^{K} p_k \log_2 p_k \]&#xA;&lt;/span&gt;&#xA;&#xA;&lt;span&gt;&#xA;  \(\)&#xA;&lt;/span&gt;&#xA;&#xA;The term &lt;span&gt;&#xA;  \(\log_2\frac{1}{p}\)&#xA;&lt;/span&gt;&#xA; quantifies the notion or surprise or uncertainty and hence entropy is the average uncertainty.&lt;/p&gt;</description>
    </item>
  </channel>
</rss>
