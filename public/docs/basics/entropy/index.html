<!DOCTYPE html>
<html lang="en-us" dir="ltr">
<head><script src="/livereload.js?mindelay=10&amp;v=2&amp;port=1313&amp;path=livereload" data-no-instant defer></script>
  <meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1.0">
<meta name="description" content="
A brief primer on entropy, cross-entropy and perplexity.

  Entropy
  #





  \(\)

The entropy of a discrete random variable 
  \(X\)

 with 
  \(K\)

 states/categories with distribution 
  \(p_k = \text{Pr}(X=k)\)

 for 
  \(k=1,...,K\)

  is a measure of uncertainty and is defined as follows.

  \[H(X) = \sum_{k=1}^{K} p_k \log_2 \frac{1}{p_k} = - \sum_{k=1}^{K} p_k \log_2 p_k \]



  \(\)


The term 
  \(\log_2\frac{1}{p}\)

 quantifies the notion or surprise or uncertainty and hence entropy is the average uncertainty.">
<meta name="theme-color" media="(prefers-color-scheme: light)" content="#ffffff">
<meta name="theme-color" media="(prefers-color-scheme: dark)" content="#343a40">
<meta name="color-scheme" content="light dark"><meta property="og:url" content="http://localhost:1313/docs/basics/entropy/">
  <meta property="og:site_name" content="Deep Learning">
  <meta property="og:title" content="Entropy">
  <meta property="og:description" content="A brief primer on entropy, cross-entropy and perplexity.
Entropy # \(\) The entropy of a discrete random variable \(X\) with \(K\) states/categories with distribution \(p_k = \text{Pr}(X=k)\) for \(k=1,...,K\) is a measure of uncertainty and is defined as follows. \[H(X) = \sum_{k=1}^{K} p_k \log_2 \frac{1}{p_k} = - \sum_{k=1}^{K} p_k \log_2 p_k \] \(\) The term \(\log_2\frac{1}{p}\) quantifies the notion or surprise or uncertainty and hence entropy is the average uncertainty.">
  <meta property="og:locale" content="en_us">
  <meta property="og:type" content="article">
    <meta property="article:section" content="docs">
<title>Entropy | Deep Learning</title>
<link rel="icon" href="/favicon.png" >
<link rel="manifest" href="/manifest.json">
<link rel="canonical" href="http://localhost:1313/docs/basics/entropy/">
<link rel="stylesheet" href="/book.min.6c8b9d2a1fc95075ed7da46ca81060b39add8fff6741ac51259f768929281e2c.css" integrity="sha256-bIudKh/JUHXtfaRsqBBgs5rdj/9nQaxRJZ92iSkoHiw=" crossorigin="anonymous">
  <script defer src="/fuse.min.js"></script>
  <script defer src="/en.search.min.24e5a6133e2174d2f5691ca2bf8956293d655dd8f57954880046ed507e29f4ec.js" integrity="sha256-JOWmEz4hdNL1aRyiv4lWKT1lXdj1eVSIAEbtUH4p9Ow=" crossorigin="anonymous"></script>
<!--
Made with Book Theme
https://github.com/alex-shpak/hugo-book
-->
  
</head>
<body dir="ltr">
  <input type="checkbox" class="hidden toggle" id="menu-control" />
  <input type="checkbox" class="hidden toggle" id="toc-control" />
  <main class="container flex">
    <aside class="book-menu">
      <div class="book-menu-content">
        
  <nav>
<h2 class="book-brand">
  <a class="flex align-center" href="/"><span>Deep Learning</span>
  </a>
</h2>


<div class="book-search hidden">
  <input type="text" id="book-search-input" placeholder="Search" aria-label="Search" maxlength="64" data-hotkeys="s/" />
  <div class="book-search-spinner hidden"></div>
  <ul id="book-search-results"></ul>
</div>
<script>document.querySelector(".book-search").classList.remove("hidden")</script>















  
  <ul>
    
      
        <li class="book-section-flat" >
          
  
  

  
    <a href="/docs/basics/" class="">Basics</a>
  

          
  <ul>
    
      
        <li>
          
  
  

  
    <a href="/docs/basics/entropy/" class="active">Entropy</a>
  

        </li>
      
    
      
        <li>
          
  
  

  
    <a href="/docs/basics/metrics/" class="">Metrics</a>
  

        </li>
      
    
      
        <li>
          
  
  

  
    <a href="/docs/basics/gpu/" class="">GPU primer</a>
  

        </li>
      
    
  </ul>

        </li>
      
    
      
        <li class="book-section-flat" >
          
  
  

  
    <a href="/docs/neural_networks/" class="">Neural Networks</a>
  

          
  <ul>
    
      
        <li>
          
  
  

  
    <a href="/docs/neural_networks/linear_regression/" class="">Linear Regression</a>
  

        </li>
      
    
      
        <li>
          
  
  

  
    <a href="/docs/neural_networks/logistic_regression/" class="">Logistic Regression</a>
  

        </li>
      
    
      
        <li>
          
  
  

  
    <a href="/docs/neural_networks/softmax_regression/" class="">Softmax Regression</a>
  

        </li>
      
    
      
        <li>
          
  
  

  
    <a href="/docs/neural_networks/mlp/" class="">Multilayer perceptron</a>
  

        </li>
      
    
      
        <li>
          
  
  

  
    <a href="/docs/neural_networks/activations/" class="">Activation functions</a>
  

        </li>
      
    
  </ul>

        </li>
      
    
      
        <li class="book-section-flat" >
          
  
  

  
    <a href="/docs/training/" class="">Training</a>
  

          
  <ul>
    
      
        <li>
          
  
  

  
    <a href="/docs/training/gradient_descent/" class="">Gradient Descent</a>
  

        </li>
      
    
      
        <li>
          
  
  

  
    <a href="/docs/training/backpropagation/" class="">Backpropagation</a>
  

        </li>
      
    
      
        <li>
          
  
  

  
    <a href="/docs/training/autograd/" class="">AutoDiff</a>
  

        </li>
      
    
      
        <li>
          
  
  

  
    <a href="/docs/training/initialization/" class="">Initialization</a>
  

        </li>
      
    
      
        <li>
          
  
  

  
    <a href="/docs/training/normalization/" class="">Normalization</a>
  

        </li>
      
    
      
        <li>
          
  
  

  
    <a href="/docs/training/regularization/" class="">Regularization</a>
  

        </li>
      
    
      
        <li>
          
  
  

  
    <a href="/docs/training/training_loop/" class="">Training loop</a>
  

        </li>
      
    
      
        <li>
          
  
  

  
    <a href="/docs/training/quiz/" class="">Quiz</a>
  

        </li>
      
    
      
        <li>
          
  
  

  
    <a href="/docs/training/coding/" class="">Coding</a>
  

        </li>
      
    
  </ul>

        </li>
      
    
      
        <li class="book-section-flat" >
          
  
  

  
    <a href="/docs/transformers/" class="">Transformers</a>
  

          
  <ul>
    
      
        <li>
          
  
  

  
    <a href="/docs/transformers/tokenizers/" class="">Tokenizers</a>
  

        </li>
      
    
      
        <li>
          
  
  

  
    <a href="/docs/transformers/transformers101/" class="">Transformers101</a>
  

        </li>
      
    
      
        <li>
          
  
  

  
    <a href="/docs/transformers/transformers102/" class="">Transformers102</a>
  

        </li>
      
    
      
        <li>
          
  
  

  
    <a href="/docs/transformers/alignment/" class="">Alignment</a>
  

        </li>
      
    
      
        <li>
          
  
  

  
    <a href="/docs/transformers/attentionvariants/" class="">Attention variants</a>
  

        </li>
      
    
      
        <li>
          
  
  

  
    <a href="/docs/transformers/inference/" class="">Inference</a>
  

        </li>
      
    
      
        <li>
          
  
  

  
    <a href="/docs/transformers/models/" class="">Frontier models</a>
  

        </li>
      
    
      
        <li>
          
  
  

  
    <a href="/docs/transformers/moe/" class="">MoE</a>
  

        </li>
      
    
      
        <li>
          
  
  

  
    <a href="/docs/transformers/scaling/" class="">Scaling</a>
  

        </li>
      
    
      
        <li>
          
  
  

  
    <a href="/docs/transformers/ssm/" class="">SSM</a>
  

        </li>
      
    
      
        <li>
          
  
  

  
    <a href="/docs/transformers/datasets/" class="">Datasets</a>
  

        </li>
      
    
  </ul>

        </li>
      
    
      
        <li class="book-section-flat" >
          
  
  

  
    <a href="/docs/rl/" class="">Reinforcement</a>
  

          
  <ul>
    
      
        <li>
          
  
  

  
    <a href="/docs/rl/basics/" class="">Basics</a>
  

        </li>
      
    
  </ul>

        </li>
      
    
  </ul>














</nav>




  <script>(function(){var e=document.querySelector("aside .book-menu-content");addEventListener("beforeunload",function(){localStorage.setItem("menu.scrollTop",e.scrollTop)}),e.scrollTop=localStorage.getItem("menu.scrollTop")})()</script>


 
      </div>
    </aside>

    <div class="book-page">
      <header class="book-header">
        
  <div class="flex align-center justify-between">
  <label for="menu-control">
    <img src="/svg/menu.svg" class="book-icon" alt="Menu" />
  </label>

  <h3>Entropy</h3>

  <label for="toc-control">
    
    <img src="/svg/toc.svg" class="book-icon" alt="Table of Contents" />
    
  </label>
</div>


  
  <aside class="hidden clearfix">
    
  
<nav id="TableOfContents">
  <ul>
    <li><a href="#entropy">Entropy</a></li>
    <li><a href="#binary-entropy">Binary entropy</a></li>
    <li><a href="#cross-entropy">Cross entropy</a></li>
    <li><a href="#perplexity">Perplexity</a></li>
    <li><a href="#kl-divergence">KL Divergence</a></li>
    <li><a href="#mutual-information">Mutual Information</a></li>
  </ul>
</nav>



  </aside>
  
 
      </header>

      
      
  <article class="markdown book-article"><blockquote>
<p>A brief primer on entropy, cross-entropy and perplexity.</p></blockquote>
<h2 id="entropy">
  Entropy
  <a class="anchor" href="#entropy">#</a>
</h2>
<p>
<link rel="stylesheet" href="/katex/katex.min.css" />
<script defer src="/katex/katex.min.js"></script>
<script defer src="/katex/auto-render.min.js" onload="renderMathInElement(document.body);"></script><span>
  \(\)
</span>
The <strong>entropy</strong> of a discrete random variable <span>
  \(X\)
</span>
 with <span>
  \(K\)
</span>
 states/categories with distribution <span>
  \(p_k = \text{Pr}(X=k)\)
</span>
 for <span>
  \(k=1,...,K\)
</span>
  is a measure of uncertainty and is defined as follows.
<span>
  \[H(X) = \sum_{k=1}^{K} p_k \log_2 \frac{1}{p_k} = - \sum_{k=1}^{K} p_k \log_2 p_k \]
</span>

<span>
  \(\)
</span>

The term <span>
  \(\log_2\frac{1}{p}\)
</span>
 quantifies the notion or surprise or uncertainty and hence entropy is the average uncertainty.</p>
<p>The unit is bits (<span>
  \(\in [0,\log_2 K]\)
</span>
) (or nats incase of natural log).</p>
<p>The discrete distribution with maximum entropy (<span>
  \(\log_2 K\)
</span>
) is uniform.</p>
<p>The discrete distribution with minimum entropy (<span>
  \(0\)
</span>
) is any delta function which puts all mass on one state/category.</p>
<blockquote class="book-hint info">
<p><a href="https://www.princeton.edu/~wbialek/rome/refs/shannon_51.pdf">Prediction and Entropy of Printed English</a>, C. E. Shannon, 1950.</p>
</blockquote>
<h2 id="binary-entropy">
  Binary entropy
  <a class="anchor" href="#binary-entropy">#</a>
</h2>
<p><span>
  \(\)
</span>
For a binary random variable <span>
  \(X \in {0,1}\)
</span>
 with <span>
  \(\text{Pr}(X=1) = \theta\)
</span>
 and <span>
  \(\text{Pr}(X=0) = 1-\theta\)
</span>
 the entropy is as follows.</p>
<span>
  \[H(\theta) = - [ \theta \log_2 \theta + (1-\theta) \log_2 (1-\theta) ] \]
</span>

<p>The range is <span>
  \(H(\theta) \in [0,1]\)
</span>
 and is maximum when <span>
  \(\theta=0.5\)
</span>
.</p>
<h2 id="cross-entropy">
  Cross entropy
  <a class="anchor" href="#cross-entropy">#</a>
</h2>
<p><span>
  \(\)
</span>
Cross entropy is the average number of bits needed to encode the data from from a source <span>
  \(p\)
</span>
 when we model it using <span>
  \(q\)
</span>
.</p>
<span>
  \[H(p,q) = - \sum_{k=1}^{K} p_k \log_2 q_k \]
</span>

<h2 id="perplexity">
  Perplexity
  <a class="anchor" href="#perplexity">#</a>
</h2>
<span>
  \[\text{PPL}(p,q) = 2^{H(p,q)}\]
</span>

<span>
  \[\text{PPL}(p,q) = e^{H(p,q)}\]
</span>

<h2 id="kl-divergence">
  KL Divergence
  <a class="anchor" href="#kl-divergence">#</a>
</h2>
<p>The <strong>Kullback-Leibler</strong> (KL) divergence or <strong>relative entropy</strong> measures the dissimilarity between two probability distributions <span>
  \(p\)
</span>
 and <span>
  \(q\)
</span>
.</p>
<span>
  \[KL(p,q) = \sum_{k=1}^{K} p_k \log_2 \frac{p_k}{q_k}\]
</span>

<span>
  \[KL(p,q) = H(p,q) - H(p,p) \geq 0\]
</span>

<h2 id="mutual-information">
  Mutual Information
  <a class="anchor" href="#mutual-information">#</a>
</h2>
<span>
  \(I(X,Y) = KL(P(X,Y)\|P(X)P(Y))\)
</span>

</article>
 
      

      <footer class="book-footer">
        
  <div class="flex flex-wrap justify-between">





</div>



  <script>(function(){function e(e){const t=window.getSelection(),n=document.createRange();n.selectNodeContents(e),t.removeAllRanges(),t.addRange(n)}document.querySelectorAll("pre code").forEach(t=>{t.addEventListener("click",function(){if(window.getSelection().toString())return;e(t.parentElement),navigator.clipboard&&navigator.clipboard.writeText(t.parentElement.textContent)})})})()</script>


 
        
      </footer>

      
  
  <div class="book-comments">

</div>
  
 

      <label for="menu-control" class="hidden book-menu-overlay"></label>
    </div>

    
    <aside class="book-toc">
      <div class="book-toc-content">
        
  
<nav id="TableOfContents">
  <ul>
    <li><a href="#entropy">Entropy</a></li>
    <li><a href="#binary-entropy">Binary entropy</a></li>
    <li><a href="#cross-entropy">Cross entropy</a></li>
    <li><a href="#perplexity">Perplexity</a></li>
    <li><a href="#kl-divergence">KL Divergence</a></li>
    <li><a href="#mutual-information">Mutual Information</a></li>
  </ul>
</nav>


 
      </div>
    </aside>
    
  </main>

  
</body>
</html>












