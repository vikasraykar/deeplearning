<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Training deep neural networks on Deep Learning</title>
    <link>http://localhost:1313/docs/training/</link>
    <description>Recent content in Training deep neural networks on Deep Learning</description>
    <generator>Hugo</generator>
    <language>en-us</language>
    <atom:link href="http://localhost:1313/docs/training/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>Models</title>
      <link>http://localhost:1313/docs/training/model/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>http://localhost:1313/docs/training/model/</guid>
      <description>&lt;h2 id=&#34;models&#34;&gt;&#xA;  Models&#xA;  &lt;a class=&#34;anchor&#34; href=&#34;#models&#34;&gt;#&lt;/a&gt;&#xA;&lt;/h2&gt;&#xA;&lt;h2 id=&#34;parameters&#34;&gt;&#xA;  Parameters&#xA;  &lt;a class=&#34;anchor&#34; href=&#34;#parameters&#34;&gt;#&lt;/a&gt;&#xA;&lt;/h2&gt;&#xA;&lt;h2 id=&#34;loss-functions&#34;&gt;&#xA;  Loss functions&#xA;  &lt;a class=&#34;anchor&#34; href=&#34;#loss-functions&#34;&gt;#&lt;/a&gt;&#xA;&lt;/h2&gt;</description>
    </item>
    <item>
      <title>Gradient Descent</title>
      <link>http://localhost:1313/docs/training/gradient_descent/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>http://localhost:1313/docs/training/gradient_descent/</guid>
      <description>&lt;h1 id=&#34;gradient-descent&#34;&gt;&#xA;  Gradient Descent&#xA;  &lt;a class=&#34;anchor&#34; href=&#34;#gradient-descent&#34;&gt;#&lt;/a&gt;&#xA;&lt;/h1&gt;&#xA;&lt;blockquote&gt;&#xA;&lt;p&gt;Steepest descent.&lt;/p&gt;&lt;/blockquote&gt;&#xA;&lt;p&gt;Let &lt;span&gt;&#xA;  \(\mathbf{w}\)&#xA;&lt;/span&gt;&#xA; be a vector of all the parameters for a model.&lt;/p&gt;&#xA;&lt;p&gt;Let &lt;span&gt;&#xA;  \(L(\mathbf{w})\)&#xA;&lt;/span&gt;&#xA; be the loss function (or error function).&lt;/p&gt;&#xA;&lt;p&gt;We need to choose the model parameters that optimizes (minimizes) the loss function.&lt;/p&gt;&#xA;&lt;span&gt;&#xA;  \[&#xA;\hat{\mathbf{w}} = \argmin_{\mathbf{w}} L(\mathbf{w})&#xA;\]&#xA;&lt;/span&gt;&#xA;&#xA;&lt;p&gt;Let &lt;span&gt;&#xA;  \(\nabla L(\mathbf{w})\)&#xA;&lt;/span&gt;&#xA; be the &lt;strong&gt;gradient vector&lt;/strong&gt;, where each element is the partial derivative of the loss fucntion wrt each parameter.&lt;/p&gt;&#xA;&lt;p&gt;The gradient vector points in the direction of the greatest rate of increase of the loss function.&lt;/p&gt;</description>
    </item>
    <item>
      <title>Normalization</title>
      <link>http://localhost:1313/docs/training/normalization/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>http://localhost:1313/docs/training/normalization/</guid>
      <description>&lt;h2 id=&#34;data-normalization&#34;&gt;&#xA;  Data normalization&#xA;  &lt;a class=&#34;anchor&#34; href=&#34;#data-normalization&#34;&gt;#&lt;/a&gt;&#xA;&lt;/h2&gt;&#xA;&lt;h2 id=&#34;batch-normalization&#34;&gt;&#xA;  Batch normalization&#xA;  &lt;a class=&#34;anchor&#34; href=&#34;#batch-normalization&#34;&gt;#&lt;/a&gt;&#xA;&lt;/h2&gt;&#xA;&lt;h2 id=&#34;layer-normalization&#34;&gt;&#xA;  Layer normalization&#xA;  &lt;a class=&#34;anchor&#34; href=&#34;#layer-normalization&#34;&gt;#&lt;/a&gt;&#xA;&lt;/h2&gt;</description>
    </item>
    <item>
      <title>Backpropagation</title>
      <link>http://localhost:1313/docs/training/backpropagation/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>http://localhost:1313/docs/training/backpropagation/</guid>
      <description>&lt;h2 id=&#34;backpropagation&#34;&gt;&#xA;  Backpropagation&#xA;  &lt;a class=&#34;anchor&#34; href=&#34;#backpropagation&#34;&gt;#&lt;/a&gt;&#xA;&lt;/h2&gt;&#xA;&lt;h2 id=&#34;algorithmic-differenciation&#34;&gt;&#xA;  Algorithmic differenciation&#xA;  &lt;a class=&#34;anchor&#34; href=&#34;#algorithmic-differenciation&#34;&gt;#&lt;/a&gt;&#xA;&lt;/h2&gt;&#xA;&lt;h2 id=&#34;autograd&#34;&gt;&#xA;  Autograd&#xA;  &lt;a class=&#34;anchor&#34; href=&#34;#autograd&#34;&gt;#&lt;/a&gt;&#xA;&lt;/h2&gt;</description>
    </item>
    <item>
      <title>Quiz</title>
      <link>http://localhost:1313/docs/training/quiz/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>http://localhost:1313/docs/training/quiz/</guid>
      <description>&lt;h2 id=&#34;question-1&#34;&gt;&#xA;  Question 1&#xA;  &lt;a class=&#34;anchor&#34; href=&#34;#question-1&#34;&gt;#&lt;/a&gt;&#xA;&lt;/h2&gt;</description>
    </item>
    <item>
      <title>Coding</title>
      <link>http://localhost:1313/docs/training/coding/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>http://localhost:1313/docs/training/coding/</guid>
      <description>&lt;h2 id=&#34;coding-assignment&#34;&gt;&#xA;  Coding assignment&#xA;  &lt;a class=&#34;anchor&#34; href=&#34;#coding-assignment&#34;&gt;#&lt;/a&gt;&#xA;&lt;/h2&gt;</description>
    </item>
    <item>
      <title></title>
      <link>http://localhost:1313/docs/training/regularization/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>http://localhost:1313/docs/training/regularization/</guid>
      <description></description>
    </item>
  </channel>
</rss>
