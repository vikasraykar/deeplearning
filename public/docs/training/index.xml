<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Training deep neural networks on Deep Learning</title>
    <link>http://localhost:1313/docs/training/</link>
    <description>Recent content in Training deep neural networks on Deep Learning</description>
    <generator>Hugo</generator>
    <language>en-us</language>
    <atom:link href="http://localhost:1313/docs/training/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>Models</title>
      <link>http://localhost:1313/docs/training/model/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>http://localhost:1313/docs/training/model/</guid>
      <description>&lt;h2 id=&#34;models&#34;&gt;&#xA;  Models&#xA;  &lt;a class=&#34;anchor&#34; href=&#34;#models&#34;&gt;#&lt;/a&gt;&#xA;&lt;/h2&gt;&#xA;&lt;h2 id=&#34;parameters&#34;&gt;&#xA;  Parameters&#xA;  &lt;a class=&#34;anchor&#34; href=&#34;#parameters&#34;&gt;#&lt;/a&gt;&#xA;&lt;/h2&gt;&#xA;&lt;h2 id=&#34;loss-functions&#34;&gt;&#xA;  Loss functions&#xA;  &lt;a class=&#34;anchor&#34; href=&#34;#loss-functions&#34;&gt;#&lt;/a&gt;&#xA;&lt;/h2&gt;</description>
    </item>
    <item>
      <title>Gradient Descent</title>
      <link>http://localhost:1313/docs/training/gradient_descent/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>http://localhost:1313/docs/training/gradient_descent/</guid>
      <description>&lt;h1 id=&#34;gradient-descent&#34;&gt;&#xA;  Gradient Descent&#xA;  &lt;a class=&#34;anchor&#34; href=&#34;#gradient-descent&#34;&gt;#&lt;/a&gt;&#xA;&lt;/h1&gt;&#xA;&lt;blockquote&gt;&#xA;&lt;p&gt;Steepest descent.&lt;/p&gt;&lt;/blockquote&gt;&#xA;&lt;h2 id=&#34;batch-gradient-descent&#34;&gt;&#xA;  Batch Gradient Descent&#xA;  &lt;a class=&#34;anchor&#34; href=&#34;#batch-gradient-descent&#34;&gt;#&lt;/a&gt;&#xA;&lt;/h2&gt;&#xA;&lt;p&gt;We take a small step in the direction of the &lt;strong&gt;negative gradient&lt;/strong&gt;.&lt;/p&gt;&#xA;&#xA;&lt;link rel=&#34;stylesheet&#34; href=&#34;http://localhost:1313/katex/katex.min.css&#34; /&gt;&#xA;&lt;script defer src=&#34;http://localhost:1313/katex/katex.min.js&#34;&gt;&lt;/script&gt;&#xA;&lt;script defer src=&#34;http://localhost:1313/katex/auto-render.min.js&#34; onload=&#34;renderMathInElement(document.body);&#34;&gt;&lt;/script&gt;&lt;span&gt;&#xA;  \[&#xA;\mathbf{w}^t \leftarrow \mathbf{w}^{t-1} - \eta \nabla L(\mathbf{w}^{t-1})&#xA;\]&#xA;&lt;/span&gt;&#xA;&#xA;&lt;p&gt;The parameter &lt;span&gt;&#xA;  \(\eta &gt; 0\)&#xA;&lt;/span&gt;&#xA; is called the &lt;strong&gt;learning rate&lt;/strong&gt;.&lt;/p&gt;&#xA;&lt;p&gt;Each step requires that the &lt;strong&gt;entire training data&lt;/strong&gt; be processed to compute the gradient &lt;span&gt;&#xA;  \(\nabla L(\mathbf{w}^{t-1})\)&#xA;&lt;/span&gt;&#xA;. For large datasets this is not comptationally efficient.&lt;/p&gt;</description>
    </item>
    <item>
      <title>Normalization</title>
      <link>http://localhost:1313/docs/training/normalization/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>http://localhost:1313/docs/training/normalization/</guid>
      <description>&lt;h2 id=&#34;data-normalization&#34;&gt;&#xA;  Data normalization&#xA;  &lt;a class=&#34;anchor&#34; href=&#34;#data-normalization&#34;&gt;#&lt;/a&gt;&#xA;&lt;/h2&gt;&#xA;&lt;h2 id=&#34;batch-normalization&#34;&gt;&#xA;  Batch normalization&#xA;  &lt;a class=&#34;anchor&#34; href=&#34;#batch-normalization&#34;&gt;#&lt;/a&gt;&#xA;&lt;/h2&gt;&#xA;&lt;h2 id=&#34;layer-normalization&#34;&gt;&#xA;  Layer normalization&#xA;  &lt;a class=&#34;anchor&#34; href=&#34;#layer-normalization&#34;&gt;#&lt;/a&gt;&#xA;&lt;/h2&gt;</description>
    </item>
    <item>
      <title>Backpropagation</title>
      <link>http://localhost:1313/docs/training/backpropagation/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>http://localhost:1313/docs/training/backpropagation/</guid>
      <description>&lt;h2 id=&#34;backpropagation&#34;&gt;&#xA;  Backpropagation&#xA;  &lt;a class=&#34;anchor&#34; href=&#34;#backpropagation&#34;&gt;#&lt;/a&gt;&#xA;&lt;/h2&gt;&#xA;&lt;h2 id=&#34;algorithmic-differenciation&#34;&gt;&#xA;  Algorithmic differenciation&#xA;  &lt;a class=&#34;anchor&#34; href=&#34;#algorithmic-differenciation&#34;&gt;#&lt;/a&gt;&#xA;&lt;/h2&gt;&#xA;&lt;h2 id=&#34;autograd&#34;&gt;&#xA;  Autograd&#xA;  &lt;a class=&#34;anchor&#34; href=&#34;#autograd&#34;&gt;#&lt;/a&gt;&#xA;&lt;/h2&gt;</description>
    </item>
    <item>
      <title>Quiz</title>
      <link>http://localhost:1313/docs/training/quiz/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>http://localhost:1313/docs/training/quiz/</guid>
      <description>&lt;h2 id=&#34;question-1&#34;&gt;&#xA;  Question 1&#xA;  &lt;a class=&#34;anchor&#34; href=&#34;#question-1&#34;&gt;#&lt;/a&gt;&#xA;&lt;/h2&gt;</description>
    </item>
    <item>
      <title>Coding</title>
      <link>http://localhost:1313/docs/training/coding/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>http://localhost:1313/docs/training/coding/</guid>
      <description>&lt;h2 id=&#34;coding-assignment&#34;&gt;&#xA;  Coding assignment&#xA;  &lt;a class=&#34;anchor&#34; href=&#34;#coding-assignment&#34;&gt;#&lt;/a&gt;&#xA;&lt;/h2&gt;</description>
    </item>
    <item>
      <title></title>
      <link>http://localhost:1313/docs/training/regularization/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>http://localhost:1313/docs/training/regularization/</guid>
      <description></description>
    </item>
  </channel>
</rss>
