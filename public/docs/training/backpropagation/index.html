<!DOCTYPE html>
<html lang="en-us" dir="ltr">
<head><script src="/livereload.js?mindelay=10&amp;v=2&amp;port=1313&amp;path=livereload" data-no-instant defer></script>
  <meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1.0">
<meta name="description" content="
  Backpropagation
  #


Backpropagation, Error Backpropagation, Backprop.
Backpropagation (or backprop) is an efficient technique to compute the gradient of the loss function.
It boils down to a local message passing scheme in which information is sent backwards through the network.

  Forward propagation
  #





  





stateDiagram-v2
    direction LR
    z1: $$z_1$$
    z2: $$z_2$$
    zi: $$z_i$$
    zM: $$...$$
    aj: $$a_j=\sum_i w_{ji} z_i$$
    zj: $$z_j=h(a_j)$$
    START1:::hidden --&gt; z1
    START2:::hidden --&gt; z2
    STARTi:::hidden --&gt; zi
    STARTM:::hidden --&gt; zM
    z1 --&gt; aj
    z2 --&gt; aj
    zi --&gt; aj:$$w_{ji}$$
    zM --&gt; aj
    aj --&gt; zj
    zj --&gt; END:::hidden
    note left of aj : Pre-activation
    note left of zj : Activation
    classDef hidden display: none;


Let&rsquo;s consider a hidden unit in a general feed forward neural network.

  \[
a_j=\sum_i w_{ji} z_i
\]


where 
  \(z_i\)

 is the activation of another unit or an input that sends an connection of unit 
  \(j\)

 and 
  \(w_{ji}\)

 is the weight associated with that connection. The sum 
  \(a_j\)

 is known as pre-activation and is transformed by a non-linear activation function 
  \(h()\)

 to give the activation 
  \(z_j\)

 of unit 
  \(j\)

.

  \[
z_j=h(a_j)
\]


For any given data point in the training set, we can pass the input and compute the activations of all the hidden and output units. This process is called forward propagation since it is the forward flow of information through the network.">
<meta name="theme-color" media="(prefers-color-scheme: light)" content="#ffffff">
<meta name="theme-color" media="(prefers-color-scheme: dark)" content="#343a40">
<meta name="color-scheme" content="light dark"><meta property="og:url" content="http://localhost:1313/docs/training/backpropagation/">
  <meta property="og:site_name" content="Deep Learning">
  <meta property="og:title" content="Backpropagation">
  <meta property="og:description" content="Backpropagation # Backpropagation, Error Backpropagation, Backprop.
Backpropagation (or backprop) is an efficient technique to compute the gradient of the loss function.
It boils down to a local message passing scheme in which information is sent backwards through the network.
Forward propagation # stateDiagram-v2 direction LR z1: $$z_1$$ z2: $$z_2$$ zi: $$z_i$$ zM: $$...$$ aj: $$a_j=\sum_i w_{ji} z_i$$ zj: $$z_j=h(a_j)$$ START1:::hidden --&gt; z1 START2:::hidden --&gt; z2 STARTi:::hidden --&gt; zi STARTM:::hidden --&gt; zM z1 --&gt; aj z2 --&gt; aj zi --&gt; aj:$$w_{ji}$$ zM --&gt; aj aj --&gt; zj zj --&gt; END:::hidden note left of aj : Pre-activation note left of zj : Activation classDef hidden display: none; Letâ€™s consider a hidden unit in a general feed forward neural network. \[ a_j=\sum_i w_{ji} z_i \] where \(z_i\) is the activation of another unit or an input that sends an connection of unit \(j\) and \(w_{ji}\) is the weight associated with that connection. The sum \(a_j\) is known as pre-activation and is transformed by a non-linear activation function \(h()\) to give the activation \(z_j\) of unit \(j\) . \[ z_j=h(a_j) \] For any given data point in the training set, we can pass the input and compute the activations of all the hidden and output units. This process is called forward propagation since it is the forward flow of information through the network.">
  <meta property="og:locale" content="en_us">
  <meta property="og:type" content="article">
    <meta property="article:section" content="docs">
<title>Backpropagation | Deep Learning</title>
<link rel="icon" href="/favicon.png" >
<link rel="manifest" href="/manifest.json">
<link rel="canonical" href="http://localhost:1313/docs/training/backpropagation/">
<link rel="stylesheet" href="/book.min.6c8b9d2a1fc95075ed7da46ca81060b39add8fff6741ac51259f768929281e2c.css" integrity="sha256-bIudKh/JUHXtfaRsqBBgs5rdj/9nQaxRJZ92iSkoHiw=" crossorigin="anonymous">
  <script defer src="/fuse.min.js"></script>
  <script defer src="/en.search.min.fd3018753c6c93d586a3a76f8bd7ce57a76bc7e4a38e2b863f11580aa9b71f1a.js" integrity="sha256-/TAYdTxsk9WGo6dvi9fOV6drx&#43;SjjiuGPxFYCqm3Hxo=" crossorigin="anonymous"></script>
<!--
Made with Book Theme
https://github.com/alex-shpak/hugo-book
-->
  
</head>
<body dir="ltr">
  <input type="checkbox" class="hidden toggle" id="menu-control" />
  <input type="checkbox" class="hidden toggle" id="toc-control" />
  <main class="container flex">
    <aside class="book-menu">
      <div class="book-menu-content">
        
  <nav>
<h2 class="book-brand">
  <a class="flex align-center" href="/"><span>Deep Learning</span>
  </a>
</h2>


<div class="book-search hidden">
  <input type="text" id="book-search-input" placeholder="Search" aria-label="Search" maxlength="64" data-hotkeys="s/" />
  <div class="book-search-spinner hidden"></div>
  <ul id="book-search-results"></ul>
</div>
<script>document.querySelector(".book-search").classList.remove("hidden")</script>















  
  <ul>
    
      
        <li class="book-section-flat" >
          
  
  

  
    <a href="/docs/neural_networks/" class="">Neural Networks</a>
  

          
  <ul>
    
      
        <li>
          
  
  

  
    <a href="/docs/neural_networks/linear_regression/" class="">Linear Regression</a>
  

        </li>
      
    
      
        <li>
          
  
  

  
    <a href="/docs/neural_networks/logistic_regression/" class="">Logistic Regression</a>
  

        </li>
      
    
      
        <li>
          
  
  

  
    <a href="/docs/neural_networks/softmax_regression/" class="">Softmax Regression</a>
  

        </li>
      
    
      
        <li>
          
  
  

  
    <a href="/docs/neural_networks/mlp/" class="">Multilayer perceptron</a>
  

        </li>
      
    
      
        <li>
          
  
  

  
    <a href="/docs/neural_networks/activations/" class="">Activation functions</a>
  

        </li>
      
    
      
        <li>
          
  
  

  
    <a href="/docs/neural_networks/entropy/" class="">Entropy</a>
  

        </li>
      
    
  </ul>

        </li>
      
    
      
        <li class="book-section-flat" >
          
  
  

  
    <a href="/docs/training/" class="">Training neural networks</a>
  

          
  <ul>
    
      
        <li>
          
  
  

  
    <a href="/docs/training/gradient_descent/" class="">Gradient Descent</a>
  

        </li>
      
    
      
        <li>
          
  
  

  
    <a href="/docs/training/backpropagation/" class="active">Backpropagation</a>
  

        </li>
      
    
      
        <li>
          
  
  

  
    <a href="/docs/training/autograd/" class="">Automatic Differentiation</a>
  

        </li>
      
    
      
        <li>
          
  
  

  
    <a href="/docs/training/initialization/" class="">Initialization</a>
  

        </li>
      
    
      
        <li>
          
  
  

  
    <a href="/docs/training/normalization/" class="">Normalization</a>
  

        </li>
      
    
      
        <li>
          
  
  

  
    <a href="/docs/training/regularization/" class="">Regularization</a>
  

        </li>
      
    
      
        <li>
          
  
  

  
    <a href="/docs/training/training_loop/" class="">Training loop</a>
  

        </li>
      
    
      
        <li>
          
  
  

  
    <a href="/docs/training/quiz/" class="">Quiz</a>
  

        </li>
      
    
      
        <li>
          
  
  

  
    <a href="/docs/training/coding/" class="">Coding</a>
  

        </li>
      
    
  </ul>

        </li>
      
    
      
        <li class="book-section-flat" >
          
  
  

  
    <a href="/docs/transformers/" class="">Transformers</a>
  

          
  <ul>
    
      
        <li>
          
  
  

  
    <a href="/docs/transformers/transformers101/" class="">Transformers</a>
  

        </li>
      
    
      
        <li>
          
  
  

  
    <a href="/docs/transformers/alignment/" class="">Alignment</a>
  

        </li>
      
    
  </ul>

        </li>
      
    
      
        <li class="book-section-flat" >
          
  
  

  
    <a href="/docs/rl/" class="">Reinforcement Learning</a>
  

          
  <ul>
    
      
        <li>
          
  
  

  
    <a href="/docs/rl/basics/" class="">Basics</a>
  

        </li>
      
    
  </ul>

        </li>
      
    
  </ul>














</nav>




  <script>(function(){var e=document.querySelector("aside .book-menu-content");addEventListener("beforeunload",function(){localStorage.setItem("menu.scrollTop",e.scrollTop)}),e.scrollTop=localStorage.getItem("menu.scrollTop")})()</script>


 
      </div>
    </aside>

    <div class="book-page">
      <header class="book-header">
        
  <div class="flex align-center justify-between">
  <label for="menu-control">
    <img src="/svg/menu.svg" class="book-icon" alt="Menu" />
  </label>

  <h3>Backpropagation</h3>

  <label for="toc-control">
    
    <img src="/svg/toc.svg" class="book-icon" alt="Table of Contents" />
    
  </label>
</div>


  
  <aside class="hidden clearfix">
    
  
<nav id="TableOfContents">
  <ul>
    <li><a href="#forward-propagation">Forward propagation</a></li>
    <li><a href="#backward-propagation">Backward propagation</a></li>
    <li><a href="#algorithm">Algorithm</a></li>
  </ul>
</nav>



  </aside>
  
 
      </header>

      
      
  <article class="markdown book-article"><h1 id="backpropagation">
  Backpropagation
  <a class="anchor" href="#backpropagation">#</a>
</h1>
<blockquote>
<p>Backpropagation, Error Backpropagation, Backprop.</p></blockquote>
<p>Backpropagation (or backprop) is an efficient technique to compute the gradient of the loss function.</p>
<p>It boils down to a local message passing scheme in which information is sent backwards through the network.</p>
<h2 id="forward-propagation">
  Forward propagation
  <a class="anchor" href="#forward-propagation">#</a>
</h2>


<script src="/mermaid.min.js"></script>

  <script>mermaid.initialize({
  "flowchart": {
    "useMaxWidth":true
  },
  "theme": "default"
}
)</script>




<pre class="mermaid">
stateDiagram-v2
    direction LR
    z1: $$z_1$$
    z2: $$z_2$$
    zi: $$z_i$$
    zM: $$...$$
    aj: $$a_j=\sum_i w_{ji} z_i$$
    zj: $$z_j=h(a_j)$$
    START1:::hidden --> z1
    START2:::hidden --> z2
    STARTi:::hidden --> zi
    STARTM:::hidden --> zM
    z1 --> aj
    z2 --> aj
    zi --> aj:$$w_{ji}$$
    zM --> aj
    aj --> zj
    zj --> END:::hidden
    note left of aj : Pre-activation
    note left of zj : Activation
    classDef hidden display: none;
</pre>

<p>Let&rsquo;s consider a hidden unit in a general feed forward neural network.
<span>
  \[
a_j=\sum_i w_{ji} z_i
\]
</span>

where <span>
  \(z_i\)
</span>
 is the activation of another unit or an input that sends an connection of unit <span>
  \(j\)
</span>
 and <span>
  \(w_{ji}\)
</span>
 is the weight associated with that connection. The sum <span>
  \(a_j\)
</span>
 is known as <strong>pre-activation</strong> and is transformed by a non-linear activation function <span>
  \(h()\)
</span>
 to give the <strong>activation</strong> <span>
  \(z_j\)
</span>
 of unit <span>
  \(j\)
</span>
.
<span>
  \[
z_j=h(a_j)
\]
</span>

For any given data point in the training set, we can pass the input and compute the activations of all the hidden and output units. This process is called <strong>forward propagation</strong> since it is the forward flow of information through the network.</p>
<h2 id="backward-propagation">
  Backward propagation
  <a class="anchor" href="#backward-propagation">#</a>
</h2>
<p>In general most loss functions can be written as sum over each training instance.
<span>
  \[
L(\mathbf{w}) = \sum_{n=1}^{N} L_n(\mathbf{w})
\]
</span>

Hence we will consider evaluating the gradient of <span>
  \(L_n(\mathbf{w})\)
</span>
 with respect to the weight parameters <span>
  \(w_{ji}\)
</span>
. We will now use chain rule to derive the gradient of the loss function.
<span>
  \[
\frac{\partial L_n}{\partial w_{ji}} = \frac{\partial L_n}{\partial a_{j}} \frac{\partial a_j}{\partial w_{ji}} = \delta_j z_i
\]
</span>

where <span>
  \(\delta_j\)
</span>
 are referred to as <strong>errors</strong>
<span>
  \[
\frac{\partial L_n}{\partial a_{j}} := \delta_j
\]
</span>

and
<span>
  \[
\frac{\partial a_j}{\partial w_{ji}} = z_i
\]
</span>

So we now have
<span>
  \[
\frac{\partial L_n}{\partial w_{ji}} = \delta_j z_i
\]
</span>
</p>
<blockquote>
<p>The required derivative for <span>
  \(w_{ij}\)
</span>
 is simply obtained by multiplying the value of <span>
  \(\delta_j\)
</span>
 for the unit at the output end of the weight by the value of <span>
  \(z_i\)
</span>
 for the unit at the input end of the weight. This can be seen as a <strong>local computation</strong> involving the <strong>error signal</strong> at the output end with the <strong>activation signal</strong> at the input end.</p></blockquote>


<pre class="mermaid">
stateDiagram-v2
    direction LR
    zi: $$z_i$$
    zj: $$z_j$$
    zi --> zj:$$w_{ji}$$
    note left of zi : $$\delta_i$$
    note right of zj : $$\delta_j$$
</pre>

<p>So this now boils down to computing <span>
  \(\delta_j\)
</span>
  for all the hidden and the output units.
<span>
  \(\delta\)
</span>
 for the output units are based on the loss function. For example for the MSE loss
<span>
  \[
\delta_k = y_{nk} - t_{nk}
\]
</span>

To evaluate the <span>
  \(\delta\)
</span>
 for the hidden units we again make use of the the chain rule for partial derivatives.
<span>
  \[
\delta_j := \frac{\partial L_n}{\partial a_{j}} = \sum_{k} \frac{\partial L_n}{\partial a_{k}} \frac{\partial a_k}{\partial a_{j}}
\]
</span>

where the sum runs over all the units k to which j sends connections. This gives rise to the final <strong>backpropagation formula</strong>
<span>
  \[
\delta_j = h^{'}(a_j)\sum_{k} w_{kj} \delta_k
\]
</span>



<pre class="mermaid">
stateDiagram-v2
    direction LR
    zi: $$z_i$$
    zj: $$z_j$$
    z1: $$z_1$$
    zk: $$z_k$$
    zi --> zj:$$w_{ji}$$
    zj --> zk:$$w_{kj}$$
    zj --> z1:$$w_{1j}$$
    zk --> zj
    z1 --> zj
    note right of zj : $$\delta_j$$
    note right of zk : $$\delta_k$$
    note right of z1 : $$\delta_1$$
</pre>

This tells us that the value of <span>
  \(\delta\)
</span>
 for a particular hidden unit can be obtained by propagating the <span>
  \(\delta\)
</span>
 backward from units higher up in the network.</p>
<h2 id="algorithm">
  Algorithm
  <a class="anchor" href="#algorithm">#</a>
</h2>
<div class="book-columns flex flex-wrap">
<div class="flex-even markdown-inner" style="flex-grow: 1;">
<p><strong>Forward propagation</strong></p>
<p>For all hidden and output units compute in <strong>forward order</strong></p>
<link rel="stylesheet" href="/katex/katex.min.css" />
<script defer src="/katex/katex.min.js"></script>
<script defer src="/katex/auto-render.min.js" onload="renderMathInElement(document.body);"></script><span>
  \[
a_j \leftarrow \sum_i w_{ji} z_i
\]
</span>
<span>
  \[
z_j \leftarrow h(a_j)
\]
</span>
  </div>
<div class="flex-even markdown-inner" style="flex-grow: 1;">
<p><strong>Error evaluation</strong></p>
<p>For all output units compute</p>
<span>
  \[
\delta_k \leftarrow \frac{\partial L_n}{\partial a_k}
\]
</span>
  </div>
<div class="flex-even markdown-inner" style="flex-grow: 1;">
<p><strong>Backward propagation</strong></p>
<p>For all hidden units compute in <strong>reverse order</strong></p>
<span>
  \[
\delta_j \leftarrow h^{'}(a_j)\sum_{k} w_{kj} \delta_k
\]
</span>
<span>
  \[
\frac{\partial L_n}{\partial w_{ji}} \leftarrow \delta_j z_i
\]
</span>
  </div>
</div>


<pre class="mermaid">
stateDiagram-v2
    direction LR
    z1: $$z_1$$
    z2: $$z_2$$
    zi: $$z_i$$
    zM: $$...$$
    delta1: $$\delta_1$$
    delta2: $$\delta_2$$
    deltak: $$\delta_k$$
    deltaM: $$...$$
    aj: $$a_j$$
    zj: $$z_j$$
    START1:::hidden --> z1
    START2:::hidden --> z2
    STARTi:::hidden --> zi
    STARTM:::hidden --> zM
    z1 --> aj
    z2 --> aj
    zi --> aj:$$w_{ji}$$
    zM --> aj
    aj --> zj
    zj --> delta1
    zj --> delta2
    zj --> deltak:$$w_{kj}$$
    zj --> deltaM
    delta1 --> zj
    delta2 --> zj
    deltak --> zj
    deltaM --> zj
    delta1 --> START11:::hidden
    delta2 --> START22:::hidden
    deltak --> STARTii:::hidden
    deltaM --> STARTMM:::hidden
    note left of aj : Pre-activation
    note left of zj : Activation
    note right of deltak : Errors
    classDef hidden display: none;
</pre>

</article>
 
      

      <footer class="book-footer">
        
  <div class="flex flex-wrap justify-between">





</div>



  <script>(function(){function e(e){const t=window.getSelection(),n=document.createRange();n.selectNodeContents(e),t.removeAllRanges(),t.addRange(n)}document.querySelectorAll("pre code").forEach(t=>{t.addEventListener("click",function(){if(window.getSelection().toString())return;e(t.parentElement),navigator.clipboard&&navigator.clipboard.writeText(t.parentElement.textContent)})})})()</script>


 
        
      </footer>

      
  
  <div class="book-comments">

</div>
  
 

      <label for="menu-control" class="hidden book-menu-overlay"></label>
    </div>

    
    <aside class="book-toc">
      <div class="book-toc-content">
        
  
<nav id="TableOfContents">
  <ul>
    <li><a href="#forward-propagation">Forward propagation</a></li>
    <li><a href="#backward-propagation">Backward propagation</a></li>
    <li><a href="#algorithm">Algorithm</a></li>
  </ul>
</nav>


 
      </div>
    </aside>
    
  </main>

  
</body>
</html>












