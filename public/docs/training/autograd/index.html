<!DOCTYPE html>
<html lang="en-us" dir="ltr">
<head><script src="/livereload.js?mindelay=10&amp;v=2&amp;port=1313&amp;path=livereload" data-no-instant defer></script>
  <meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1.0">
<meta name="description" content="
  Automatic differentiation
  #


Algorithmic differentiation, autodiff, autograd
There are broadly 4 appoaches to compute derivatives.

  
      
          Approach
          Pros
          Cons
      
  
  
      
          Manual derivation of backprop equations.
          If done carefully can result in efficient code.
          Manual process, prone to errors and not easy to iterate on models
      
      
          Numerical evaluation of gradients via finite differences.
          Sometimes used to check for correctness of other methods.
          Limited by computational accuracy. Scales poorly with the size of the network.
      
      
          Symbolic differentiation using packages like sympy
          
          Closed form needed. Resulting expression can be very long (expression swell).
      
      
          Automatic differentiation
          Most preferred.
          
      
  


Atılım Günes Baydin, Barak A. Pearlmutter, Alexey Andreyevich Radul, and Jeffrey Mark Siskind. 2017. Automatic differentiation in machine learning: a survey. J. Mach. Learn. Res. 18, 1 (January 2017), 5595–5637.">
<meta name="theme-color" media="(prefers-color-scheme: light)" content="#ffffff">
<meta name="theme-color" media="(prefers-color-scheme: dark)" content="#343a40">
<meta name="color-scheme" content="light dark"><meta property="og:url" content="http://localhost:1313/docs/training/autograd/">
  <meta property="og:site_name" content="Deep Learning">
  <meta property="og:title" content="AutoDiff">
  <meta property="og:description" content="Automatic differentiation # Algorithmic differentiation, autodiff, autograd
There are broadly 4 appoaches to compute derivatives.
Approach Pros Cons Manual derivation of backprop equations. If done carefully can result in efficient code. Manual process, prone to errors and not easy to iterate on models Numerical evaluation of gradients via finite differences. Sometimes used to check for correctness of other methods. Limited by computational accuracy. Scales poorly with the size of the network. Symbolic differentiation using packages like sympy Closed form needed. Resulting expression can be very long (expression swell). Automatic differentiation Most preferred. Atılım Günes Baydin, Barak A. Pearlmutter, Alexey Andreyevich Radul, and Jeffrey Mark Siskind. 2017. Automatic differentiation in machine learning: a survey. J. Mach. Learn. Res. 18, 1 (January 2017), 5595–5637.">
  <meta property="og:locale" content="en_us">
  <meta property="og:type" content="article">
    <meta property="article:section" content="docs">
<title>AutoDiff | Deep Learning</title>
<link rel="icon" href="/favicon.png" >
<link rel="manifest" href="/manifest.json">
<link rel="canonical" href="http://localhost:1313/docs/training/autograd/">
<link rel="stylesheet" href="/book.min.6c8b9d2a1fc95075ed7da46ca81060b39add8fff6741ac51259f768929281e2c.css" integrity="sha256-bIudKh/JUHXtfaRsqBBgs5rdj/9nQaxRJZ92iSkoHiw=" crossorigin="anonymous">
  <script defer src="/fuse.min.js"></script>
  <script defer src="/en.search.min.806346de3eb16675e860bc9156ff04165ae8983979413477e43e4270df22612a.js" integrity="sha256-gGNG3j6xZnXoYLyRVv8EFlromDl5QTR35D5CcN8iYSo=" crossorigin="anonymous"></script>
<!--
Made with Book Theme
https://github.com/alex-shpak/hugo-book
-->
  
</head>
<body dir="ltr">
  <input type="checkbox" class="hidden toggle" id="menu-control" />
  <input type="checkbox" class="hidden toggle" id="toc-control" />
  <main class="container flex">
    <aside class="book-menu">
      <div class="book-menu-content">
        
  <nav>
<h2 class="book-brand">
  <a class="flex align-center" href="/"><span>Deep Learning</span>
  </a>
</h2>


<div class="book-search hidden">
  <input type="text" id="book-search-input" placeholder="Search" aria-label="Search" maxlength="64" data-hotkeys="s/" />
  <div class="book-search-spinner hidden"></div>
  <ul id="book-search-results"></ul>
</div>
<script>document.querySelector(".book-search").classList.remove("hidden")</script>















  
  <ul>
    
      
        <li class="book-section-flat" >
          
  
  

  
    <a href="/docs/basics/" class="">Basics</a>
  

          
  <ul>
    
      
        <li>
          
  
  

  
    <a href="/docs/basics/entropy/" class="">Entropy</a>
  

        </li>
      
    
      
        <li>
          
  
  

  
    <a href="/docs/basics/gpu/" class="">GPU primer</a>
  

        </li>
      
    
      
        <li>
          
  
  

  
    <a href="/docs/basics/metrics/" class="">Metrics</a>
  

        </li>
      
    
  </ul>

        </li>
      
    
      
        <li class="book-section-flat" >
          
  
  

  
    <a href="/docs/neural_networks/" class="">Neural Networks</a>
  

          
  <ul>
    
      
        <li>
          
  
  

  
    <a href="/docs/neural_networks/linear_regression/" class="">Linear Regression</a>
  

        </li>
      
    
      
        <li>
          
  
  

  
    <a href="/docs/neural_networks/logistic_regression/" class="">Logistic Regression</a>
  

        </li>
      
    
      
        <li>
          
  
  

  
    <a href="/docs/neural_networks/softmax_regression/" class="">Softmax Regression</a>
  

        </li>
      
    
      
        <li>
          
  
  

  
    <a href="/docs/neural_networks/mlp/" class="">Multilayer perceptron</a>
  

        </li>
      
    
      
        <li>
          
  
  

  
    <a href="/docs/neural_networks/activations/" class="">Activation functions</a>
  

        </li>
      
    
  </ul>

        </li>
      
    
      
        <li class="book-section-flat" >
          
  
  

  
    <a href="/docs/training/" class="">Training</a>
  

          
  <ul>
    
      
        <li>
          
  
  

  
    <a href="/docs/training/gradient_descent/" class="">Gradient Descent</a>
  

        </li>
      
    
      
        <li>
          
  
  

  
    <a href="/docs/training/backpropagation/" class="">Backpropagation</a>
  

        </li>
      
    
      
        <li>
          
  
  

  
    <a href="/docs/training/autograd/" class="active">AutoDiff</a>
  

        </li>
      
    
      
        <li>
          
  
  

  
    <a href="/docs/training/initialization/" class="">Initialization</a>
  

        </li>
      
    
      
        <li>
          
  
  

  
    <a href="/docs/training/normalization/" class="">Normalization</a>
  

        </li>
      
    
      
        <li>
          
  
  

  
    <a href="/docs/training/regularization/" class="">Regularization</a>
  

        </li>
      
    
      
        <li>
          
  
  

  
    <a href="/docs/training/training_loop/" class="">Training loop</a>
  

        </li>
      
    
      
        <li>
          
  
  

  
    <a href="/docs/training/quiz/" class="">Quiz</a>
  

        </li>
      
    
      
        <li>
          
  
  

  
    <a href="/docs/training/coding/" class="">Coding</a>
  

        </li>
      
    
  </ul>

        </li>
      
    
      
        <li class="book-section-flat" >
          
  
  

  
    <a href="/docs/transformers/" class="">Transformers</a>
  

          
  <ul>
    
      
        <li>
          
  
  

  
    <a href="/docs/transformers/tokenizers/" class="">Tokenizers</a>
  

        </li>
      
    
      
        <li>
          
  
  

  
    <a href="/docs/transformers/transformers101/" class="">Transformers101</a>
  

        </li>
      
    
      
        <li>
          
  
  

  
    <a href="/docs/transformers/transformers102/" class="">Transformers102</a>
  

        </li>
      
    
      
        <li>
          
  
  

  
    <a href="/docs/transformers/alignment/" class="">Alignment</a>
  

        </li>
      
    
      
        <li>
          
  
  

  
    <a href="/docs/transformers/attentionvariants/" class="">Attention variants</a>
  

        </li>
      
    
      
        <li>
          
  
  

  
    <a href="/docs/transformers/models/" class="">Frontier models</a>
  

        </li>
      
    
      
        <li>
          
  
  

  
    <a href="/docs/transformers/moe/" class="">MoE</a>
  

        </li>
      
    
      
        <li>
          
  
  

  
    <a href="/docs/transformers/scaling/" class="">Scaling</a>
  

        </li>
      
    
      
        <li>
          
  
  

  
    <a href="/docs/transformers/ssm/" class="">SSM</a>
  

        </li>
      
    
      
        <li>
          
  
  

  
    <a href="/docs/transformers/datasets/" class="">Datasets</a>
  

        </li>
      
    
  </ul>

        </li>
      
    
      
        <li class="book-section-flat" >
          
  
  

  
    <a href="/docs/rl/" class="">Reinforcement</a>
  

          
  <ul>
    
      
        <li>
          
  
  

  
    <a href="/docs/rl/basics/" class="">Basics</a>
  

        </li>
      
    
  </ul>

        </li>
      
    
  </ul>














</nav>




  <script>(function(){var e=document.querySelector("aside .book-menu-content");addEventListener("beforeunload",function(){localStorage.setItem("menu.scrollTop",e.scrollTop)}),e.scrollTop=localStorage.getItem("menu.scrollTop")})()</script>


 
      </div>
    </aside>

    <div class="book-page">
      <header class="book-header">
        
  <div class="flex align-center justify-between">
  <label for="menu-control">
    <img src="/svg/menu.svg" class="book-icon" alt="Menu" />
  </label>

  <h3>AutoDiff</h3>

  <label for="toc-control">
    
    <img src="/svg/toc.svg" class="book-icon" alt="Table of Contents" />
    
  </label>
</div>


  
  <aside class="hidden clearfix">
    
  
<nav id="TableOfContents">
  <ul>
    <li><a href="#forward-mode-automatic-differentiation">Forward-mode automatic differentiation</a></li>
    <li><a href="#reverse-mode-automatic-differentiation">Reverse-mode automatic differentiation</a></li>
    <li><a href="#autograd-in-pytorch">Autograd in pytorch</a></li>
  </ul>
</nav>



  </aside>
  
 
      </header>

      
      
  <article class="markdown book-article"><h1 id="automatic-differentiation">
  Automatic differentiation
  <a class="anchor" href="#automatic-differentiation">#</a>
</h1>
<blockquote>
<p>Algorithmic differentiation, autodiff, autograd</p></blockquote>
<p>There are broadly 4 appoaches to compute derivatives.</p>
<table>
  <thead>
      <tr>
          <th>Approach</th>
          <th>Pros</th>
          <th>Cons</th>
      </tr>
  </thead>
  <tbody>
      <tr>
          <td><strong>Manual</strong> derivation of backprop equations.</td>
          <td>If done carefully can result in efficient code.</td>
          <td>Manual process, prone to errors and not easy to iterate on models</td>
      </tr>
      <tr>
          <td><strong>Numerical</strong> evaluation of gradients via finite differences.</td>
          <td>Sometimes used to check for correctness of other methods.</td>
          <td>Limited by computational accuracy. Scales poorly with the size of the network.</td>
      </tr>
      <tr>
          <td><strong>Symbolic</strong> differentiation using packages like <code>sympy</code></td>
          <td></td>
          <td>Closed form needed. Resulting expression can be very long (<em>expression swell</em>).</td>
      </tr>
      <tr>
          <td><strong>Automatic differentiation</strong></td>
          <td>Most preferred.</td>
          <td></td>
      </tr>
  </tbody>
</table>
<blockquote class="book-hint warning">
<p>Atılım Günes Baydin, Barak A. Pearlmutter, Alexey Andreyevich Radul, and Jeffrey Mark Siskind. 2017. <a href="https://dl.acm.org/doi/pdf/10.5555/3122009.3242010">Automatic differentiation in machine learning: a survey.</a> J. Mach. Learn. Res. 18, 1 (January 2017), 5595–5637.</p>
</blockquote>
<h2 id="forward-mode-automatic-differentiation">
  Forward-mode automatic differentiation
  <a class="anchor" href="#forward-mode-automatic-differentiation">#</a>
</h2>
<blockquote>
<p>We augment each intermediate variable 
<link rel="stylesheet" href="/katex/katex.min.css" />
<script defer src="/katex/katex.min.js"></script>
<script defer src="/katex/auto-render.min.js" onload="renderMathInElement(document.body);"></script><span>
  \(z_i\)
</span>
 (known as <strong>primal</strong> variable) with an additional variable representing the value of some derivative of that variable, which we denote as <span>
  \(\dot{z}_i\)
</span>
, known as <strong>tangent</strong> variable. The tangent variables are generated automatically.</p></blockquote>
<p>Consider the following function.
<span>
  \[
f(x_1,x_2) = x_1x_2 + \exp(x_1x_2) - \sin(x_2)
\]
</span>

When implemented in software the code consists of a sequence of operations than can be expressed as an <strong>evaluation trace</strong> of the underlying elementary operations. This trace can be visualized as a computation graph with respect to the following 7 primal variables.


<script src="/mermaid.min.js"></script>

  <script>mermaid.initialize({
  "flowchart": {
    "useMaxWidth":true
  },
  "theme": "default"
}
)</script>




<pre class="mermaid">
stateDiagram-v2
    direction LR
    x1: $$x_1$$
    x2: $$x_2$$
    v1: $$v_1 = x_1$$
    v2: $$v_2 = x_2$$
    v3: $$v_3 = v_1v_2$$
    v4: $$v_4 = \sin(v_2)$$
    v5: $$v_5 = \exp(v_3)$$
    v6: $$v_6 = v_3 - v_4$$
    v7: $$v_7 = v_5 + v_6$$
    f: $$f = v_5 + v_6$$
    x1 --> v1
    x2 --> v2
    v1 --> v3
    v2 --> v4
    v2 --> v3
    v3 --> v5
    v4 --> v6
    v3 --> v6
    v5 --> v7
    v6 --> v7
    v7 --> f
</pre>

We first write code to implement the evaluation of the primal variables.
<span>
  \[
v_1 = x_1
\]
</span>

<span>
  \[
v_2 = x_2
\]
</span>

<span>
  \[
v_3 = v_1v_2
\]
</span>

<span>
  \[
v_4 = \sin(v_2)
\]
</span>

<span>
  \[
v_5 = \exp(v_3)
\]
</span>

<span>
  \[
v_6 = v_3 - v_4
\]
</span>

<span>
  \[
v_7 = v_5 + v_6
\]
</span>

Now say we wish to evaluate the derivative <span>
  \(\partial f/\partial x_1\)
</span>
. First we define the tangent variables by
<span>
  \[\dot{v}_i = \frac{\partial v_i}{\partial x_1}\]
</span>

Expressions for evaluating these can be constructed automatically using the chain rule of calculus.
<span>
  \[
\dot{v}_i = \frac{\partial v_i}{\partial x_1} = \sum_{j\in\text{parents}(i)} \frac{\partial v_i}{\partial v_j} \frac{\partial v_j}{\partial x_1} = \sum_{j\in\text{parents}(i)} \dot{v}_j \frac{\partial v_i}{\partial v_j}
\]
</span>

where <span>
  \(\text{parents}(i)\)
</span>
 denotes the set of <strong>parents</strong> of node <span>
  \(i\)
</span>
 in the evaluation trace diagram.</p>
<p>The associated equations and corresponding code for evaluating the tangent variables are generated automatically.
<span>
  \[
\dot{v}_1 = 1
\]
</span>

<span>
  \[
\dot{v}_2 = 0
\]
</span>

<span>
  \[
\dot{v}_3 = v_1\dot{v}_2+\dot{v}_1v_2
\]
</span>

<span>
  \[
\dot{v}_4 = \dot{v}_2\cos(v_2)
\]
</span>

<span>
  \[
\dot{v}_5 = \dot{v}_3\exp(v_3)
\]
</span>

<span>
  \[
\dot{v}_6 = \dot{v}_3 - \dot{v}_4
\]
</span>

<span>
  \[
\dot{v}_7 = \dot{v}_5 + \dot{v}_6
\]
</span>
</p>
<p>To evaluate the derivative <span>
  \(\frac{\partial f}{\partial x_1}\)
</span>
 we input specific values of <span>
  \(x_1\)
</span>
 and <span>
  \(x_2\)
</span>
 and the code then executes the primal and tangent equations, numerically evaluating the tuples <span>
  \((v_i,\dot{v}_i)\)
</span>
 in <strong>forward</strong> order until we obtain the required derivative.</p>
<blockquote class="book-hint danger">
<p>The forward mode with slight modifications can handle multiple outputs in the same pass but the process has to be repeated for every parameter that we need the derivative. Since we are often in the regime of one output with millions of parameters this is not scalable for modern deep neural networks. We therefore turn to an alternative version based on the backwards flow of derivative data through the evaluation trace graph.</p>
</blockquote>
<h2 id="reverse-mode-automatic-differentiation">
  Reverse-mode automatic differentiation
  <a class="anchor" href="#reverse-mode-automatic-differentiation">#</a>
</h2>
<p>Reverse-mode automatic differentiation is a generalization of the error backpropagation procedure we discussed earlier.</p>
<p>As with forward mode, we augment each primal variable <span>
  \(v_i\)
</span>
 with an additional variable called <strong>adjoint</strong> variable, denoted as <span>
  \(\bar{v}_i\)
</span>
.
<span>
  \[\bar{v}_i = \frac{\partial f}{\partial v_i}\]
</span>

Expressions for evaluating these can be constructed automatically using the chain rule of calculus.
<span>
  \[
\bar{v}_i = \frac{\partial f}{\partial v_i} = \sum_{j\in\text{children}(i)} \frac{\partial f}{\partial v_j} \frac{\partial v_j}{\partial v_i} = \sum_{j\in\text{children}(i)} \bar{v}_j \frac{\partial v_j}{\partial v_i}
\]
</span>

where <span>
  \(\text{children}(i)\)
</span>
 denotes the set of <strong>children</strong> of node i in the evaluation trace diagram.</p>
<blockquote>
<p>The successive evaluation of the adjoint variables represents a flow of information backwards through the graph. For multiple parameters a single backward pass is enough. Reverse mode is more memory intensive than forward mode.</p></blockquote>
<p><span>
  \[
\bar{v}_7 = 1
\]
</span>

<span>
  \[
\bar{v}_6 = \bar{v}_7
\]
</span>

<span>
  \[
\bar{v}_5 = \bar{v}_7
\]
</span>

<span>
  \[
\bar{v}_4 = -\bar{v}_6
\]
</span>

<span>
  \[
\bar{v}_3 = \bar{v}_5v_5+\bar{v}_6
\]
</span>

<span>
  \[
\bar{v}_2 = \bar{v}_2v_1+\bar{v}_4\cos(v_2)
\]
</span>

<span>
  \[
\bar{v}_1 = \bar{v}_3v_2
\]
</span>
</p>
<h2 id="autograd-in-pytorch">
  Autograd in pytorch
  <a class="anchor" href="#autograd-in-pytorch">#</a>
</h2>
<ul>
<li><a href="https://pytorch.org/tutorials/beginner/blitz/autograd_tutorial.html">A Gentle Introduction to <code>torch.autograd</code></a></li>
<li><a href="https://pytorch.org/tutorials/beginner/introyt/autogradyt_tutorial.html">The Fundamentals of Autograd</a></li>
</ul>
</article>
 
      

      <footer class="book-footer">
        
  <div class="flex flex-wrap justify-between">





</div>



  <script>(function(){function e(e){const t=window.getSelection(),n=document.createRange();n.selectNodeContents(e),t.removeAllRanges(),t.addRange(n)}document.querySelectorAll("pre code").forEach(t=>{t.addEventListener("click",function(){if(window.getSelection().toString())return;e(t.parentElement),navigator.clipboard&&navigator.clipboard.writeText(t.parentElement.textContent)})})})()</script>


 
        
      </footer>

      
  
  <div class="book-comments">

</div>
  
 

      <label for="menu-control" class="hidden book-menu-overlay"></label>
    </div>

    
    <aside class="book-toc">
      <div class="book-toc-content">
        
  
<nav id="TableOfContents">
  <ul>
    <li><a href="#forward-mode-automatic-differentiation">Forward-mode automatic differentiation</a></li>
    <li><a href="#reverse-mode-automatic-differentiation">Reverse-mode automatic differentiation</a></li>
    <li><a href="#autograd-in-pytorch">Autograd in pytorch</a></li>
  </ul>
</nav>


 
      </div>
    </aside>
    
  </main>

  
</body>
</html>












