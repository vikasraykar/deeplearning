<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Transformers on Deep Learning</title>
    <link>http://localhost:1313/docs/transformers/</link>
    <description>Recent content in Transformers on Deep Learning</description>
    <generator>Hugo</generator>
    <language>en-us</language>
    <atom:link href="http://localhost:1313/docs/transformers/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>Transformers101</title>
      <link>http://localhost:1313/docs/transformers/transformers101/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>http://localhost:1313/docs/transformers/transformers101/</guid>
      <description>&lt;h1 id=&#34;transformers-101&#34;&gt;&#xA;  Transformers 101&#xA;  &lt;a class=&#34;anchor&#34; href=&#34;#transformers-101&#34;&gt;#&lt;/a&gt;&#xA;&lt;/h1&gt;&#xA;&lt;p&gt;A transformer has 3 main components.&lt;/p&gt;&#xA;&lt;ol&gt;&#xA;&lt;li&gt;Multi-head scaled self-attention.&lt;/li&gt;&#xA;&lt;li&gt;MLP with residual connections and layer normalization.&lt;/li&gt;&#xA;&lt;li&gt;Positional encodings.&lt;/li&gt;&#xA;&lt;/ol&gt;&#xA;&lt;h2 id=&#34;tldr&#34;&gt;&#xA;  TLDR&#xA;  &lt;a class=&#34;anchor&#34; href=&#34;#tldr&#34;&gt;#&lt;/a&gt;&#xA;&lt;/h2&gt;&#xA;&lt;table&gt;&#xA;  &lt;thead&gt;&#xA;      &lt;tr&gt;&#xA;          &lt;th style=&#34;text-align: left&#34;&gt;Transformer&lt;/th&gt;&#xA;          &lt;th style=&#34;text-align: left&#34;&gt;Description&lt;/th&gt;&#xA;      &lt;/tr&gt;&#xA;  &lt;/thead&gt;&#xA;  &lt;tbody&gt;&#xA;      &lt;tr&gt;&#xA;          &lt;td style=&#34;text-align: left&#34;&gt;&#xA;&lt;link rel=&#34;stylesheet&#34; href=&#34;http://localhost:1313/katex/katex.min.css&#34; /&gt;&#xA;&lt;script defer src=&#34;http://localhost:1313/katex/katex.min.js&#34;&gt;&lt;/script&gt;&#xA;&lt;script defer src=&#34;http://localhost:1313/katex/auto-render.min.js&#34; onload=&#34;renderMathInElement(document.body);&#34;&gt;&lt;/script&gt;&lt;span&gt;&#xA;  \(\mathbf{X}\)&#xA;&lt;/span&gt;&#xA;&lt;/td&gt;&#xA;          &lt;td style=&#34;text-align: left&#34;&gt;embedding matrix&lt;/td&gt;&#xA;      &lt;/tr&gt;&#xA;      &lt;tr&gt;&#xA;          &lt;td style=&#34;text-align: left&#34;&gt;&lt;span&gt;&#xA;  \(\mathbf{X} = \mathbf{X} + \mathbf{R}\)&#xA;&lt;/span&gt;&#xA;&lt;/td&gt;&#xA;          &lt;td style=&#34;text-align: left&#34;&gt;position encoding matrix&lt;/td&gt;&#xA;      &lt;/tr&gt;&#xA;      &lt;tr&gt;&#xA;          &lt;td style=&#34;text-align: left&#34;&gt;&lt;span&gt;&#xA;  \(\mathbf{Y} = \text{TransformerLayer}[\mathbf{X}]\)&#xA;&lt;/span&gt;&#xA;&lt;/td&gt;&#xA;          &lt;td style=&#34;text-align: left&#34;&gt;transformer&lt;/td&gt;&#xA;      &lt;/tr&gt;&#xA;      &lt;tr&gt;&#xA;          &lt;td style=&#34;text-align: left&#34;&gt;&lt;span&gt;&#xA;  \(\mathbf{Y} = \text{SoftMax}[\mathbf{X}\mathbf{X}^{T}] \mathbf{X}\)&#xA;&lt;/span&gt;&#xA;&lt;/td&gt;&#xA;          &lt;td style=&#34;text-align: left&#34;&gt;dot-product self-attention&lt;/td&gt;&#xA;      &lt;/tr&gt;&#xA;      &lt;tr&gt;&#xA;          &lt;td style=&#34;text-align: left&#34;&gt;&lt;span&gt;&#xA;  \(\mathbf{Y} = \text{SoftMax}[\mathbf{X} \mathbf{W}^{q}\mathbf{W}^{kT}\mathbf{X}^{T}] \mathbf{X} \mathbf{W}^{v}\)&#xA;&lt;/span&gt;&#xA;&lt;/td&gt;&#xA;          &lt;td style=&#34;text-align: left&#34;&gt;Query, Key, Value matrices&lt;/td&gt;&#xA;      &lt;/tr&gt;&#xA;      &lt;tr&gt;&#xA;          &lt;td style=&#34;text-align: left&#34;&gt;&lt;span&gt;&#xA;  \(\mathbf{Y} = \text{SoftMax}[\mathbf{Q}\mathbf{K}^{T}] \mathbf{V} \)&#xA;&lt;/span&gt;&#xA;&lt;/td&gt;&#xA;          &lt;td style=&#34;text-align: left&#34;&gt;Query, Key, Value matrices&lt;/td&gt;&#xA;      &lt;/tr&gt;&#xA;      &lt;tr&gt;&#xA;          &lt;td style=&#34;text-align: left&#34;&gt;&lt;span&gt;&#xA;  \(\mathbf{Y} = \text{SoftMax}[\frac{\mathbf{Q}\mathbf{K}^{T}}{\sqrt{D}}] \mathbf{V} \)&#xA;&lt;/span&gt;&#xA;&lt;/td&gt;&#xA;          &lt;td style=&#34;text-align: left&#34;&gt;Scaled dot-product self attention&lt;/td&gt;&#xA;      &lt;/tr&gt;&#xA;      &lt;tr&gt;&#xA;          &lt;td style=&#34;text-align: left&#34;&gt;&lt;span&gt;&#xA;  \(\mathbf{Y} = \text{Concat}[\mathbf{H}_1,...,\mathbf{H}_H]\mathbf{W}^o \)&#xA;&lt;/span&gt;&#xA; where &lt;span&gt;&#xA;  \(\mathbf{H}_h = \text{SoftMax}\left[\frac{\mathbf{Q_h}\mathbf{K_h}^{T}}{\sqrt{D_k}}\right] \mathbf{V_h}\)&#xA;&lt;/span&gt;&#xA;&lt;/td&gt;&#xA;          &lt;td style=&#34;text-align: left&#34;&gt;Multi-head attention&lt;/td&gt;&#xA;      &lt;/tr&gt;&#xA;      &lt;tr&gt;&#xA;          &lt;td style=&#34;text-align: left&#34;&gt;&lt;span&gt;&#xA;  \(\mathbf{Z} = \text{LayerNorm}\left[\mathbf{Y}(\mathbf{X})+\mathbf{X}\right]\)&#xA;&lt;/span&gt;&#xA;&lt;/td&gt;&#xA;          &lt;td style=&#34;text-align: left&#34;&gt;layer normalization and residual connection&lt;/td&gt;&#xA;      &lt;/tr&gt;&#xA;      &lt;tr&gt;&#xA;          &lt;td style=&#34;text-align: left&#34;&gt;&lt;span&gt;&#xA;  \(\mathbf{X^*} = \text{LayerNorm}\left[\text{MLP}(\mathbf{Z})+\mathbf{Z}\right]\)&#xA;&lt;/span&gt;&#xA;&lt;/td&gt;&#xA;          &lt;td style=&#34;text-align: left&#34;&gt;MLP layer&lt;/td&gt;&#xA;      &lt;/tr&gt;&#xA;  &lt;/tbody&gt;&#xA;&lt;/table&gt;&#xA;&lt;table&gt;&#xA;  &lt;thead&gt;&#xA;      &lt;tr&gt;&#xA;          &lt;th style=&#34;text-align: left&#34;&gt;Parameter&lt;/th&gt;&#xA;          &lt;th style=&#34;text-align: left&#34;&gt;Count&lt;/th&gt;&#xA;          &lt;th style=&#34;text-align: left&#34;&gt;Description&lt;/th&gt;&#xA;      &lt;/tr&gt;&#xA;  &lt;/thead&gt;&#xA;  &lt;tbody&gt;&#xA;      &lt;tr&gt;&#xA;          &lt;td style=&#34;text-align: left&#34;&gt;&lt;span&gt;&#xA;  \(\mathbf{E}\)&#xA;&lt;/span&gt;&#xA;&lt;/td&gt;&#xA;          &lt;td style=&#34;text-align: left&#34;&gt;&lt;span&gt;&#xA;  \(VD\)&#xA;&lt;/span&gt;&#xA;&lt;/td&gt;&#xA;          &lt;td style=&#34;text-align: left&#34;&gt;The token embedding matrix. &lt;span&gt;&#xA;  \(V\)&#xA;&lt;/span&gt;&#xA; is the size of the vocabulary and &lt;span&gt;&#xA;  \(D\)&#xA;&lt;/span&gt;&#xA; is the dimensions of the embeddings.&lt;/td&gt;&#xA;      &lt;/tr&gt;&#xA;      &lt;tr&gt;&#xA;          &lt;td style=&#34;text-align: left&#34;&gt;&lt;span&gt;&#xA;  \(\mathbf{W}^q_h\)&#xA;&lt;/span&gt;&#xA; &lt;span&gt;&#xA;  \(\mathbf{W}^k_h\)&#xA;&lt;/span&gt;&#xA; &lt;span&gt;&#xA;  \(\mathbf{W}^v_h\)&#xA;&lt;/span&gt;&#xA;&lt;/td&gt;&#xA;          &lt;td style=&#34;text-align: left&#34;&gt;&lt;span&gt;&#xA;  \(3HD^2\)&#xA;&lt;/span&gt;&#xA;&lt;/td&gt;&#xA;          &lt;td style=&#34;text-align: left&#34;&gt;The query, key and the value matrices each of dimension &lt;span&gt;&#xA;  \(D \times D \)&#xA;&lt;/span&gt;&#xA;for the &lt;span&gt;&#xA;  \(H\)&#xA;&lt;/span&gt;&#xA;heads.&lt;/td&gt;&#xA;      &lt;/tr&gt;&#xA;      &lt;tr&gt;&#xA;          &lt;td style=&#34;text-align: left&#34;&gt;&lt;span&gt;&#xA;  \(\mathbf{W}^o\)&#xA;&lt;/span&gt;&#xA;&lt;/td&gt;&#xA;          &lt;td style=&#34;text-align: left&#34;&gt;&lt;span&gt;&#xA;  \(HD^2\)&#xA;&lt;/span&gt;&#xA;&lt;/td&gt;&#xA;          &lt;td style=&#34;text-align: left&#34;&gt;The output matrix of dimension &lt;span&gt;&#xA;  \(HD \times D \)&#xA;&lt;/span&gt;&#xA;.&lt;/td&gt;&#xA;      &lt;/tr&gt;&#xA;      &lt;tr&gt;&#xA;          &lt;td style=&#34;text-align: left&#34;&gt;&lt;span&gt;&#xA;  \(\mathbf{W}^{ff}_{1}\)&#xA;&lt;/span&gt;&#xA; &lt;span&gt;&#xA;  \(\mathbf{W}^{ff}_{2}\)&#xA;&lt;/span&gt;&#xA;&lt;/td&gt;&#xA;          &lt;td style=&#34;text-align: left&#34;&gt;&lt;span&gt;&#xA;  \(2DD_{ff}\)&#xA;&lt;/span&gt;&#xA;&lt;/td&gt;&#xA;          &lt;td style=&#34;text-align: left&#34;&gt;The parameters of the two-layer MLP.&lt;/td&gt;&#xA;      &lt;/tr&gt;&#xA;      &lt;tr&gt;&#xA;          &lt;td style=&#34;text-align: left&#34;&gt;&lt;/td&gt;&#xA;          &lt;td style=&#34;text-align: left&#34;&gt;&lt;span&gt;&#xA;  \(8D^2\)&#xA;&lt;/span&gt;&#xA;&lt;/td&gt;&#xA;          &lt;td style=&#34;text-align: left&#34;&gt;Typically &lt;span&gt;&#xA;  \(D_{ff} = 4 D\)&#xA;&lt;/span&gt;&#xA;&lt;/td&gt;&#xA;      &lt;/tr&gt;&#xA;      &lt;tr&gt;&#xA;          &lt;td style=&#34;text-align: left&#34;&gt;&lt;/td&gt;&#xA;          &lt;td style=&#34;text-align: left&#34;&gt;&lt;/td&gt;&#xA;          &lt;td style=&#34;text-align: left&#34;&gt;&lt;strong&gt;&lt;span&gt;&#xA;  \((4H+8)D^2\)&#xA;&lt;/span&gt;&#xA; total parameters&lt;/strong&gt;&lt;/td&gt;&#xA;      &lt;/tr&gt;&#xA;  &lt;/tbody&gt;&#xA;&lt;/table&gt;&#xA;&lt;h2 id=&#34;multi-head-scaled-self-attention&#34;&gt;&#xA;  Multi-head scaled self-attention.&#xA;  &lt;a class=&#34;anchor&#34; href=&#34;#multi-head-scaled-self-attention&#34;&gt;#&lt;/a&gt;&#xA;&lt;/h2&gt;&#xA;&lt;h3 id=&#34;tokens&#34;&gt;&#xA;  Tokens&#xA;  &lt;a class=&#34;anchor&#34; href=&#34;#tokens&#34;&gt;#&lt;/a&gt;&#xA;&lt;/h3&gt;&#xA;&lt;p&gt;We will start with the concept of &lt;strong&gt;tokens&lt;/strong&gt;. As token can be&lt;/p&gt;</description>
    </item>
    <item>
      <title>Tokenizers</title>
      <link>http://localhost:1313/docs/transformers/tokenizers/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>http://localhost:1313/docs/transformers/tokenizers/</guid>
      <description>&lt;p&gt;Tokenization pipeline&lt;/p&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;Normalization&lt;/li&gt;&#xA;&lt;li&gt;Pre-tokenization&lt;/li&gt;&#xA;&lt;li&gt;Model&lt;/li&gt;&#xA;&lt;li&gt;Post-processor&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;h2 id=&#34;sentencepiece&#34;&gt;&#xA;  SentencePiece&#xA;  &lt;a class=&#34;anchor&#34; href=&#34;#sentencepiece&#34;&gt;#&lt;/a&gt;&#xA;&lt;/h2&gt;&#xA;&lt;p&gt;&lt;a href=&#34;https://github.com/google/sentencepiece&#34;&gt;https://github.com/google/sentencepiece&lt;/a&gt;&lt;/p&gt;&#xA;&lt;p&gt;SentencePiece is a tokenization algorithm for the preprocessing of text that you can use with either BPE, WordPiece, or Unigram model.&lt;/p&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;It considers the text as a sequence of Unicode characters, and replaces spaces with a special character, &lt;code&gt;▁&lt;/code&gt;.&lt;/li&gt;&#xA;&lt;li&gt;Used in conjunction with the Unigram algorithm, it doesn’t require a pre-tokenization step, which is very useful for languages where the space character is not used (like Chinese or Japanese).&lt;/li&gt;&#xA;&lt;li&gt;SentencePiece is &lt;strong&gt;reversible tokenization&lt;/strong&gt;: since there is no special treatment of spaces, decoding the tokens is done simply by concatenating them and replacing the &lt;code&gt;_&lt;/code&gt;s with spaces — this results in the normalized text.&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;h2 id=&#34;bpe&#34;&gt;&#xA;  BPE&#xA;  &lt;a class=&#34;anchor&#34; href=&#34;#bpe&#34;&gt;#&lt;/a&gt;&#xA;&lt;/h2&gt;&#xA;&lt;blockquote&gt;&#xA;&lt;p&gt;GPT-2&lt;/p&gt;</description>
    </item>
    <item>
      <title>Alignment</title>
      <link>http://localhost:1313/docs/transformers/alignment/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>http://localhost:1313/docs/transformers/alignment/</guid>
      <description>&lt;h1 id=&#34;alignment&#34;&gt;&#xA;  Alignment&#xA;  &lt;a class=&#34;anchor&#34; href=&#34;#alignment&#34;&gt;#&lt;/a&gt;&#xA;&lt;/h1&gt;&#xA;&lt;p&gt;LLMs are typically trained for next-token prediction.&lt;/p&gt;&#xA;&lt;p&gt;Pre-trained LLMs may not be able to follow user instructions because they were not trained to do so.&lt;/p&gt;&#xA;&lt;p&gt;Pre-trained LLMs may generate harmful content or perpetuate  biases inherent in their training data.&lt;/p&gt;&#xA;&#xA;&#xA;&lt;script src=&#34;http://localhost:1313/mermaid.min.js&#34;&gt;&lt;/script&gt;&#xA;&#xA;  &lt;script&gt;mermaid.initialize({&#xA;  &#34;flowchart&#34;: {&#xA;    &#34;useMaxWidth&#34;:true&#xA;  },&#xA;  &#34;theme&#34;: &#34;default&#34;&#xA;}&#xA;)&lt;/script&gt;&#xA;&#xA;&#xA;&#xA;&#xA;&lt;pre class=&#34;mermaid&#34;&gt;&#xA;---&#xA;title: LLM training stages&#xA;---&#xA;flowchart LR&#xA;    subgraph Pre-training&#xA;    A[Pre-training]&#xA;    end&#xA;    subgraph Post-training&#xA;    B[&#34;Instruction Alignment (SFT)&#34;]&#xA;    C[&#34;Preference Alignment (RLHF)&#34;]&#xA;    end&#xA;    subgraph Inference&#xA;    D[Prompt engineering]&#xA;    end&#xA;    A--&gt;B&#xA;    B--&gt;C&#xA;    C--&gt;D&#xA;&lt;/pre&gt;&#xA;&#xA;&lt;h2 id=&#34;fine-tune-llms-with-labelled-data&#34;&gt;&#xA;  Fine tune LLMs with labelled data&#xA;  &lt;a class=&#34;anchor&#34; href=&#34;#fine-tune-llms-with-labelled-data&#34;&gt;#&lt;/a&gt;&#xA;&lt;/h2&gt;&#xA;&lt;h3 id=&#34;supervised-fine-tuning-sft&#34;&gt;&#xA;  Supervised Fine Tuning (SFT)&#xA;  &lt;a class=&#34;anchor&#34; href=&#34;#supervised-fine-tuning-sft&#34;&gt;#&lt;/a&gt;&#xA;&lt;/h3&gt;&#xA;&lt;p&gt;Training data is task specific instructions paired with their expected outputs.&lt;/p&gt;</description>
    </item>
  </channel>
</rss>
