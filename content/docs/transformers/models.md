---
title: Frontier models
weight: 5
bookToc: false
---

https://crfm.stanford.edu/ecosystem-graphs/index.html?mode=table


## Closed models

API access only

## Open-weight models

weights available

## Open-source models

weights, data, training details available

## BERT

| year | architecture | type | open | # parameters | who| checkpoint | tokenizer
| :----| :---- | :---- |:---- |:---- | :---- | :---- | :----
| 2018 Oct | [`BERT`](https://huggingface.co/docs/transformers/model_doc/bert) | encoder | ✅ | 340M | Google | | WordPiece
|  | [`ALBERT`](https://huggingface.co/docs/transformers/model_doc/albert) | encoder | ✅ |
|  | [`ELECTRA`](https://huggingface.co/docs/transformers/model_doc/electra) | encoder | ✅ |
|  | [`RoBERTa`](https://huggingface.co/docs/transformers/model_doc/roberta) | encoder | ✅ |
| 2019 Oct | [`DistilBERT`](https://huggingface.co/docs/transformers/model_doc/distilbert) | encoder | ✅ | 66 M | HuggingFace
| 2019 Oct | `BART` | encoder-decoder |  | 400M | Meta
| 2019 Oct | `T5` | encoder-decoder |  | 11B | Google | | SentencePiece


## GPT

> Open AI, closed, decoder

https://platform.openai.com/docs/models

| year | architecture |  parameters |  tokenizer | notes
| :----| :---- | ----: |:---- |:---- |
| 2018 Jun | `GPT` |  110 M |
| 2019 Feb | [`GPT-2`](https://cdn.openai.com/better-language-models/language_models_are_unsupervised_multitask_learners.pdf) |  1.5 B |  BPE | fluent text, first signs of zero-shot, staged release
| 2020 May | [`GPT-3`](https://arxiv.org/pdf/2005.14165) | 175 B | | in-context learning
| 2024 | [`GPT-4`](https://arxiv.org/pdf/2303.08774) | 1.8 T |
| 2025 Jan | [`OpenAI o3`](https://openai.com/index/openai-o3-mini/) |
|  | `OpenAI o4-mini` |

## Gemini

Gemini 2.5 - https://blog.google/technology/google-deepmind/gemini-model-thinking-updates-march-2025/

## PaLM

> Google

| year | architecture |  # parameters |  tokenizer | notes
| :----| :---- | :---- |:---- |:---- |
| 2018 Jun | [`PaLM`](https://arxiv.org/pdf/2204.02311) |  540 B | | massive scale, undertrained



## Llama

> Meta

https://arxiv.org/pdf/2302.13971

LLama2 https://arxiv.org/pdf/2307.09288

Llama3 https://arxiv.org/abs/2407.21783

Llama3.3 https://ai.meta.com/blog/meta-llama-3/


## Qwen

> ALibaba

https://arxiv.org/abs/2412.15115

Qwen2.5-Max https://qwenlm.github.io/blog/qwen2.5-max/

## Deepseek

https://arxiv.org/pdf/2401.02954

DeepSeek-v2 https://arxiv.org/abs/2405.04434

DeepSeek-v3 https://arxiv.org/pdf/2412.19437

DeepSeek-R1 https://arxiv.org/pdf/2501.12948

## OLMo

> AI2

https://arxiv.org/pdf/2402.00838

OLMo2 https://arxiv.org/abs/2501.00656

## Mistral

## Claude

> Anthropic

Claude 3.7 Sonnet https://www.anthropic.com/news/claude-3-7-sonnet

## Phi

## Grok

> xAI

Grok3 - https://x.ai/news/grok-3
